<!-- TOC -->

- [特征工程入门与实践](#特征工程入门与实践)
  - [特征工程简介](#特征工程简介)
    - [评估监督学习算法](#评估监督学习算法)
    - [评估无监督学习算法](#评估无监督学习算法)
    - [特征理解：我的数据集里有什么](#特征理解我的数据集里有什么)
    - [特征增强：清洗数据](#特征增强清洗数据)
    - [特征选择：对坏属性说不](#特征选择对坏属性说不)
    - [特征构建：能生成新特征吗](#特征构建能生成新特征吗)
    - [特征转换：数学显神通](#特征转换数学显神通)
    - [特征学习：以AI促AI](#特征学习以ai促ai)
    - [小结](#小结)
  - [特征理解：我的数据集里有什么](#特征理解我的数据集里有什么-1)
    - [数据结构的有无](#数据结构的有无)
      - [非结构化数据的例子：服务器日志](#非结构化数据的例子服务器日志)
    - [定量数据和定性数据](#定量数据和定性数据)
      - [按工作分类的工资](#按工作分类的工资)
    - [数据的4个等级](#数据的4个等级)
      - [定类等级](#定类等级)
        - [可以执行的数学操作](#可以执行的数学操作)
      - [定序等级](#定序等级)
        - [可以执行的数学操作](#可以执行的数学操作-1)
      - [定距等级](#定距等级)
        - [可以执行的数学操作](#可以执行的数学操作-2)
          - [在定距等级绘制两列数据](#在定距等级绘制两列数据)
      - [定比等级](#定比等级)
        - [可以执行的数学操作](#可以执行的数学操作-3)
    - [数据等级总结](#数据等级总结)
  - [特征增强：清洗数据](#特征增强清洗数据-1)
    - [识别数据中的缺失值](#识别数据中的缺失值)
      - [探索性数据分析](#探索性数据分析)
    - [处理数据集中的缺失值](#处理数据集中的缺失值)
      - [删除有害的行](#删除有害的行)
      - [填充缺失值](#填充缺失值)
      - [在机器学习流水线中填充值](#在机器学习流水线中填充值)
  - [标准化和归一化](#标准化和归一化)
    - [z分数标准化](#z分数标准化)
    - [min-max标准化](#min-max标准化)
    - [行归一化](#行归一化)
    - [整合起来](#整合起来)

<!-- /TOC -->
# 特征工程入门与实践
## 特征工程简介
### 评估监督学习算法
当进行预测建模（即监督学习）时，性能直接与模型利用数据结构的能力，以及使用数据结构进行恰当预测的能力有关。一般而言，可以将监督学习分为两种更具体的类型：分类（预测定性响应）和回归（预测定量响应）。

评估分类问题时，直接用5折交叉验证计算逻辑回归模型的准确率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000754.png)

与之类似，对于回归问题，我们用线性回归的均方误差（MSE，mean squared error）进行评估，同样使用5折交叉验证：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000814.png)

我们用这两个线性模型，而不是出于速度和低方差的考虑使用更新、更高级的模型。这样可以更加确定，性能的增长直接与特征工程相关，而不是因为模型可以发现隐藏的模式。
### 评估无监督学习算法
这个问题比较棘手。因为无监督学习不做出预测，所以不能直接根据模型预测的准确率进行评估。尽管如此，如果我们进行了聚类分析（例如之前的市场细分例子），通常会利用轮廓系数（silhouette coefficient，这是一个表示聚类分离性的变量，在-1和1之间）加上一些人工分析来确定特征工程是提升了性能还是在浪费时间。

下面的例子用Python和scikit-learn导入并计算了一些假数据的轮廓系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000954.png)

>需要记住，之所以对评估的算法和指标进行标准化，是因为要展示特征工程的强大，而且要让你成功复现我们的过程。实践中，你优化的性能有可能不是准确率，例如真阳性率（true positive rate），想用决策树而不是逻辑回归。这样很好，我们鼓励这样做。始终记住，要按步骤评估特征工程的结果，并将特征工程后的结果与基准性能进行对比。

大体上，我们会在3个领域内对特征工程的好处进行量化。

1. 监督学习：也叫预测分析
   1.  回归——预测定量数据:主要使用均方误差作为测量指标
   2.  分类——预测定性数据:主要使用准确率作为测量指标
2. 无监督学习：聚类——将数据按特征行为进行分类:主要用轮廓系数作为测量指标
3. 统计检验：用相关系数、$t$检验、卡方检验，以及其他方法评估并量化原始数据和转换后数据的效果

###  特征理解：我的数据集里有什么
在繁杂的切入点中，我们将着眼于以下几个方面：
1. 结构化数据与非结构化数据；
2. 数据的4个等级；
3. 识别数据的缺失值；
4. 探索性数据分析；
5. 描述性统计；
6. 数据可视化。

### 特征增强：清洗数据
我们探索的主题包括以下这些。
1. 对非结构化数据进行结构化。
2. 数据填充——在原先没有数据的位置填充（缺失）数据。
3. 数据归一化： 
   1. 标准化（也称为$z$分数标准化）；
   2. 极差法（也称为$min-max$标准化）；
   3. $L1$和$L2$正则化（将数据投影到不同的空间，很有趣）。

### 特征选择：对坏属性说不
这些过程包括：
1. 相关系数；
2. 识别并移除多重共线性；
3. 卡方检验；
4. 方差分析；
5. 理解$p$值；
6. 迭代特征选择；
7. 用机器学习测量熵和信息增益。

### 特征构建：能生成新特征吗
### 特征转换：数学显神通
这一章将开始着眼于自动创建特征，因为这些特征适用于数学维度。如果把数据理解为一个$n$维空间中的向量（$n$是列数），那么我们可以考虑，能不能创建一个$k$维（$k＜n$）的子集，完全或几乎完全表示原数据，从而提升机器学习速度或性能？这里的目标是，创建一个维度更低、比原有高维度数据集性能更好的数据集。

一个值得注意的例子是主成分分析（PCA，principal component analysis），我们会花一些时间深入探讨。这种转换将数据分成3个完全不同的数据集，然后可以用这些结果创造全新的数据集，让其性能超过原先的数据集！

### 特征学习：以AI促AI
我们会主要关注基于神经网络（节点和权重）的算法。这些算法在数据上增加的特征虽然有时不为人类所理解，但是机器会大受裨益。这章的主题包括：
1. 受限玻尔兹曼机（RBM，restricted Boltzmann machine）；
2. Word2vec/GloVe等词嵌入（word embedding）算法。

Word2vec和GloVe算法可以将高维度数据嵌入文本的词项（token）中。例如，Word2vec算法的可视化结果可能如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507002418.png)

在欧几里得空间中将单词表示为向量后，就可以得到数学样式的结果。在上面的例子中，加入自动生成的特征后，可以在Word2vec算法的帮助下通过计算单词的向量表示来对单词进行加减。然后可以得到有趣的结论，例如国王-男+女=女王。

### 小结
1. 特征理解：学习如何识别定量数据和定性数据。
2. 特征增强：清洗和填充缺失值，最大化数据集的价值。
3. 特征选择：通过统计方法选择一部分特征，以减少数据噪声。
4. 特征构建：构建新的特征，探索特征间的联系。
5. 特征转换：提取数据中的隐藏结构，用数学方法转换数据集、增强效果。
6. 特征学习：利用深度学习的力量，以全新的视角看待数据，从而揭示新的问题，并予以解决。

## 特征理解：我的数据集里有什么
### 数据结构的有无
拿到一个新的数据集后，首要任务是确认数据是结构化还是非结构化的。

1. 结构化（有组织）数据：可以分成观察值和特征的数据，一般以表格的形式组织（行是观察值，列是特征）。
2. 非结构化（无组织）数据：作为自由流动的实体，不遵循标准组织结构（例如表格）的数据。通常，非结构化数据在我们看来是一团数据，或只有一个特征（列）。

下面两个例子展示了结构化和非结构化数据的区别：
1. 以原始文本格式存储的数据，例如服务器日志和推文，是非结构化数据；
2. 科学仪器报告的气象数据是高度结构化的，因为存在表格的行列结构。

#### 非结构化数据的例子：服务器日志
我们从公共数据中提取了一些服务器日志，放在文本文件中，作为非结构化数据的例子。可以看看这种数据的样子，方便日后识别：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507002925.png)

我们创建了一个叫作logs的Pandas DataFrame，用于存放服务器日志。可以用`.head()`方法看一下前几行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003019.png)

logs DataFrame中数据的前5行如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003201.png)

可以发现，表中每行代表一篇日志，而且只有一列：日志文本。这个文本并不是特征，只是来自服务器的原始日志。这个例子很好地代表了非结构化数据。通常，文本形式的数据都是非结构化的。

### 定量数据和定性数据
为了完成对数据的判断，我们从区分度最高的顺序开始。在处理结构化的表格数据时（大部分时候都是如此），第一个问题一般是：数据是定量的，还是定性的？

- 定量数据本质上是数值，应该是衡量某样东西的数量。
- 定性数据本质上是类别，应该是描述某样东西的性质。

基本示例：
1. 以华氏度或摄氏度表示的气温是定量的；
2. 阴天或晴天是定性的；
3. 白宫参观者的名字是定性的；
4. 献血的血量是定量的。

有时，数据可以同时是定量和定性的。例如，餐厅的评分（1～5星）虽然是数，但是这个数也可以代表类别。如果餐厅评分应用要求你用定量的星级系统打分，并且公布带小数的平均分数（例如4.71星），那么这个数据是定量的。如果该应用问你的评价是讨厌、还行、喜欢、喜爱还是特别喜爱，那么这些就是类别。由于定量数据和定性数据之间的模糊性，我们会使用一个更深层次的方法进行处理，称为数据的4个等级。

#### 按工作分类的工资
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003915.png)

然后导入第一个数据集，探索在旧金山做不同工作的工资。这个数据集可以公开获得，随意使用。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003944.png)

可以看到表格有很多列，而且已经能发现其中一些是定性或定量的。我们用`.info()`方法了解一下数据有多少行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004026.png)

可以看到，数据有1356个条目（行）和13列。`.info()`方法也会报告每列的非空（non-null）项目数。这点非常重要，因为缺失数据是特征工程中最常见的问题之一。在Pandas包中有很多方法可以识别和处理缺失值，其中计算缺失值数量最快的方法是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004050.png)

数据中看起来没有缺失值，可以（暂时）松一口气了。接下来用`describe`方法查看一些定量数据的描述性统计（应该有定量列）。注意，`describe`方法默认描述定量列，但是如果没有定量列，也会描述定性列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004122.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004131.png)

Pandas认为，数据只有3个定量列：Step、Union Code和Extended Step（步进、工会代码和增强步进）。先不说步进和增强步进，很明显工会代码不是定量的。虽然这一列是数，但这些数不代表数量，只代表某个工会的代码。因此需要做一些工作来理解我们感兴趣的特征。最值得注意的特征是一个定量列Biweekly High Rate（双周最高工资）和一个定性列Grade（工作种类）。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004201.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004209.png)

我们清理一下数据，移除工资前面的美元符号，保证数据类型正确。当处理定量数据时，一般使用整数或浮点数作为类型（最好使用浮点数）；定性数据则一般使用字符串或Unicode对象。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004250.png)

我们可以用Pandas的`map`功能，将函数映射到整个数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004316.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004329.png)

最后，将Biweekly High Rate列中的数据转换为浮点数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004344.png)

同时，将Grade列中的数据转换为字符串：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004455.png)

可以看见，我们共有：1356行（和开始时相同）,2列（我们选择的）

- 双周最高工资：
  1. 定量列，代表某个部门的平均最高工资
  2. 此列是定量的，因为其中的值是数，代表某人每两周的工资
  3. 数据类型是浮点数，因为进行了强制转换
- 工作种类：
  1. 工资对应的部门
  2. 此列肯定是定性的，因为代码代表一个部门，而不是数量
  3. 数据类型是对象，Pandas会把字符串归为此类
  4. 为了进一步研究定量数据和定性数据，我们开始研究数据的4个等级。

### 数据的4个等级
我们已经可以将数据分为定量和定性的，但是还可以继续分类。数据的4个等级是：

1. 定类等级（nominal level）
2. 定序等级（ordinal level）
3. 定距等级（interval level）
4. 定比等级（ratio level）

每个等级都有不同的控制和数学操作等级。了解数据的等级十分重要，因为它决定了可以执行的可视化类型和操作。

#### 定类等级
定类等级是数据的第一个等级，其结构最弱。这个等级的数据只按名称分类。例如，血型（A、B、O和AB型）、动物物种和人名。这些数据都是定性的。

##### 可以执行的数学操作
对于每个等级，我们都会简要介绍可以执行的数学操作，以及不可以执行的数学操作。在这个等级上，不能执行任何定量数学操作，例如加法或除法。这些数学操作没有意义。因为没有加法和除法，所以在此等级上找不到平均值。当然了，没有“平均名”或“平均工作”这种说法。

但是，我们可以用Pandas的`value_counts`方法进行计数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005040.png)

出现最多的工作种类是00000，意味着这个种类是众数，即最多的类别。因为能在定类等级上进行计数，所以可以绘制图表（如条形图）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005107.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005116.png)

#### 定序等级
定类等级为我们提供了很多进一步探索的方法。向上一级就到了定序等级。定序等级继承了定类等级的所有属性，而且有重要的附加属性：

1. 定序等级的数据可以自然排序；
2. 这意味着，可以认为列中的某些数据比其他数据更好或更大。

和定类等级一样，定序等级的天然数据属性仍然是类别，即使用数来表示类别也是如此。

##### 可以执行的数学操作
和定类等级相比，定序等级多了一些新的能力。在定序等级，我们可以像定类等级那样进行计数，也可以引入比较和排序。因此，可以使用新的图表了。不仅可以继续使用条形图和饼图，而且因为能排序和比较，所以能计算中位数和百分位数。对于中位数和百分位数，我们可以绘制茎叶图和箱线图。

我们引入一个新的数据集来解释定序等级的数据。这个数据集表示多少人喜欢旧金山国际机场。

现在，我们关注Q7A_ART这一列。如数据字典所述，Q7A_ART是关于艺术品和展览的。可能的选择是0、1、2、3、4、5、6，每个数字都有含义。

- 1：不可接受
- 2：低于平均
- 3：平均
- 4：不错
- 5：特别好
- 6：从未有人使用或参观过
- 0：空

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005506.png)

Pandas把这个列当作数值处理了，因为这个列充满数。然而我们需要知道，虽然这些值是数，但每个数其实代表的是类别，所以该数据是定性的，更具体地说，是属于定序等级。如果删除0和6这两个类别，剩下的5个有序类别类似于餐厅的评分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005540.png)

然后将这些值转换为字符串：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010008.png)

现在定序数据的格式是正确的，可以进行可视化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010051.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010059.png)

#### 定距等级
我们开始加大火力了。在定类和定序等级，我们一直在处理定性数据。即使其内容是数，也不代表真实的数量。在定距等级，我们摆脱了这个限制，开始研究定量数据。在定距等级，数值数据不仅可以像定序等级的数据一样排序，而且值之间的差异也有意义。这意味着，在定距等级，我们不仅可以对值进行排序和比较，而且可以加减。

例子：定距等级的一个经典例子是温度。如果美国得克萨斯州的温度是32℃，阿拉斯加州的温度是4℃，那么可以计算出32-4=28℃的温差。这个例子看上去简单，但是回首之前的两个等级，我们从未对数据执行过这种操作。

##### 可以执行的数学操作
请记住，在定距等级上可以进行加减，这改变了整个游戏规则。既然可以把值加在一起，就能引入两个熟悉的概念：算术平均数（就是均值）和标准差。为了举例说明，我们引入一个新的数据集，它是关于气候变化的：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010243.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010254.png)

这个数据集有860万行，每行代表某个城市某月的平均温度，上溯到18世纪。请注意，只看前5行，我们已经可以注意到有数据缺失了。把这些数据删除，美化一下结果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010323.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010331.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010341.png)

我们关注的是AverageTemperature（平均温度）列。温度数据属于定距等级，这里不能使用条形图或饼图进行可视化，因为值太多了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010359.png)

对111994个值绘图非常奇怪，当然也没有必要，因为我们知道这些数是定量的。从这个级别开始，最常用的图是直方图。直方图是条形图的“近亲”，用不同的桶包含不同的数据，对数据的频率进行可视化。

对世界平均温度画一个直方图，从整体的角度看温度分布：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010424.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010431.png)

可以看到，平均温度约为20℃。下面确认一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010447.png)

很接近，均值大概是17℃。我们继续处理数据，加入year（年）和century（世纪）两列，只观察美国的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010754.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010818.png)

我们用新的century列，对每个世纪画直方图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010952.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010959.png)

这4幅直方图显示AverageTemperature随时间略微上升。确认一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011031.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011041.png)

因为差值在这个等级是有意义的，所以我们可以回答美国从18世纪至今平均温度上升多少这个问题。先把随世纪变化的温度数据存储到Pandas的Series对象中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011115.png)

现在可以对这个Series进行切片，用21世纪的数据减去18世纪的数据，得到温差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011148.png)

###### 在定距等级绘制两列数据
定距及更高等级的一大好处是，我们可以使用散点图：在两个轴上绘制两列数据，将数据点可视化为图像中真正的点。在气候变化数据集中，year和averageTemperature列都属于定距等级，因为它们的差值是有意义的。因此可以对美国每月的温度绘制散点图，其中x轴是年份，y轴是温度。我们希望可以看见之前折线图表示的升温趋势：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011734.png)

好像不怎么好看。和预期一样，里面有很多噪声。考虑到每年每个城镇都会报告好几个平均气温，在图上每年有很多点也是理所应当的。

我们用`groupby`清除年份的大部分噪声：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012515.png)

好多了！可以看出气温随年份上升的趋势，但是可以再用滑动均值（rolling mean）平滑一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012539.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012548.png)

我们在定距等级同时绘制两列数据，重新确认了之前用折线图表示的内容：美国的平均气温总体的确有上升的趋势。

#### 定比等级
最终，我们到达了最高的等级：定比等级。在这个等级上，可以说我们拥有最高程度的控制和数学运算能力。和定距等级一样，我们在定比等级上处理的也是定量数据。这里不仅继承了定距等级的加减运算，而且有了一个绝对零点的概念，可以做乘除运算。

##### 可以执行的数学操作
在定比等级，我们可以进行乘除运算。虽然看起来没什么大不了，但是这些运算可以让我们对这个等级上的数据进行独特的观察，而这在低等级上是无法做到的。我们先看几个例子，了解一下这意味着什么。

当处理金融数据时，我们几乎肯定要计算一些货币的值。货币处于定比等级，因为“零资金”这个概念可以存在。那么我们就可以说：

1. $100是$50的两倍，因为100/50=2；
2. 0mg青霉素是20mg青霉素的一半，因为10/20=0.5。

因为存在0这个概念，所以这种比较是有意义的。

我们一般认为，温度属于定距等级，而不是定比等级，因为100℃比50℃高两倍这种说法没有意义，并不合理。温度是主观的，不是客观正确的。

回到旧金山的工资数据，可以看到Biweekly High Rate列处于定比等级，因而可以进行新的观察。先看一下最高的工资：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015042.png)

会发现，工资最高的是公共交通部总经理（General Manager, Public Transportation Dept.）。我们用同样的办法查看工资最低的工作：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015142.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015152.png)

对照可知，工资最低的是集会助理（Camp Assistant）。

因为金钱处于定比等级，所以可以计算最高工资和最低工资的比值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015224.png)

### 数据等级总结
理解数据的不同等级对于特征工程是非常必要的。当需要构建新特征或修复旧特征时，我们必须有办法确定如何处理每一列。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015549.png)

下表展示了每个等级上可行与不可行的统计类型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015612.png)

最后这张表显示了每个等级上可以或不可以绘制的图表。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015636.png)

当你拿到一个新的数据集时，下面是基本的工作流程。
1. 数据有没有组织？数据是以表格形式存在、有不同的行列，还是以非结构化的文本格式存在？
2. 每列的数据是定量的还是定性的？单元格中的数代表的是数值还是字符串？
3. 每列处于哪个等级？是定类、定序、定距，还是定比？
4. 我可以用什么图表？条形图、饼图、茎叶图、箱线图、直方图，还是其他？

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015726.png)

## 特征增强：清洗数据
1. 识别数据中的缺失值；
2. 删除有害数据；
3. 输入（填充）缺失值；
4. 对数据进行归一化/标准化；

### 识别数据中的缺失值
特征增强的第一种方法是识别数据的缺失值，这可以让我们更好地明白如何使用真实世界中的数据。通常，数据集会因为各种原因有所缺失，例如调查时没有记录某些观察值等。分析数据并了解缺失的数据是什么至关重要，这样才可以决定下一步如何处理这些缺失值。首先，我们深入了解一下本章要使用的数据集——皮马印第安人糖尿病预测数据集。

皮马印第安人糖尿病预测数据集,数据有9列，共768个数据点（行）。这个数据集希望通过体检结果细节，预测21岁以上的女性皮马印第安人5年内是否会患糖尿病。

这个数据集和机器学习的二分类（两个类别）问题相对应。意思是，我们希望知道，这个人5年内会不会得糖尿病？数据每列的含义（按顺序）如下：
1. 怀孕次数；
2. 口服葡萄糖耐量试验中的2小时血浆葡萄糖浓度；
3. 舒张压（mmHg）；
4. 三头肌皮褶厚度（mm）；
5. 2小时血清胰岛素浓度（μU/ml）；
6. 体重指数[BMI，即体重（kg）除以身高（m）的平方]；
7. 糖尿病家族函数；
8. 年龄（岁）；
9. 类变量（0或1，代表无或有糖尿病）。

对于这个数据集，我们的目标是向机器学习函数输入8个特征值，来预测最后一列类变量的值，即此人是否患有糖尿病。

采用这个数据集有两点很重要的原因：
1. 我们必须应对缺失值；
2. 所有特征都是定量的。

因为我们的目的就是研究缺失值。本章只处理定量特征，因为目前没有足够的工具处理缺失的定性特征。

#### 探索性数据分析
首先进行探索性数据分析（EDA，exploratory data analysis）来识别缺失的值。我们会使用Pandas和NumPy这两个得力的Python包来存储数据并进行一些简单的计算，还会使用流行的可视化工具来观察数据的分布情况。开始写一点代码吧。
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022037.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022100.png)

看起来不大对劲：表格没有列名。源数据的CSV文件肯定没有内置列的标题。没关系，可以用数据源网站的信息手动添加标题，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022125.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022137.png)

看起来好多了，可以用列名做一些基本的统计、选择和可视化操作。先算一下空准确率:

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022216.png)

可以用列名做一些基本的统计、选择和可视化操作。先算一下空准确率(空准确率是指当模型总是预测频率较高的类别时达到的正确率)：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022757.png)

既然终极目标是研究数据的规律以预测是否会患糖尿病，那么可以对糖尿病患者和健康人的区别进行可视化。希望直方图可以显示一些规律，或者这两类之间的显著差异：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022934.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022947.png)

看起来患者和常人的血浆葡萄糖浓度（plasma_glucose_concentration）有很大的差异。我们继续按此绘制其他列的直方图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023007.png)

上面的代码会输出3张直方图。第一张直方图是两类（正常人和患者）的身体质量指数（BMI，body massindex）分布：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023026.png)

下一张直方图也表明两类人有显著区别，这次是在舒张压方面：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023042.png)

最后的直方图是血浆葡萄糖浓度方面的区别：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023058.png)

观察几张直方图后，可以看出两类人存在明显区别。例如，对于最终患有糖尿病的患者，其血浆葡萄糖浓度会有很大的增长。我们可以用线性相关矩阵来量化这些变量间的关系。用本章一开始导入的Seaborn作为可视化工具：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023118.png)

下图是数据集的相关矩阵，显示了皮马人数据集中不同列的相关性。输出如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023138.png)

相关矩阵显示，plasma_glucose_concentration（血浆葡萄糖浓度）和onset_diabetes（糖尿病）有很强的相关性。我们进一步研究onset_diabetes列的相关性数值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023247.png)

而目前探索性数据分析提示，plasma_glucose_concentration是预测糖尿病的重要变量。

我们要看看数据集中是否有数据点是空的（缺失值）。用Pandas DataFrame内置的`isnull()`方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023320.png)

数据中没有缺失值。继续探索数据，首先用`shape`方法看看数据的行数和列数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023416.png)

我们确定数据有9列（包括要预测的变量）和768个观察值（行）。用下面的代码看看糖尿病的发病率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023430.png)


65%的人没有糖尿病，35%的人有糖尿病。`DataFrame`内置的`describe`方法可以提供数据基本的描述性统计：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023455.png)

表格里有基本的统计量，例如均值、标准差，以及一些百分位数据。但是，请注意：BMI的最小值是0。这是有悖医学常识的，肯定事出有因。也许数据中缺失或不存在的点都用0填充了。继续观察，我们发现以下列的最小值都是0：

1. times_pregnant
2. plasma_glucose_concentration
3. diastolic_blood_pressure
4. triceps_thickness
5. serum_insulin
6. bmi
7. onset_diabetes

因为onset_diabetes中的0代表没有糖尿病，人也可以怀孕0次，所以可以得出结论，下面这些列中的缺失值用0填充了：
1. plasma_glucose_concentration
2. diastolic_blood_pressure
3. triceps_thickness
4. serum_insulin
5. bmi

数据中还是存在缺失值的！我们已经知道缺失的数据用0填充过了，真不走运。作为数据科学家，你必须时刻保持警惕，尽可能地了解数据集，以便找到使用其他符号填充的缺失数据。务必阅读公开数据集的所有文档，里面有可能提到了缺失数据的问题。

如果数据集没有文档，缺失值的常见填充方法有：
1. 0（数值型）
2. unknown或Unknown（类别型）
3. ？（类别型）

### 处理数据集中的缺失值
在处理数据时，数据科学家遇到的最常见问题之一就是存在缺失值。最常见的情况是某个单元格（行列交叉点）是空白的，数据出于某种原因没有被收集到。缺失值会引发很多问题，最重要的是，大部分（不是全部的）学习算法不能处理缺失值。

因此，数据科学家和机器学习工程师有很多处理缺失值的办法和技巧。虽然办法有很多变种，但是两个最主要的处理方法是：

1. 删除缺少值的行；
2. 填充缺失值。

这两种办法都会清洗我们的数据集，让算法可以处理，但是每种办法都各有优缺点。

在进一步处理前，先用Python中的`None`填充所有的数字0，这样Pandas的`fillna`和`dropna`方法就可以正常工作了。我们可以手动将每列的0替换成`None`，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023840.png)

既可以手动一列列地操作，也可以用for循环和内置的`replace`方法加速，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023939.png)

如果现在用`isnull`方法计算缺失值的数量，应该可以看见正确的结果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024004.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024709.png)

现在数据有意义多了。我们可以看见其中5列有缺失值，缺失程度有所不同。有些列，例如plasma_glucose_concentration只缺少5个值，但是serum_insulin差不多一半的值都是缺失的。

我们已经将缺失值正确插入了数据集，缺失数据不再是原来的占位符0。这样探索性数据分析也会更准确：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024749.png)

注意describe方法不包括有缺失值的列。尽管不理想，但还是可以对某些列取均值和标准差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025007.png)

#### 删除有害的行
在处理缺失数据的两种办法中，最常见也最容易的方法大概是直接删除存在缺失值的行。通过这种操作，我们会留下具有数据的完整数据点。可以在Pandas中利用dropna方法获取新的DataFrame，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025213.png)

当然，现在的问题是我们丢失了一些行。用下面的代码检查删除了多少行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025235.png)

我们丢失了原始数据集中大约51%的行！从机器学习的角度考虑，尽管数据都有值、很干净，但是我们没有利用尽可能多的数据，忽略了一半以上的观察值。就像医生在研究心脏病发作原理时，忽略了一半以上的患者。

下面对数据集做进一步的探索性数据分析，比较一下丢弃缺失值前后的统计数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025732.png)

看一下丢弃数据后的统计数据，如下面代码所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030019.png)

在大刀阔斧的转换后，二元响应看起来没什么变化。我们用`pima.mean`函数比较一下转换前后列的均值，看看数据的形状，结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030052.png)

用`pima_dropped.mean()`函数同样看一下丢弃缺失值后的统计数据，结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030112.png)

为了更好地了解这些数的变化，我们创建一个新图表，将每列均值变化的百分比可视化。首先创建一个表格，列出每列均值变化的百分比，如下方代码所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030206.png)

现在用条形图对这些变化进行可视化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030225.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030236.png)

可以看到，times_pregnant（怀孕次数）的均值在删除缺失值后下降了14%，变化很大！pedigree_function（糖尿病血系功能）也上升了11%，也是个飞跃。可以看到，删除行（观察值）会严重影响数据的形状，所以应该保留尽可能多的数据。在介绍下一个处理方法前，我们先进行一些机器学习。

类似于下面的代码块（稍后会一行行地讲解）将在本书中多次出现。这段代码基于目前的特征描述并实现了机器学习在多个参数上的一次拟合，希望取得最佳模型：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030437.png)

我们会使用scikit-learn的K最近邻（KNN，k-nearest neighbor）分类模型，以及一个网格搜索模块。这个模块会自动找到最适合我们模型的、交叉验证准确率最好的KNN参数组合（暴力搜索）。接下来，用删除缺失值后的数据集为预测模型创建一个X和一个y变量。我们从X（特征矩阵）开始,

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030549.png)

现在已经看出问题了：机器学习算法使用的数据比一开始拿到的数据少得多。然后创建一个y（响应变量）：

有了X和y变量，就可以为网格搜索创建需要的参数和实例了。为了简便起见，我们将params（参数）的数量设成7。对于要尝试的每一种数据清洗和特征工程方法（删除行，填充数据），都用1～7个邻居进行拟合。然后实例化一个网格搜索模块，如以下代码所示，并用特征矩阵和响应变量进行拟合。拟合后，代码会打印出最佳准确率，以及相应的最佳参数，

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031315.png)

看来，最好的邻居数是7个，此时KNN模型的准确率是74.5%（比空准确率65%好）。但是要记住，这个模型只用了49%的数据，如果能用到所有数据，会不会更好一些？

很明显，虽然删除脏数据并不完全是特征工程，但这种操作的确是一种数据清洗技术，有助于清洗机器学习流水线的输入。接下来尝试一个稍难的办法。

#### 填充缺失值
填充数据是处理缺失值的一种更复杂的方法。填充指的是利用现有知识/数据来确定缺失的数量值并填充的行为。我们有几个选择，最常见的是用此列其余部分的均值填充缺失值，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031411.png)

看一下plasma_glucose_concentration列的5个缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031441.png)

现在可以用内置的`fillna`方法，将所有的`None`填充为plasma_glucose_concentration列其余数据的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031546.png)

如果查看各列，应该会发现原来的`None`被121.68取代了，这个值就是该列的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031617.png)

这样有点麻烦。我们用scikit-learn预处理类的`Imputer`模块,称它为“填充器”名副其实。可以这样导入：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031659.png)

和大部分scikit-learn模块一样，我们有几个新的参数可以调节，但是主要关注`strategy`。这个参数可以调节如何填充缺失值。对于定量值，可以使用内置的均值或中位数策略来填充值。为了使用`Imputer`，必须先实例化对象，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031725.png)

然后调用fit_transform方法创建新对象：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031747.png)

我们有个小问题要处理。`Imputer`的输出值不是Pandas的`DataFrame`，而是NumPy数组：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031812.png)

解决方法很简单，因为我们可以将任何数组直接变成`DataFrame`，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031835.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031845.png)

我们检查一下plasma_glucose_concentration列，确保填充的值和之前手动计算的值相同：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033001.png)

Imputer在很大程度上解决了填充缺失值的琐事。我们尝试填充一些别的值，看看对KNN模型的影响。首先尝试一种更简单的填充方法，用0替代所有的缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033046.png)

如果用0填充，准确率会低于直接删掉有缺失值的行。目前，我们的目标是建立一个可以从全部768行中学习的机器学习流水线，而且比仅用392行的结果还好。也就是说，我们的结果要好于0.745，即74.5%。

#### 在机器学习流水线中填充值
用`Imputer`类填充值的时候不使用流水线其实是很不恰当的，因此流水线尤其重要。这是因为学习算法的目标是泛化训练集的模式并将其应用于测试集。如果在划分数据集和应用算法之前直接对整个数据集填充值，我们就是在作弊，模型其实学不到任何模式。为了将这个概念可视化，我们对训练集和测试集进行一次划分，在交叉验证中可能会进行多次划分。

复制一份皮马印第安人数据集，从scikit-learn中导入一个划分模块：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033228.png)

现在进行一次划分。在划分前，对整个数据集填充变量X的均值，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033258.png)

用KNN模型拟合训练集和测试集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033358.png)

注意我们没有进行任何网格搜索，只做了简单的拟合。可以看见，模型的准确率是66%（并不好，但这不是重点）。重点是，训练集和测试集都是用整个X矩阵的均值填充的。这违反了机器学习流程的核心原则。当预测测试集的响应值时，不能假设我们已经知道了整个数据集的均值。简而言之，我们的KNN模型利用了测试集的信息以拟合训练集，所以亮红灯了。

现在用恰当的方法再做一遍。我们先计算出训练集的均值，然后用它填充测试集的缺失值。这个过程再一次测试了模型用训练数据的均值预测未知测试集的能力：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033434.png)

这里不取整个X矩阵的均值，而是用训练集的均值填充训练集和测试集的缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033507.png)

最后，给在同一个数据集上正确填充缺失值的KNN模型打分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033522.png)


准确率的确低得多，但是至少更诚实地代表了模型的泛化能力，即从训练集的特征中学习并将所学应用到未知隐藏数据上的能力。通过为机器学习流水线的各个步骤提供结构和顺序，scikit-learn让搭建流水线变得更加容易。下面看看如何结合使用scikit-learn的`Pipeline`和`Imputer`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033608.png)
有几件事需要注意。第一，我们的Pipeline分两步：
1. 拥有`strategy='mean'`的`Imputer`；
2. KNN类型的分类器。
第二，要为网格搜索重新定义`param`字典，因为必须明确`n_neighbors`参数所属的步骤：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033807.png)

除此之外，其他一切都很平常。Pipeline类会替我们处理大部分流程：可以恰当地从多个训练集取值并用其填充测试集的缺失值，还可以正确测试KNN的泛化能力，最终输出性能最佳的模型。本例中的准确率是0.73，略微低于我们的目标0.745。了解这一语法后，我们可以重写一遍代码，但是略作修改，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033848.png)

这里唯一的区别是，我们的流水线尝试了一种不同的填充策略：用剩余值的中位数填充缺失值。重申一下，这里的准确率可能比完全删除存在缺失值的行更差，但是我们的训练数据是之前的两倍！况且，这个办法还是比一开始所有数据都是0要好。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033910.png)

如果只看准确率，最好的办法似乎是删掉有缺失值的行。也许只靠scikit-learn的Pipeline和Imputer还不够。如果有可能，我们还是希望利用全部768行的实现类似的性能，甚至更好。为此，我们引入全新的特征工程技巧：标准化和归一化。

## 标准化和归一化
到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过4种不同的方式处理数据集，最佳的KNN交叉验证准确率是0.745。如果回头看之前的探索性数据分析，会发现一些特征的性质：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034026.png)

到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过4种不同的方式处理数据集，最佳的KNN交叉验证准确率是0.745。如果回头看之前的探索性数据分析，会发现一些特征的性质：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034506.png)

用标准直方图查看所有9列的分布情况，指定一个图像大小：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034531.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034540.png)

每列的均值、最小值、最大值和标准差差别很大。通过`describe`方法也可以看到很明显的差别：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034635.png)

这有什么关系呢？因为一些机器学习模型受数据尺度（scale）的影响很大。这意味着如果diastolic_blood_pressure列的舒张压在24～122，但是年龄是21～81，那么算法不会达到最优化状态。我们可以在直方图方法中调用可选的sharex和sharey参数，在同一比例下查看每个图表：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034733.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034741.png)

很明显，所有的数据尺度都不同。数据工程师可以选用某种归一化操作，在机器学习流水线上处理该问题。归一化操作旨在将行和列对齐并转化为一致的规则。例如，归一化的一种常见形式是将所有定量列转化为同一个静态范围中的值（例如，所有数都位于0～1）。我们也可以使用数学规则，例如所有列的均值和标准差必须相同，以便在同一个直方图上显示（和上面皮马人的直方图不同）。标准化通过确保所有行和列在机器学习中得到平等对待，让数据的处理保持一致。

我们将重点关注3种数据归一化方法：
1. z分数标准化；
2. min-max标准化；
3. 行归一化。

前两个办法特别用于调整特征，而第三个办法虽然操作行，但效果和前两个相当。

### z分数标准化
z分数标准化是最常见的标准化技术，利用了统计学里简单的z分数（标准分数）思想。z分数标准化的输出会被重新缩放，使均值为0、标准差为1。通过缩放特征、统一化均值和方差（标准差的平方），可以让KNN这种模型达到最优化，而不会倾向于较大比例的特征。公式很简单，对于每列，用这个公式替换单元格：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042059.png)

在这个公式中：
1. z是新的值（z分数）；
2. x是单元格原来的值；
3. μ是该列的均值；
4. σ是列的标准差。

以plasma_glucose_concentration列的缩放为例：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042149.png)

用下面的代码手动计算列的z分数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042225.png)

可以看到，该列中的每个值都会被替换，而且某些值是负的。这是因为该值代表到均值的距离，如果它最初低于该列的均值，z分数就是负数。当然，在scikit-learn中有内置的对象帮助我们计算，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042249.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042257.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042314.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042322.png)

可以看见该列处理前的分布情况。现在应用z分数标准化，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042345.png)

在应用缩放后，均值下降到0，标准差为1。下面进一步查看缩放后数据的分布情况：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042427.png)

我们观察到x轴更紧密了，y轴则没有变化。注意，数据的形状没有变化。在对每一列都进行z分数转换后，我们观察一下`DataFrame`的直方图。操作时，`StandardScaler`会对每列单独计算均值和标准差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042554.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042606.png)

注意整个数据集的x轴都更加紧密了。现在将`StandardScaler`插入之前的机器学习流水线中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042649.png)

有几件事需要注意。我们在网格搜索中加入了一些新的参数，填充缺失值。现在我们正在寻找KNN中策略和邻居数的最佳组合，目前的得分是0.742，这是至今为止的最佳得分，接近目标0.745。这条流水线从整个768行中学习。我们现在看看另一种标准化方法。

### min-max标准化
min-max标准化和z分数标准化类似，因为它也用一个公式替换列中的每个值。此处的公式是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042739.png)

在这个公式中：
1. m是新的值；
2. x是单元格原来的值；
3. xmin是该列的最小值；
4. xmax是该列的最大值。

使用这个公式可以看到，每列所有的值都会位于0～1。我们用scikit-learn的内置模块试试：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042824.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042840.png)

注意，最小值都是0，最大值都是1。进一步注意，这种缩放的副作用是标准差都非常小。这有可能不利于某些模型，因为异常值的权重降低了。我们将新的标准化方法插入机器学习流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042857.png)

这是至今使用包括缺失数据的全部768行的最好结果。看起来min-max缩放对KNN有很大帮助！我们现在试试第三种标准化，把关注点从列转移到行上。

### 行归一化
最后这个标准化方法是关于行，而不是关于列的。行归一化不是计算每列的统计值（均值、最小值、最大值等），而是会保证每行有单位范数（unit norm），意味着每行的向量长度相同。想象一下，如果每行数据都在一个n维空间内，那么每行都有一个向量范数（长度）。也就是说，我们认为每行都是空间内的一个向量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042956.png)

在皮马人数据集中n为8，每个特征一个（不包括响应），那么范数的计算方法是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043040.png)

这是L2范数。其他类型的范数也存在，但是不在这里讨论。我们关心的是，让每行都有相同的范数。在使用文本数据或聚类算法时，这非常方便。

在开始之前，先看看用均值填充后的输入矩阵的平均范数，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043119.png)

我们用下面的代码引入行归一化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043137.png)

在归一化后，所有行的范数都是1了。看看这个方法在流水线上表现如何：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043234.png)

不怎么样，但值得一试。现在我们知道了3种不同的数据标准化方法，把它们整合在一起，看看如何处理这个数据集吧。

很多算法会受尺度的影响，下面就是其中一些流行的学习算法：
1. KNN——因为依赖欧几里得距离；
2. K均值聚类——和KNN的原因一样；
3. 逻辑回归、支持向量机、神经网络——如果使用梯度下降来学习权重；
4. 主成分分析——特征向量将偏向较大的列。

### 整合起来
处理了数据集的各种问题（包括识别隐藏为0的缺失值，填充缺失值，以及按不同比例标准化数据），现在可以列出所有的得分，看看哪种办法最好。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043347.png)

看来我们终于可以用均值填充和min-max标准化的方法得到最好的结果了，而且依旧使用全部的768列。不错！

特征增强的意义是，识别有问题的区域，并确定哪种修复方法最有效。我们的主要想法应该是用数据科学家的眼光看数据。我们应该考虑如何用最好的方法解决问题，而不是删除了事。一般来说，机器学习算法最终会因此取得让我们欣慰的表现。


























