<!-- TOC -->

- [特征工程入门与实践](#特征工程入门与实践)
  - [特征工程简介](#特征工程简介)
    - [评估监督学习算法](#评估监督学习算法)
    - [评估无监督学习算法](#评估无监督学习算法)
    - [特征理解：我的数据集里有什么](#特征理解我的数据集里有什么)
    - [特征增强：清洗数据](#特征增强清洗数据)
    - [特征选择：对坏属性说不](#特征选择对坏属性说不)
    - [特征构建：能生成新特征吗](#特征构建能生成新特征吗)
    - [特征转换：数学显神通](#特征转换数学显神通)
    - [特征学习：以AI促AI](#特征学习以ai促ai)
    - [小结](#小结)
  - [特征理解：我的数据集里有什么](#特征理解我的数据集里有什么-1)
    - [数据结构的有无](#数据结构的有无)
      - [非结构化数据的例子：服务器日志](#非结构化数据的例子服务器日志)
    - [定量数据和定性数据](#定量数据和定性数据)
      - [按工作分类的工资](#按工作分类的工资)
    - [数据的4个等级](#数据的4个等级)
      - [定类等级](#定类等级)
        - [可以执行的数学操作](#可以执行的数学操作)
      - [定序等级](#定序等级)
        - [可以执行的数学操作](#可以执行的数学操作-1)
      - [定距等级](#定距等级)
        - [可以执行的数学操作](#可以执行的数学操作-2)
          - [在定距等级绘制两列数据](#在定距等级绘制两列数据)
      - [定比等级](#定比等级)
        - [可以执行的数学操作](#可以执行的数学操作-3)
    - [数据等级总结](#数据等级总结)
  - [特征增强：清洗数据](#特征增强清洗数据-1)
    - [识别数据中的缺失值](#识别数据中的缺失值)
      - [探索性数据分析](#探索性数据分析)
    - [处理数据集中的缺失值](#处理数据集中的缺失值)
      - [删除有害的行](#删除有害的行)
      - [填充缺失值](#填充缺失值)
      - [在机器学习流水线中填充值](#在机器学习流水线中填充值)
  - [标准化和归一化](#标准化和归一化)
    - [z分数标准化](#z分数标准化)
    - [min-max标准化](#min-max标准化)
    - [行归一化](#行归一化)
    - [整合起来](#整合起来)
  - [特征构建：我能生成新特征吗](#特征构建我能生成新特征吗)
    - [检查数据集](#检查数据集)
    - [填充分类特征](#填充分类特征)
      - [自定义填充器](#自定义填充器)
      - [自定义分类填充器](#自定义分类填充器)
      - [自定义定量填充器](#自定义定量填充器)
    - [编码分类变量](#编码分类变量)
      - [定类等级的编码](#定类等级的编码)
      - [定序等级的编码](#定序等级的编码)
      - [将连续特征分箱](#将连续特征分箱)
      - [创建流水线](#创建流水线)
    - [扩展数值特征](#扩展数值特征)
      - [根据胸部加速度计识别动作的数据集](#根据胸部加速度计识别动作的数据集)
      - [多项式特征](#多项式特征)
        - [参数](#参数)
        - [探索性数据分析](#探索性数据分析-1)
    - [针对文本的特征构建](#针对文本的特征构建)
      - [词袋法](#词袋法)
      - [`CountVectorizer`](#countvectorizer)
        - [`CountVectorizer`的参数](#countvectorizer的参数)
      - [TF-IDF向量化器](#tf-idf向量化器)
      - [在机器学习流水线中使用文本](#在机器学习流水线中使用文本)
  - [特征选择：对坏属性说不](#特征选择对坏属性说不-1)
    - [在特征工程中实现更好的性能](#在特征工程中实现更好的性能)
      - [案例分析：信用卡逾期数据集](#案例分析信用卡逾期数据集)
    - [创建基准机器学习流水线](#创建基准机器学习流水线)
    - [特征选择的类型](#特征选择的类型)
      - [基于统计的特征选择](#基于统计的特征选择)
        - [使用皮尔逊相关系数](#使用皮尔逊相关系数)
        - [使用假设检验](#使用假设检验)
          - [理解p值](#理解p值)
          - [p值排列](#p值排列)
      - [基于模型的特征选择](#基于模型的特征选择)
        - [再议自然语言处理](#再议自然语言处理)
        - [使用机器学习选择特征](#使用机器学习选择特征)
          - [特征选择指标——针对基于树的模型](#特征选择指标针对基于树的模型)
        - [线性模型和正则化](#线性模型和正则化)
          - [正则化简介](#正则化简介)
          - [另一个特征重要性指标：线性模型参数](#另一个特征重要性指标线性模型参数)
    - [选用正确的特征选择方法](#选用正确的特征选择方法)
  - [特征转换：数学显神通](#特征转换数学显神通-1)
    - [维度缩减：特征转换、特征选择与特征构建](#维度缩减特征转换特征选择与特征构建)
    - [主成分分析](#主成分分析)
      - [PCA的工作原理](#pca的工作原理)
      - [鸢尾花数据集的PCA——手动处理](#鸢尾花数据集的pca手动处理)
        - [创建数据集的协方差矩阵](#创建数据集的协方差矩阵)
        - [计算协方差矩阵的特征值](#计算协方差矩阵的特征值)
        - [按降序保留前k个特征值](#按降序保留前k个特征值)
        - [使用保留的特征向量转换新的数据点](#使用保留的特征向量转换新的数据点)
      - [scikit-learn的PCA](#scikit-learn的pca)
      - [中心化和缩放对PCA的影响](#中心化和缩放对pca的影响)
      - [深入解释主成分](#深入解释主成分)
    - [线性判别分析](#线性判别分析)
      - [LDA的工作原理](#lda的工作原理)
        - [计算每个类别的均值向量](#计算每个类别的均值向量)
        - [计算类内和类间的散布矩阵](#计算类内和类间的散布矩阵)
        - [计算特征值和特征向量](#计算特征值和特征向量)
        - [降序排列特征值，保留前k个特征向量](#降序排列特征值保留前k个特征向量)
        - [使用前几个特征向量投影到新空间](#使用前几个特征向量投影到新空间)
      - [在scikit-learn中使用LDA](#在scikit-learn中使用lda)
    - [LDA与PCA：使用鸢尾花数据集](#lda与pca使用鸢尾花数据集)
    - [小结](#小结-1)

<!-- /TOC -->
# 特征工程入门与实践
## 特征工程简介
### 评估监督学习算法
当进行预测建模（即监督学习）时，性能直接与模型利用数据结构的能力，以及使用数据结构进行恰当预测的能力有关。一般而言，可以将监督学习分为两种更具体的类型：分类（预测定性响应）和回归（预测定量响应）。

评估分类问题时，直接用5折交叉验证计算逻辑回归模型的准确率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000754.png)

与之类似，对于回归问题，我们用线性回归的均方误差（MSE，mean squared error）进行评估，同样使用5折交叉验证：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000814.png)

我们用这两个线性模型，而不是出于速度和低方差的考虑使用更新、更高级的模型。这样可以更加确定，性能的增长直接与特征工程相关，而不是因为模型可以发现隐藏的模式。
### 评估无监督学习算法
这个问题比较棘手。因为无监督学习不做出预测，所以不能直接根据模型预测的准确率进行评估。尽管如此，如果我们进行了聚类分析（例如之前的市场细分例子），通常会利用轮廓系数（silhouette coefficient，这是一个表示聚类分离性的变量，在-1和1之间）加上一些人工分析来确定特征工程是提升了性能还是在浪费时间。

下面的例子用Python和scikit-learn导入并计算了一些假数据的轮廓系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000954.png)

>需要记住，之所以对评估的算法和指标进行标准化，是因为要展示特征工程的强大，而且要让你成功复现我们的过程。实践中，你优化的性能有可能不是准确率，例如真阳性率（true positive rate），想用决策树而不是逻辑回归。这样很好，我们鼓励这样做。始终记住，要按步骤评估特征工程的结果，并将特征工程后的结果与基准性能进行对比。

大体上，我们会在3个领域内对特征工程的好处进行量化。

1. 监督学习：也叫预测分析
   1.  回归——预测定量数据:主要使用均方误差作为测量指标
   2.  分类——预测定性数据:主要使用准确率作为测量指标
2. 无监督学习：聚类——将数据按特征行为进行分类:主要用轮廓系数作为测量指标
3. 统计检验：用相关系数、$t$检验、卡方检验，以及其他方法评估并量化原始数据和转换后数据的效果

###  特征理解：我的数据集里有什么
在繁杂的切入点中，我们将着眼于以下几个方面：
1. 结构化数据与非结构化数据；
2. 数据的4个等级；
3. 识别数据的缺失值；
4. 探索性数据分析；
5. 描述性统计；
6. 数据可视化。

### 特征增强：清洗数据
我们探索的主题包括以下这些。
1. 对非结构化数据进行结构化。
2. 数据填充——在原先没有数据的位置填充（缺失）数据。
3. 数据归一化： 
   1. 标准化（也称为$z$分数标准化）；
   2. 极差法（也称为$min-max$标准化）；
   3. $L1$和$L2$正则化（将数据投影到不同的空间，很有趣）。

### 特征选择：对坏属性说不
这些过程包括：
1. 相关系数；
2. 识别并移除多重共线性；
3. 卡方检验；
4. 方差分析；
5. 理解$p$值；
6. 迭代特征选择；
7. 用机器学习测量熵和信息增益。

### 特征构建：能生成新特征吗
### 特征转换：数学显神通
这一章将开始着眼于自动创建特征，因为这些特征适用于数学维度。如果把数据理解为一个$n$维空间中的向量（$n$是列数），那么我们可以考虑，能不能创建一个$k$维（$k＜n$）的子集，完全或几乎完全表示原数据，从而提升机器学习速度或性能？这里的目标是，创建一个维度更低、比原有高维度数据集性能更好的数据集。

一个值得注意的例子是主成分分析（PCA，principal component analysis），我们会花一些时间深入探讨。这种转换将数据分成3个完全不同的数据集，然后可以用这些结果创造全新的数据集，让其性能超过原先的数据集！

### 特征学习：以AI促AI
我们会主要关注基于神经网络（节点和权重）的算法。这些算法在数据上增加的特征虽然有时不为人类所理解，但是机器会大受裨益。这章的主题包括：
1. 受限玻尔兹曼机（RBM，restricted Boltzmann machine）；
2. Word2vec/GloVe等词嵌入（word embedding）算法。

Word2vec和GloVe算法可以将高维度数据嵌入文本的词项（token）中。例如，Word2vec算法的可视化结果可能如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507002418.png)

在欧几里得空间中将单词表示为向量后，就可以得到数学样式的结果。在上面的例子中，加入自动生成的特征后，可以在Word2vec算法的帮助下通过计算单词的向量表示来对单词进行加减。然后可以得到有趣的结论，例如国王-男+女=女王。

### 小结
1. 特征理解：学习如何识别定量数据和定性数据。
2. 特征增强：清洗和填充缺失值，最大化数据集的价值。
3. 特征选择：通过统计方法选择一部分特征，以减少数据噪声。
4. 特征构建：构建新的特征，探索特征间的联系。
5. 特征转换：提取数据中的隐藏结构，用数学方法转换数据集、增强效果。
6. 特征学习：利用深度学习的力量，以全新的视角看待数据，从而揭示新的问题，并予以解决。

## 特征理解：我的数据集里有什么
### 数据结构的有无
拿到一个新的数据集后，首要任务是确认数据是结构化还是非结构化的。

1. 结构化（有组织）数据：可以分成观察值和特征的数据，一般以表格的形式组织（行是观察值，列是特征）。
2. 非结构化（无组织）数据：作为自由流动的实体，不遵循标准组织结构（例如表格）的数据。通常，非结构化数据在我们看来是一团数据，或只有一个特征（列）。

下面两个例子展示了结构化和非结构化数据的区别：
1. 以原始文本格式存储的数据，例如服务器日志和推文，是非结构化数据；
2. 科学仪器报告的气象数据是高度结构化的，因为存在表格的行列结构。

#### 非结构化数据的例子：服务器日志
我们从公共数据中提取了一些服务器日志，放在文本文件中，作为非结构化数据的例子。可以看看这种数据的样子，方便日后识别：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507002925.png)

我们创建了一个叫作logs的Pandas DataFrame，用于存放服务器日志。可以用`.head()`方法看一下前几行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003019.png)

logs DataFrame中数据的前5行如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003201.png)

可以发现，表中每行代表一篇日志，而且只有一列：日志文本。这个文本并不是特征，只是来自服务器的原始日志。这个例子很好地代表了非结构化数据。通常，文本形式的数据都是非结构化的。

### 定量数据和定性数据
为了完成对数据的判断，我们从区分度最高的顺序开始。在处理结构化的表格数据时（大部分时候都是如此），第一个问题一般是：数据是定量的，还是定性的？

- 定量数据本质上是数值，应该是衡量某样东西的数量。
- 定性数据本质上是类别，应该是描述某样东西的性质。

基本示例：
1. 以华氏度或摄氏度表示的气温是定量的；
2. 阴天或晴天是定性的；
3. 白宫参观者的名字是定性的；
4. 献血的血量是定量的。

有时，数据可以同时是定量和定性的。例如，餐厅的评分（1～5星）虽然是数，但是这个数也可以代表类别。如果餐厅评分应用要求你用定量的星级系统打分，并且公布带小数的平均分数（例如4.71星），那么这个数据是定量的。如果该应用问你的评价是讨厌、还行、喜欢、喜爱还是特别喜爱，那么这些就是类别。由于定量数据和定性数据之间的模糊性，我们会使用一个更深层次的方法进行处理，称为数据的4个等级。

#### 按工作分类的工资
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003915.png)

然后导入第一个数据集，探索在旧金山做不同工作的工资。这个数据集可以公开获得，随意使用。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003944.png)

可以看到表格有很多列，而且已经能发现其中一些是定性或定量的。我们用`.info()`方法了解一下数据有多少行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004026.png)

可以看到，数据有1356个条目（行）和13列。`.info()`方法也会报告每列的非空（non-null）项目数。这点非常重要，因为缺失数据是特征工程中最常见的问题之一。在Pandas包中有很多方法可以识别和处理缺失值，其中计算缺失值数量最快的方法是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004050.png)

数据中看起来没有缺失值，可以（暂时）松一口气了。接下来用`describe`方法查看一些定量数据的描述性统计（应该有定量列）。注意，`describe`方法默认描述定量列，但是如果没有定量列，也会描述定性列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004122.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004131.png)

Pandas认为，数据只有3个定量列：Step、Union Code和Extended Step（步进、工会代码和增强步进）。先不说步进和增强步进，很明显工会代码不是定量的。虽然这一列是数，但这些数不代表数量，只代表某个工会的代码。因此需要做一些工作来理解我们感兴趣的特征。最值得注意的特征是一个定量列Biweekly High Rate（双周最高工资）和一个定性列Grade（工作种类）。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004201.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004209.png)

我们清理一下数据，移除工资前面的美元符号，保证数据类型正确。当处理定量数据时，一般使用整数或浮点数作为类型（最好使用浮点数）；定性数据则一般使用字符串或Unicode对象。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004250.png)

我们可以用Pandas的`map`功能，将函数映射到整个数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004316.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004329.png)

最后，将Biweekly High Rate列中的数据转换为浮点数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004344.png)

同时，将Grade列中的数据转换为字符串：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004455.png)

可以看见，我们共有：1356行（和开始时相同）,2列（我们选择的）

- 双周最高工资：
  1. 定量列，代表某个部门的平均最高工资
  2. 此列是定量的，因为其中的值是数，代表某人每两周的工资
  3. 数据类型是浮点数，因为进行了强制转换
- 工作种类：
  1. 工资对应的部门
  2. 此列肯定是定性的，因为代码代表一个部门，而不是数量
  3. 数据类型是对象，Pandas会把字符串归为此类
  4. 为了进一步研究定量数据和定性数据，我们开始研究数据的4个等级。

### 数据的4个等级
我们已经可以将数据分为定量和定性的，但是还可以继续分类。数据的4个等级是：

1. 定类等级（nominal level）
2. 定序等级（ordinal level）
3. 定距等级（interval level）
4. 定比等级（ratio level）

每个等级都有不同的控制和数学操作等级。了解数据的等级十分重要，因为它决定了可以执行的可视化类型和操作。

#### 定类等级
定类等级是数据的第一个等级，其结构最弱。这个等级的数据只按名称分类。例如，血型（A、B、O和AB型）、动物物种和人名。这些数据都是定性的。

##### 可以执行的数学操作
对于每个等级，我们都会简要介绍可以执行的数学操作，以及不可以执行的数学操作。在这个等级上，不能执行任何定量数学操作，例如加法或除法。这些数学操作没有意义。因为没有加法和除法，所以在此等级上找不到平均值。当然了，没有“平均名”或“平均工作”这种说法。

但是，我们可以用Pandas的`value_counts`方法进行计数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005040.png)

出现最多的工作种类是00000，意味着这个种类是众数，即最多的类别。因为能在定类等级上进行计数，所以可以绘制图表（如条形图）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005107.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005116.png)

#### 定序等级
定类等级为我们提供了很多进一步探索的方法。向上一级就到了定序等级。定序等级继承了定类等级的所有属性，而且有重要的附加属性：

1. 定序等级的数据可以自然排序；
2. 这意味着，可以认为列中的某些数据比其他数据更好或更大。

和定类等级一样，定序等级的天然数据属性仍然是类别，即使用数来表示类别也是如此。

##### 可以执行的数学操作
和定类等级相比，定序等级多了一些新的能力。在定序等级，我们可以像定类等级那样进行计数，也可以引入比较和排序。因此，可以使用新的图表了。不仅可以继续使用条形图和饼图，而且因为能排序和比较，所以能计算中位数和百分位数。对于中位数和百分位数，我们可以绘制茎叶图和箱线图。

我们引入一个新的数据集来解释定序等级的数据。这个数据集表示多少人喜欢旧金山国际机场。

现在，我们关注Q7A_ART这一列。如数据字典所述，Q7A_ART是关于艺术品和展览的。可能的选择是0、1、2、3、4、5、6，每个数字都有含义。

- 1：不可接受
- 2：低于平均
- 3：平均
- 4：不错
- 5：特别好
- 6：从未有人使用或参观过
- 0：空

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005506.png)

Pandas把这个列当作数值处理了，因为这个列充满数。然而我们需要知道，虽然这些值是数，但每个数其实代表的是类别，所以该数据是定性的，更具体地说，是属于定序等级。如果删除0和6这两个类别，剩下的5个有序类别类似于餐厅的评分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005540.png)

然后将这些值转换为字符串：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010008.png)

现在定序数据的格式是正确的，可以进行可视化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010051.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010059.png)

#### 定距等级
我们开始加大火力了。在定类和定序等级，我们一直在处理定性数据。即使其内容是数，也不代表真实的数量。在定距等级，我们摆脱了这个限制，开始研究定量数据。在定距等级，数值数据不仅可以像定序等级的数据一样排序，而且值之间的差异也有意义。这意味着，在定距等级，我们不仅可以对值进行排序和比较，而且可以加减。

例子：定距等级的一个经典例子是温度。如果美国得克萨斯州的温度是32℃，阿拉斯加州的温度是4℃，那么可以计算出32-4=28℃的温差。这个例子看上去简单，但是回首之前的两个等级，我们从未对数据执行过这种操作。

##### 可以执行的数学操作
请记住，在定距等级上可以进行加减，这改变了整个游戏规则。既然可以把值加在一起，就能引入两个熟悉的概念：算术平均数（就是均值）和标准差。为了举例说明，我们引入一个新的数据集，它是关于气候变化的：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010243.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010254.png)

这个数据集有860万行，每行代表某个城市某月的平均温度，上溯到18世纪。请注意，只看前5行，我们已经可以注意到有数据缺失了。把这些数据删除，美化一下结果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010323.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010331.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010341.png)

我们关注的是AverageTemperature（平均温度）列。温度数据属于定距等级，这里不能使用条形图或饼图进行可视化，因为值太多了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010359.png)

对111994个值绘图非常奇怪，当然也没有必要，因为我们知道这些数是定量的。从这个级别开始，最常用的图是直方图。直方图是条形图的“近亲”，用不同的桶包含不同的数据，对数据的频率进行可视化。

对世界平均温度画一个直方图，从整体的角度看温度分布：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010424.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010431.png)

可以看到，平均温度约为20℃。下面确认一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010447.png)

很接近，均值大概是17℃。我们继续处理数据，加入year（年）和century（世纪）两列，只观察美国的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010754.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010818.png)

我们用新的century列，对每个世纪画直方图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010952.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010959.png)

这4幅直方图显示AverageTemperature随时间略微上升。确认一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011031.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011041.png)

因为差值在这个等级是有意义的，所以我们可以回答美国从18世纪至今平均温度上升多少这个问题。先把随世纪变化的温度数据存储到Pandas的Series对象中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011115.png)

现在可以对这个Series进行切片，用21世纪的数据减去18世纪的数据，得到温差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011148.png)

###### 在定距等级绘制两列数据
定距及更高等级的一大好处是，我们可以使用散点图：在两个轴上绘制两列数据，将数据点可视化为图像中真正的点。在气候变化数据集中，year和averageTemperature列都属于定距等级，因为它们的差值是有意义的。因此可以对美国每月的温度绘制散点图，其中x轴是年份，y轴是温度。我们希望可以看见之前折线图表示的升温趋势：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011734.png)

好像不怎么好看。和预期一样，里面有很多噪声。考虑到每年每个城镇都会报告好几个平均气温，在图上每年有很多点也是理所应当的。

我们用`groupby`清除年份的大部分噪声：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012515.png)

好多了！可以看出气温随年份上升的趋势，但是可以再用滑动均值（rolling mean）平滑一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012539.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012548.png)

我们在定距等级同时绘制两列数据，重新确认了之前用折线图表示的内容：美国的平均气温总体的确有上升的趋势。

#### 定比等级
最终，我们到达了最高的等级：定比等级。在这个等级上，可以说我们拥有最高程度的控制和数学运算能力。和定距等级一样，我们在定比等级上处理的也是定量数据。这里不仅继承了定距等级的加减运算，而且有了一个绝对零点的概念，可以做乘除运算。

##### 可以执行的数学操作
在定比等级，我们可以进行乘除运算。虽然看起来没什么大不了，但是这些运算可以让我们对这个等级上的数据进行独特的观察，而这在低等级上是无法做到的。我们先看几个例子，了解一下这意味着什么。

当处理金融数据时，我们几乎肯定要计算一些货币的值。货币处于定比等级，因为“零资金”这个概念可以存在。那么我们就可以说：

1. $100是$50的两倍，因为100/50=2；
2. 0mg青霉素是20mg青霉素的一半，因为10/20=0.5。

因为存在0这个概念，所以这种比较是有意义的。

我们一般认为，温度属于定距等级，而不是定比等级，因为100℃比50℃高两倍这种说法没有意义，并不合理。温度是主观的，不是客观正确的。

回到旧金山的工资数据，可以看到Biweekly High Rate列处于定比等级，因而可以进行新的观察。先看一下最高的工资：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015042.png)

会发现，工资最高的是公共交通部总经理（General Manager, Public Transportation Dept.）。我们用同样的办法查看工资最低的工作：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015142.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015152.png)

对照可知，工资最低的是集会助理（Camp Assistant）。

因为金钱处于定比等级，所以可以计算最高工资和最低工资的比值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015224.png)

### 数据等级总结
理解数据的不同等级对于特征工程是非常必要的。当需要构建新特征或修复旧特征时，我们必须有办法确定如何处理每一列。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015549.png)

下表展示了每个等级上可行与不可行的统计类型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015612.png)

最后这张表显示了每个等级上可以或不可以绘制的图表。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015636.png)

当你拿到一个新的数据集时，下面是基本的工作流程。
1. 数据有没有组织？数据是以表格形式存在、有不同的行列，还是以非结构化的文本格式存在？
2. 每列的数据是定量的还是定性的？单元格中的数代表的是数值还是字符串？
3. 每列处于哪个等级？是定类、定序、定距，还是定比？
4. 我可以用什么图表？条形图、饼图、茎叶图、箱线图、直方图，还是其他？

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015726.png)

## 特征增强：清洗数据
1. 识别数据中的缺失值；
2. 删除有害数据；
3. 输入（填充）缺失值；
4. 对数据进行归一化/标准化；

### 识别数据中的缺失值
特征增强的第一种方法是识别数据的缺失值，这可以让我们更好地明白如何使用真实世界中的数据。通常，数据集会因为各种原因有所缺失，例如调查时没有记录某些观察值等。分析数据并了解缺失的数据是什么至关重要，这样才可以决定下一步如何处理这些缺失值。首先，我们深入了解一下本章要使用的数据集——皮马印第安人糖尿病预测数据集。

皮马印第安人糖尿病预测数据集,数据有9列，共768个数据点（行）。这个数据集希望通过体检结果细节，预测21岁以上的女性皮马印第安人5年内是否会患糖尿病。

这个数据集和机器学习的二分类（两个类别）问题相对应。意思是，我们希望知道，这个人5年内会不会得糖尿病？数据每列的含义（按顺序）如下：
1. 怀孕次数；
2. 口服葡萄糖耐量试验中的2小时血浆葡萄糖浓度；
3. 舒张压（mmHg）；
4. 三头肌皮褶厚度（mm）；
5. 2小时血清胰岛素浓度（μU/ml）；
6. 体重指数[BMI，即体重（kg）除以身高（m）的平方]；
7. 糖尿病家族函数；
8. 年龄（岁）；
9. 类变量（0或1，代表无或有糖尿病）。

对于这个数据集，我们的目标是向机器学习函数输入8个特征值，来预测最后一列类变量的值，即此人是否患有糖尿病。

采用这个数据集有两点很重要的原因：
1. 我们必须应对缺失值；
2. 所有特征都是定量的。

因为我们的目的就是研究缺失值。本章只处理定量特征，因为目前没有足够的工具处理缺失的定性特征。

#### 探索性数据分析
首先进行探索性数据分析（EDA，exploratory data analysis）来识别缺失的值。我们会使用Pandas和NumPy这两个得力的Python包来存储数据并进行一些简单的计算，还会使用流行的可视化工具来观察数据的分布情况。开始写一点代码吧。
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022037.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022100.png)

看起来不大对劲：表格没有列名。源数据的CSV文件肯定没有内置列的标题。没关系，可以用数据源网站的信息手动添加标题，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022125.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022137.png)

看起来好多了，可以用列名做一些基本的统计、选择和可视化操作。先算一下空准确率:

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022216.png)

可以用列名做一些基本的统计、选择和可视化操作。先算一下空准确率(空准确率是指当模型总是预测频率较高的类别时达到的正确率)：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022757.png)

既然终极目标是研究数据的规律以预测是否会患糖尿病，那么可以对糖尿病患者和健康人的区别进行可视化。希望直方图可以显示一些规律，或者这两类之间的显著差异：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022934.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022947.png)

看起来患者和常人的血浆葡萄糖浓度（plasma_glucose_concentration）有很大的差异。我们继续按此绘制其他列的直方图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023007.png)

上面的代码会输出3张直方图。第一张直方图是两类（正常人和患者）的身体质量指数（BMI，body massindex）分布：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023026.png)

下一张直方图也表明两类人有显著区别，这次是在舒张压方面：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023042.png)

最后的直方图是血浆葡萄糖浓度方面的区别：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023058.png)

观察几张直方图后，可以看出两类人存在明显区别。例如，对于最终患有糖尿病的患者，其血浆葡萄糖浓度会有很大的增长。我们可以用线性相关矩阵来量化这些变量间的关系。用本章一开始导入的Seaborn作为可视化工具：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023118.png)

下图是数据集的相关矩阵，显示了皮马人数据集中不同列的相关性。输出如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023138.png)

相关矩阵显示，plasma_glucose_concentration（血浆葡萄糖浓度）和onset_diabetes（糖尿病）有很强的相关性。我们进一步研究onset_diabetes列的相关性数值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023247.png)

而目前探索性数据分析提示，plasma_glucose_concentration是预测糖尿病的重要变量。

我们要看看数据集中是否有数据点是空的（缺失值）。用Pandas DataFrame内置的`isnull()`方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023320.png)

数据中没有缺失值。继续探索数据，首先用`shape`方法看看数据的行数和列数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023416.png)

我们确定数据有9列（包括要预测的变量）和768个观察值（行）。用下面的代码看看糖尿病的发病率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023430.png)


65%的人没有糖尿病，35%的人有糖尿病。`DataFrame`内置的`describe`方法可以提供数据基本的描述性统计：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023455.png)

表格里有基本的统计量，例如均值、标准差，以及一些百分位数据。但是，请注意：BMI的最小值是0。这是有悖医学常识的，肯定事出有因。也许数据中缺失或不存在的点都用0填充了。继续观察，我们发现以下列的最小值都是0：

1. times_pregnant
2. plasma_glucose_concentration
3. diastolic_blood_pressure
4. triceps_thickness
5. serum_insulin
6. bmi
7. onset_diabetes

因为onset_diabetes中的0代表没有糖尿病，人也可以怀孕0次，所以可以得出结论，下面这些列中的缺失值用0填充了：
1. plasma_glucose_concentration
2. diastolic_blood_pressure
3. triceps_thickness
4. serum_insulin
5. bmi

数据中还是存在缺失值的！我们已经知道缺失的数据用0填充过了，真不走运。作为数据科学家，你必须时刻保持警惕，尽可能地了解数据集，以便找到使用其他符号填充的缺失数据。务必阅读公开数据集的所有文档，里面有可能提到了缺失数据的问题。

如果数据集没有文档，缺失值的常见填充方法有：
1. 0（数值型）
2. unknown或Unknown（类别型）
3. ？（类别型）

### 处理数据集中的缺失值
在处理数据时，数据科学家遇到的最常见问题之一就是存在缺失值。最常见的情况是某个单元格（行列交叉点）是空白的，数据出于某种原因没有被收集到。缺失值会引发很多问题，最重要的是，大部分（不是全部的）学习算法不能处理缺失值。

因此，数据科学家和机器学习工程师有很多处理缺失值的办法和技巧。虽然办法有很多变种，但是两个最主要的处理方法是：

1. 删除缺少值的行；
2. 填充缺失值。

这两种办法都会清洗我们的数据集，让算法可以处理，但是每种办法都各有优缺点。

在进一步处理前，先用Python中的`None`填充所有的数字0，这样Pandas的`fillna`和`dropna`方法就可以正常工作了。我们可以手动将每列的0替换成`None`，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023840.png)

既可以手动一列列地操作，也可以用for循环和内置的`replace`方法加速，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023939.png)

如果现在用`isnull`方法计算缺失值的数量，应该可以看见正确的结果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024004.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024709.png)

现在数据有意义多了。我们可以看见其中5列有缺失值，缺失程度有所不同。有些列，例如plasma_glucose_concentration只缺少5个值，但是serum_insulin差不多一半的值都是缺失的。

我们已经将缺失值正确插入了数据集，缺失数据不再是原来的占位符0。这样探索性数据分析也会更准确：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024749.png)

注意describe方法不包括有缺失值的列。尽管不理想，但还是可以对某些列取均值和标准差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025007.png)

#### 删除有害的行
在处理缺失数据的两种办法中，最常见也最容易的方法大概是直接删除存在缺失值的行。通过这种操作，我们会留下具有数据的完整数据点。可以在Pandas中利用dropna方法获取新的DataFrame，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025213.png)

当然，现在的问题是我们丢失了一些行。用下面的代码检查删除了多少行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025235.png)

我们丢失了原始数据集中大约51%的行！从机器学习的角度考虑，尽管数据都有值、很干净，但是我们没有利用尽可能多的数据，忽略了一半以上的观察值。就像医生在研究心脏病发作原理时，忽略了一半以上的患者。

下面对数据集做进一步的探索性数据分析，比较一下丢弃缺失值前后的统计数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025732.png)

看一下丢弃数据后的统计数据，如下面代码所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030019.png)

在大刀阔斧的转换后，二元响应看起来没什么变化。我们用`pima.mean`函数比较一下转换前后列的均值，看看数据的形状，结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030052.png)

用`pima_dropped.mean()`函数同样看一下丢弃缺失值后的统计数据，结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030112.png)

为了更好地了解这些数的变化，我们创建一个新图表，将每列均值变化的百分比可视化。首先创建一个表格，列出每列均值变化的百分比，如下方代码所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030206.png)

现在用条形图对这些变化进行可视化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030225.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030236.png)

可以看到，times_pregnant（怀孕次数）的均值在删除缺失值后下降了14%，变化很大！pedigree_function（糖尿病血系功能）也上升了11%，也是个飞跃。可以看到，删除行（观察值）会严重影响数据的形状，所以应该保留尽可能多的数据。在介绍下一个处理方法前，我们先进行一些机器学习。

类似于下面的代码块（稍后会一行行地讲解）将在本书中多次出现。这段代码基于目前的特征描述并实现了机器学习在多个参数上的一次拟合，希望取得最佳模型：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030437.png)

我们会使用scikit-learn的K最近邻（KNN，k-nearest neighbor）分类模型，以及一个网格搜索模块。这个模块会自动找到最适合我们模型的、交叉验证准确率最好的KNN参数组合（暴力搜索）。接下来，用删除缺失值后的数据集为预测模型创建一个X和一个y变量。我们从X（特征矩阵）开始,

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030549.png)

现在已经看出问题了：机器学习算法使用的数据比一开始拿到的数据少得多。然后创建一个y（响应变量）：

有了X和y变量，就可以为网格搜索创建需要的参数和实例了。为了简便起见，我们将params（参数）的数量设成7。对于要尝试的每一种数据清洗和特征工程方法（删除行，填充数据），都用1～7个邻居进行拟合。然后实例化一个网格搜索模块，如以下代码所示，并用特征矩阵和响应变量进行拟合。拟合后，代码会打印出最佳准确率，以及相应的最佳参数，

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031315.png)

看来，最好的邻居数是7个，此时KNN模型的准确率是74.5%（比空准确率65%好）。但是要记住，这个模型只用了49%的数据，如果能用到所有数据，会不会更好一些？

很明显，虽然删除脏数据并不完全是特征工程，但这种操作的确是一种数据清洗技术，有助于清洗机器学习流水线的输入。接下来尝试一个稍难的办法。

#### 填充缺失值
填充数据是处理缺失值的一种更复杂的方法。填充指的是利用现有知识/数据来确定缺失的数量值并填充的行为。我们有几个选择，最常见的是用此列其余部分的均值填充缺失值，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031411.png)

看一下plasma_glucose_concentration列的5个缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031441.png)

现在可以用内置的`fillna`方法，将所有的`None`填充为plasma_glucose_concentration列其余数据的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031546.png)

如果查看各列，应该会发现原来的`None`被121.68取代了，这个值就是该列的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031617.png)

这样有点麻烦。我们用scikit-learn预处理类的`Imputer`模块,称它为“填充器”名副其实。可以这样导入：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031659.png)

和大部分scikit-learn模块一样，我们有几个新的参数可以调节，但是主要关注`strategy`。这个参数可以调节如何填充缺失值。对于定量值，可以使用内置的均值或中位数策略来填充值。为了使用`Imputer`，必须先实例化对象，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031725.png)

然后调用fit_transform方法创建新对象：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031747.png)

我们有个小问题要处理。`Imputer`的输出值不是Pandas的`DataFrame`，而是NumPy数组：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031812.png)

解决方法很简单，因为我们可以将任何数组直接变成`DataFrame`，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031835.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031845.png)

我们检查一下plasma_glucose_concentration列，确保填充的值和之前手动计算的值相同：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033001.png)

Imputer在很大程度上解决了填充缺失值的琐事。我们尝试填充一些别的值，看看对KNN模型的影响。首先尝试一种更简单的填充方法，用0替代所有的缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033046.png)

如果用0填充，准确率会低于直接删掉有缺失值的行。目前，我们的目标是建立一个可以从全部768行中学习的机器学习流水线，而且比仅用392行的结果还好。也就是说，我们的结果要好于0.745，即74.5%。

#### 在机器学习流水线中填充值
用`Imputer`类填充值的时候不使用流水线其实是很不恰当的，因此流水线尤其重要。这是因为学习算法的目标是泛化训练集的模式并将其应用于测试集。如果在划分数据集和应用算法之前直接对整个数据集填充值，我们就是在作弊，模型其实学不到任何模式。为了将这个概念可视化，我们对训练集和测试集进行一次划分，在交叉验证中可能会进行多次划分。

复制一份皮马印第安人数据集，从scikit-learn中导入一个划分模块：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033228.png)

现在进行一次划分。在划分前，对整个数据集填充变量X的均值，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033258.png)

用KNN模型拟合训练集和测试集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033358.png)

注意我们没有进行任何网格搜索，只做了简单的拟合。可以看见，模型的准确率是66%（并不好，但这不是重点）。重点是，训练集和测试集都是用整个X矩阵的均值填充的。这违反了机器学习流程的核心原则。当预测测试集的响应值时，不能假设我们已经知道了整个数据集的均值。简而言之，我们的KNN模型利用了测试集的信息以拟合训练集，所以亮红灯了。

现在用恰当的方法再做一遍。我们先计算出训练集的均值，然后用它填充测试集的缺失值。这个过程再一次测试了模型用训练数据的均值预测未知测试集的能力：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033434.png)

这里不取整个X矩阵的均值，而是用训练集的均值填充训练集和测试集的缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033507.png)

最后，给在同一个数据集上正确填充缺失值的KNN模型打分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033522.png)


准确率的确低得多，但是至少更诚实地代表了模型的泛化能力，即从训练集的特征中学习并将所学应用到未知隐藏数据上的能力。通过为机器学习流水线的各个步骤提供结构和顺序，scikit-learn让搭建流水线变得更加容易。下面看看如何结合使用scikit-learn的`Pipeline`和`Imputer`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033608.png)
有几件事需要注意。第一，我们的Pipeline分两步：
1. 拥有`strategy='mean'`的`Imputer`；
2. KNN类型的分类器。
第二，要为网格搜索重新定义`param`字典，因为必须明确`n_neighbors`参数所属的步骤：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033807.png)

除此之外，其他一切都很平常。Pipeline类会替我们处理大部分流程：可以恰当地从多个训练集取值并用其填充测试集的缺失值，还可以正确测试KNN的泛化能力，最终输出性能最佳的模型。本例中的准确率是0.73，略微低于我们的目标0.745。了解这一语法后，我们可以重写一遍代码，但是略作修改，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033848.png)

这里唯一的区别是，我们的流水线尝试了一种不同的填充策略：用剩余值的中位数填充缺失值。重申一下，这里的准确率可能比完全删除存在缺失值的行更差，但是我们的训练数据是之前的两倍！况且，这个办法还是比一开始所有数据都是0要好。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033910.png)

如果只看准确率，最好的办法似乎是删掉有缺失值的行。也许只靠scikit-learn的Pipeline和Imputer还不够。如果有可能，我们还是希望利用全部768行的实现类似的性能，甚至更好。为此，我们引入全新的特征工程技巧：标准化和归一化。

## 标准化和归一化
到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过4种不同的方式处理数据集，最佳的KNN交叉验证准确率是0.745。如果回头看之前的探索性数据分析，会发现一些特征的性质：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034026.png)

到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过4种不同的方式处理数据集，最佳的KNN交叉验证准确率是0.745。如果回头看之前的探索性数据分析，会发现一些特征的性质：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034506.png)

用标准直方图查看所有9列的分布情况，指定一个图像大小：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034531.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034540.png)

每列的均值、最小值、最大值和标准差差别很大。通过`describe`方法也可以看到很明显的差别：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034635.png)

这有什么关系呢？因为一些机器学习模型受数据尺度（scale）的影响很大。这意味着如果diastolic_blood_pressure列的舒张压在24～122，但是年龄是21～81，那么算法不会达到最优化状态。我们可以在直方图方法中调用可选的sharex和sharey参数，在同一比例下查看每个图表：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034733.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034741.png)

很明显，所有的数据尺度都不同。数据工程师可以选用某种归一化操作，在机器学习流水线上处理该问题。归一化操作旨在将行和列对齐并转化为一致的规则。例如，归一化的一种常见形式是将所有定量列转化为同一个静态范围中的值（例如，所有数都位于0～1）。我们也可以使用数学规则，例如所有列的均值和标准差必须相同，以便在同一个直方图上显示（和上面皮马人的直方图不同）。标准化通过确保所有行和列在机器学习中得到平等对待，让数据的处理保持一致。

我们将重点关注3种数据归一化方法：
1. z分数标准化；
2. min-max标准化；
3. 行归一化。

前两个办法特别用于调整特征，而第三个办法虽然操作行，但效果和前两个相当。

### z分数标准化
z分数标准化是最常见的标准化技术，利用了统计学里简单的z分数（标准分数）思想。z分数标准化的输出会被重新缩放，使均值为0、标准差为1。通过缩放特征、统一化均值和方差（标准差的平方），可以让KNN这种模型达到最优化，而不会倾向于较大比例的特征。公式很简单，对于每列，用这个公式替换单元格：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042059.png)

在这个公式中：
1. z是新的值（z分数）；
2. x是单元格原来的值；
3. μ是该列的均值；
4. σ是列的标准差。

以plasma_glucose_concentration列的缩放为例：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042149.png)

用下面的代码手动计算列的z分数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042225.png)

可以看到，该列中的每个值都会被替换，而且某些值是负的。这是因为该值代表到均值的距离，如果它最初低于该列的均值，z分数就是负数。当然，在scikit-learn中有内置的对象帮助我们计算，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042249.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042257.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042314.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042322.png)

可以看见该列处理前的分布情况。现在应用z分数标准化，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042345.png)

在应用缩放后，均值下降到0，标准差为1。下面进一步查看缩放后数据的分布情况：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042427.png)

我们观察到x轴更紧密了，y轴则没有变化。注意，数据的形状没有变化。在对每一列都进行z分数转换后，我们观察一下`DataFrame`的直方图。操作时，`StandardScaler`会对每列单独计算均值和标准差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042554.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042606.png)

注意整个数据集的x轴都更加紧密了。现在将`StandardScaler`插入之前的机器学习流水线中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042649.png)

有几件事需要注意。我们在网格搜索中加入了一些新的参数，填充缺失值。现在我们正在寻找KNN中策略和邻居数的最佳组合，目前的得分是0.742，这是至今为止的最佳得分，接近目标0.745。这条流水线从整个768行中学习。我们现在看看另一种标准化方法。

### min-max标准化
min-max标准化和z分数标准化类似，因为它也用一个公式替换列中的每个值。此处的公式是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042739.png)

在这个公式中：
1. m是新的值；
2. x是单元格原来的值；
3. xmin是该列的最小值；
4. xmax是该列的最大值。

使用这个公式可以看到，每列所有的值都会位于0～1。我们用scikit-learn的内置模块试试：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042824.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042840.png)

注意，最小值都是0，最大值都是1。进一步注意，这种缩放的副作用是标准差都非常小。这有可能不利于某些模型，因为异常值的权重降低了。我们将新的标准化方法插入机器学习流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042857.png)

这是至今使用包括缺失数据的全部768行的最好结果。看起来min-max缩放对KNN有很大帮助！我们现在试试第三种标准化，把关注点从列转移到行上。

### 行归一化
最后这个标准化方法是关于行，而不是关于列的。行归一化不是计算每列的统计值（均值、最小值、最大值等），而是会保证每行有单位范数（unit norm），意味着每行的向量长度相同。想象一下，如果每行数据都在一个n维空间内，那么每行都有一个向量范数（长度）。也就是说，我们认为每行都是空间内的一个向量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042956.png)

在皮马人数据集中n为8，每个特征一个（不包括响应），那么范数的计算方法是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043040.png)

这是L2范数。其他类型的范数也存在，但是不在这里讨论。我们关心的是，让每行都有相同的范数。在使用文本数据或聚类算法时，这非常方便。

在开始之前，先看看用均值填充后的输入矩阵的平均范数，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043119.png)

我们用下面的代码引入行归一化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043137.png)

在归一化后，所有行的范数都是1了。看看这个方法在流水线上表现如何：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043234.png)

不怎么样，但值得一试。现在我们知道了3种不同的数据标准化方法，把它们整合在一起，看看如何处理这个数据集吧。

很多算法会受尺度的影响，下面就是其中一些流行的学习算法：
1. KNN——因为依赖欧几里得距离；
2. K均值聚类——和KNN的原因一样；
3. 逻辑回归、支持向量机、神经网络——如果使用梯度下降来学习权重；
4. 主成分分析——特征向量将偏向较大的列。

### 整合起来
处理了数据集的各种问题（包括识别隐藏为0的缺失值，填充缺失值，以及按不同比例标准化数据），现在可以列出所有的得分，看看哪种办法最好。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043347.png)

看来我们终于可以用均值填充和min-max标准化的方法得到最好的结果了，而且依旧使用全部的768列。不错！

特征增强的意义是，识别有问题的区域，并确定哪种修复方法最有效。我们的主要想法应该是用数据科学家的眼光看数据。我们应该考虑如何用最好的方法解决问题，而不是删除了事。一般来说，机器学习算法最终会因此取得让我们欣慰的表现。

## 特征构建：我能生成新特征吗
需要注意的是，之前使用的特征都是定量的。我们现在就开始转而研究分类数据。我们的主要目的是，使用现有特征构建全新的特征，让模型从中学习。

有很多方法可以构建新特征：最简单的办法是用Pandas将现有的特征扩大几倍；我们也会研究一些更依靠数学的方法，并使用scikit-learn包的很多部分；我们还会编写自己的类。稍后真正写代码时会进行深入研究。

我们会探讨如下主题：
1. 检查数据集；
2. 填充分类特征；
3. 编码分类变量；
4. 扩展数值特征；
5. 针对文本的特征构建。

### 检查数据集
为了进行演示，本章会使用我们自己创建的数据集，以便展示不同的数据等级和类型。我们先设置数据的DataFrame。

用Pandas创建要使用的DataFrame，这也是Pandas的主要数据结构。这样做的优点是可以用很多属性和方法操作数据，从而对数据进行符合逻辑的操作，以深入了解我们使用的数据，以及如何最好地构建机器学习模型。

1. 我们用Pandas的`DataFrame`方法创建表格数据结构（带行和列的表格）。这个方法可以接受不同类型的数据（例如，NumPy数组和字典等）。在本例中，我们传入一个字典，其键是列标题、值是列表，每个列表代表一列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234321.png)

2. ![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234332.png)

观察每一列，并识别每列的类型和等级。
1. boolean（布尔值）：此列是二元分类数据（是/否），定类等级。
2. city（城市）：此列是分类数据，也是定类等级。
3. ordinal_column（顺序列）：顾名思义，此列是顺序数据，定序等级。
4. quantitative_column（定量列）：此列是整数，定比等级。

### 填充分类特征
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234508.png)

有3列存在缺失值。接下来当然要填充这些值。

你应该还记得，上一章实现了scikit-learn的`Imputer`类，用于填充数值数据。`Imputer`的确有一个`most_frequent`方法可以用在定性数据上，但是只能处理整数型的分类数据。

我们不一定想这样做，因为这种转换会改变我们对分类数据的解释方式。因此我们要写一个自己的转换器，也就是一个填充每列缺失值的方法。

实际上，本章将构建好几个自定义转换器。这些转换器对转换数据帮助很大，而且可以进行Pandas和scikit-learn不支持的操作。

我们先从定性列city开始。对于数值数据，可以通过计算均值的方法填充缺失值；而对于分类数据，我们也有类似的处理方法：计算出最常见的类别用于填充。

为此，需要找出city列中最常见的类别。

>注意，要对这个列使用`value_counts`方法。这样会返回一个对象，由高到低包含列中的各个元素——第一个元素就是最常出现的。

我们只需要对象中的第一个元素：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234704.png)

我们注意到，tokyo是最频繁出现的城市。知道了应该用哪个值来填充，就可以开始处理了。`fillna`函数可以指定填充缺失值的方式：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234724.png)

city列现在是这样的：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234811.png)

现在city列没有缺失值了。不过我们的另一个列boolean依然存在缺失值。我们不再使用同样的方法，而是构建一个自定义填充器，用来处理分类数据的填充。

#### 自定义填充器
在写代码之前，快速回顾一下机器学习流水线：
1. 我们可以用流水线按顺序应用转换和最终的预测器；
2. 流水线的中间步骤只能是转换，这意味着它们必须实现`fit`和`transform`方法；
3. 最终的预测器只需要实现`fit`方法。

流水线的目的是将几个可以交叉验证的步骤组装在一起，并设置不同的参数。在为每个需要填充的列构建好自定义转换器后，就可以把它们传入流水线，一口气转换好数据。

#### 自定义分类填充器
首先，用scikit-learn的`TransformerMixin`基类创建我们的自定义分类填充器。这个转换器（以及本章中的其他转换器）会作为流水线的一环，实现`fit`和`transform`方法。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235740.png)

1. 首先是一条`import`语句：
   
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235805.png)

2. 继承scikit-learn的`TransformerMixin`类，它包括一个`.fit_transform`方法，会调用我们创建的`.fit`和`.transform`方法。这能让我们的转换器和scikit-learn的转换器保持结构一致。我们初始化这个自定义的类：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235846.png)

3.  我们已经对这个自定义类进行了实例化，并用`__init__`方法对属性进行了初始化。在这里，只需要初始化一个实例属性`self.cols`（就是我们指定为参数的列）。现在可以构建`fit`和`transform`方法了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235932.png)

4.  上面是`transform`方法，它接收一个`DataFrame`。首先将这个`DataFrame`复制一份，命名为X。然后遍历`cols`参数指定的列，填充缺失值。`fillna`部分看起来很眼熟，因为这个函数在一开始的例子里用过。我们使用同样的函数并进行设置，这样一个填充器可以在很多列上同时工作。缺失值填充完毕后，返回`DataFrame`。然后是`fit`方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000605.png)

我们的`fit`方法只有`return self`一句话，和scikit-learn的标准`.fit`方法相同。

5. 我们在两列分类数据city和boolean上试验：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000642.png)

6. 我们初始化了一个自定义分类填充器，现在需要在数据集上调用`fit_transform`函数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000705.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000714.png)

我们的city和boolean列都没有缺失值了。不过定量列还是有缺失值。既然默认的填充器不能选择列，我们再来自定义一个。

#### 自定义定量填充器
我们使用的结构和自定义分类填充器类似。主要的区别在于，此处用scikit-learn的`Imputer`类实现一个自定义的转换器，对列进行转换：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001138.png)

对于`CustomQuantitativeImputer`，我们添加了一个`strategy`参数，指定如何填充定量数据里的缺失值。这里用均值填充缺失值，依然使用`transform`和`fit`方法。

还是用`fit_transform`函数填充数据，这次我们指定要填充的列和`strategy`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001227.png)

也可以不分别调用并用`fit_transform`拟合转换`CustomCategoryImputer`和`Custom-QuantitativeImputer`，而是把它们放在流水线中。方法如下所示。

1. 写`import`语句：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001336.png)

2. 导入自定义填充器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001354.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001402.png)

### 编码分类变量
我们目前已经填充了数据集——包括定量列和定性列。你可能在想：如何让机器学习算法利用分类数据呢？

需要将分类数据转换为数值数据。到目前为止，我们已经用最常见的类别对缺失值进行了填充。现在需要进行进一步操作。

任何机器学习算法，无论是线性回归还是利用欧几里得距离的KNN算法，需要的输入特征都必须是数值。有几种办法可以将分类数据转换为数值数据。

#### 定类等级的编码
我们从定类等级开始。主要方法是将分类数据转换为虚拟变量（dummy variable），有两种选择：

1. 用Pandas自动找到分类变量并进行编码；
2. 创建自定义虚拟变量编码器，在流水线中工作。

虚拟变量的取值是1或0，代表某个类别的有无。虚拟变量是定性数据的代理，或者说是数值的替代。

考虑一个简单的工资回归分析问题。假设给定了性别（定性数据）和工龄（定量数据）。为了考察性别对工资的影响，我们用虚拟变量：female = 0代表男性，female = 1代表女性。

当使用虚拟变量时，需要小心虚拟变量陷阱。虚拟变量陷阱的意思是，自变量有多重共线性或高度相关。简单地说，这些变量能依据彼此来预测。在这个例子中，如果设置female和male两个虚拟变量，它们都可以取值为1或0，那么就出现了重复的类别，陷入了虚拟变量陷阱。我们可以直接推断female = 0代表男性。

为了避免虚拟变量陷阱，我们需要忽略一个常量或者虚拟类别。被忽略的虚拟变量可以作为基础类别，和其他变量进行比较。

回到数据集中，用第一种选择将分类数据编码成虚拟变量。Pandas有个很方便的`get_dummies`方法，可以找到所有的分类变量，并将其转换为虚拟变量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002515.png)

我们必须指定需要应用虚拟化的列，因为Pandas也会编码定序等级的列，这就没有意义了。稍后会提及为什么这种操作没有意义。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002615.png)

另一种选择是创建一个自定义虚拟化器，从而在流水线中一口气转换整个数据集。

再次使用之前两个自定义填充器的结构。在这里，我们的`transform`方法会利用`Pandas`的`get_dummies`方法，为指定的列创建虚拟变量。该自定义虚拟化器中唯一的参数是`cols`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002657.png)

我们的自定义虚拟化器模仿了scikit-learn的`OneHotEncoding`，但是可以在整个`DataFrame`上运行。

最后实例化自定义虚拟化器，保证后面的代码可以运行。运行如下代码：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002737.png)

#### 定序等级的编码
现在我们关注定序等级的列。这个等级上仍然存在有用的信息，然而我们需要将字符串转换为数值数据。在定序等级，由于数据的顺序有含义，使用虚拟变量是没有意义的。为了保持顺序，我们使用标签编码器。

标签编码器是指，顺序数据的每个标签都会有一个相关数值。在我们的例子中，这意味着顺序列的值（dislike、somewhat like和like）会用0、1、2来表示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508111224.png)

这里创建了一个列表，用于对标签排序。这一步是关键，我们会用其索引将标签转换为数值数据。

在列上实现一个`map`函数，允许我们指定需要在列上实现的函数。我们指定该函数使用`lambda`匿名函数，即不绑定到某个名称：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508113435.png)

这行代码会创建一个函数，将列表的索引ordering分配到各个元素上。现在将其映射到顺序列上：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508113450.png)

顺序列现在变成了带标签的数据。

注意，我们没有使用scikit-learn的`LabelEncoder`，因为这个方法不能像上面的代码那样对顺序进行编码（0表示dislike，1表示somewhat like，2表示like）。它默认是一个排序方法，而我们不想这么做。

还是将自定义标签编码器放进流水线中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508113613.png)

我们保留了之前其他自定义转换器的结构。此处用上面详述的`map`和`lambda`函数对特定的列进行转换。注意，关键参数是`ordering`，它会指定将标签编码成什么数值。

调用我们的自定义编码器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114620.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114657.png)

顺序列已经被编码了。到这里，我们已经转换了如下这些列。
1. boolean和city：虚拟变量编码。
2. ordinal_column：标签编码。

#### 将连续特征分箱
有时，如果数值数据是连续的，那么将其转换为分类变量可能是有意义的。例如你的手上有年龄，但是年龄段可能会更有用。

Pandas有一个有用的函数叫作cut，可以将数据分箱（binning），亦称为分桶（bucketing）。意思就是，它会创建数据的范围。

我们在quantitative_column列上看看它的作用：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114843.png)

对于我们的定量列，cut函数的输出如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114924.png)

当指定的bins为整数的时候（bins = 3），会定义X范围内的等宽分箱数。然而在本例中，X的范围向两边分别扩展了0.1%，以包括最小值和最大值。

也可以将标签设置为`False`，这将只返回分箱的整数指示器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115045.png)

quantitative_column列的整数指示器如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115059.png)

利用`cut`函数的属性，可以为流水线定义自己的`CustomCutter`。再次仿照之前转换器的结构。我们的`transform`方法会利用`cut`，所以需要bins和labels作为参数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115158.png)

注意，labels的默认值是`False`。我们可以初始化`CustomCutter`，输入需要转换的列和分箱数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115643.png)

经过`CustomCutter`转换后的`quantitative_column`列如下表中所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120039.png)

注意，现在quantitative_column列处于定序等级，不需要引入虚拟变量。

#### 创建流水线
回顾一下，我们对数据集里的列进行了以下这些转换。
1. boolean和city：虚拟变量编码。
2. ordinal_column：标签编码。
3. quantitative_column：分箱。

回顾一下，我们对数据集里的列进行了以下这些转换。
1. boolean和city：虚拟变量编码。
2. ordinal_column：标签编码。
3. quantitative_column：分箱。

既然已经转换了所有的列，就可以组装流水线了。我们从scikit-learn的`Pipeline`开始：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120421.png)

把每列的自定义转换器放在一起。我们流水线的顺序是：

1. 用`imputer`填充缺失值；
2. 用虚拟变量填充分类列；
3. 对ordinal_column进行编码；
4. 将quantitative_column分箱。

这样设置流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120521.png)

为了观察流水线对数据的完整转换，我们先看看尚未转换的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120541.png)

转换前的数据如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120557.png)

我们可以对流水线进行拟合：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120634.png)

创建流水线对象后，可以转换`DataFrame`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120651.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120700.png)

### 扩展数值特征
有多种办法可以从数值特征中创建扩展特征。之前我们研究了如何将连续的数值数据转换为顺序数据，现在开始进一步扩展数值特征。

#### 根据胸部加速度计识别动作的数据集
这个数据集来自佩戴在胸部的加速度计，它收集了15名参与者的7种动作。采样频率是52Hz，加速度计数据未校准。数据集按参与者划分，包含以下内容：
1. 序号；
2. x轴加速度；
3. y轴加速度；
4. z轴加速度；
5. 标签。

标签是数字，每个数字代表一种动作（activity），如下所示：

1. 在电脑前工作；
2. 站立、走路和上下楼梯；
3. 站立；
4. 走路；
5. 上下楼梯；
6. 与人边走边聊；
7. 站立着讲话。

我们开始研究数据。首先加载CSV文件，并设置每列标题：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121432.png)

用`.head`方法查看前几行。如果没有特殊设置，默认查看前5行。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121453.png)

这个数据集的目的是训练模型，以便根据智能手机等设备上加速度计的x、y、z读数识别用户的当前动作。根据上述网站可知，activity列的数字有如下意义。

1. 1：在电脑前工作
2. 2：站立、走路和上下楼梯
3. 3：站立
4. 4：走路
5. 5：上下楼梯
6. 6：与人边走边聊
7. 7：站立着讲话

我们的目标是预测activity列。首先确定要击败的空准确率。调用`value_counts`方法，将`normalize`选项设为`True`，以百分比的形式列出最常见的动作：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121635.png)

空准确率是51.54%，意味着如果我们猜7（站立着讲话），正确率就超过一半了。现在开始进行机器学习，一步步建立模型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121659.png)

你可能已经熟悉上一章中用过的这些语句了。我们还是用scikit-learn的KNN分类模型。依旧采用网格搜索模块，自动找到最适合数据的KNN参数组合，以达到最佳的交叉验证准确率。然后，为预测模型创建一个特征矩阵（X）和一个响应变量（y）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121910.png)

设定好X和y之后，就可以引入网格搜索所需的变量和实例了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121926.png)

然后，我们实例化一个KNN模型和一个网格搜索模块，并且用特征矩阵和响应变量拟合：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508122644.png)

现在可以打印出最佳准确率和学习到的参数了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508122658.png)

使用5个邻居作为参数时，KNN模型准确率达到了72.08%，比51.54%的空准确率高得多。也许还有别的办法可以进一步提高准确率。

#### 多项式特征
在处理数值数据、创建更多特征时，一个关键方法是使用scikit-learn的Polynomial-Features类。这个构造函数会创建新的列，它们是原有列的乘积，用于捕获特征交互。

更具体地说，这个类会生成一个新的特征矩阵，里面是原始数据各个特征的多项式组合，阶数小于或等于指定的阶数。意思是，如果输入是二维的，例如[a, b]，那么二阶的多项式特征就是[1, a, b, a^2, ab, b^2]。

##### 参数
在实例化多项式特征时，需要了解3个参数：
1. degree
2. interaction_only
3. include_bias

degree是多项式特征的阶数，默认值是2。

interaction_only是布尔值：如果为真，表示只生成互相影响/交互的特征，也就是不同阶数特征的乘积。interaction_only默认为false。

include_bias也是布尔值：如果为真（默认），会生成一列阶数为0的偏差列，也就是说列中全是数字1。

我们先导入这个多项式特征类，并设置参数来实例化。首先看看将`interaction_only`设成`False`时的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132326.png)

然后调用`fit_transform`函数，拟合多项式特征，并观察扩展后数据集的形状：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132656.png)

把数据放进`DataFrame`，将列标题设置为feature_names，查看前几行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132724.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132735.png)

##### 探索性数据分析
现在可以进行一些探索性数据分析了。因为多项式特征的目的是更好地理解原始数据的特征交互情况，所以最好的可视化办法是关联热图。

Matplotlib和Seaborn都是流行的数据可视化工具。我们可以用如下方法创建关联热图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132826.png)

`.corr`是一个可以在`DataFrame`上的调用的函数，返回相关性矩阵。我们看看特征交互情况，如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152327.png)

目前，interaction_only参数是`False`。我们不重新建立变量，将参数设成`True`试试。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152520.png)

矩阵有162501行和6列。仔细观察一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152620.png)

`DataFrame`如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152637.png)

因为`interaction_only`为真，x0^2、x1^2和x2^2都消失了，因为这几列不和其他列交互。我们看看相关性矩阵：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152658.png)

可以看见特征是如何互相影响的。我们还可以用新的多项式特征对KNN模型进行网格搜索，这也可以在流水线中进行。

1. 先设置流水线参数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152756.png)

2. 然后实例化流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152810.png)

3. 最后设置网格搜索，打印最佳准确率和学习到的参数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152902.png)

现在的准确率是72.12%，比之前不用多项式扩展的准确率有所提高。

### 针对文本的特征构建
到目前为止，我们一直在处理分类数据和数值数据。虽然分类数据是字符串，但是里面的文本仅仅是某个类别。我们现在进一步探索更长的文本数据。这种文本数据比单个类别的文本复杂得多，因为长文本包括一系列类别，或称为词项（token）。

在进一步研究之前，我们要保证对文本有了充分的理解。考虑一下商户点评服务：用户在平台上撰写对餐厅和商家的评论，分享对自己体验的看法。这些评论都是文本格式的，包含可用于机器学习的大量有用信息，例如预测最应该去的餐厅。

总体来说，在当今世界中，我们沟通方式的很大一部分还是基于书面文本，无论使用的是聊天服务、社会媒体，还是电子邮件。通过建模，我们可以从中获得海量信息，例如用Twitter数据进行情绪分析。

这种工作叫作自然语言处理（NLP，natural language processing）。这个领域主要涉及计算机与人类的交流，特别是对计算机进行编程，以处理自然语言。

之前提到过，所有的机器学习模型都需要数值输入。因此处理文本时需要有创造性，有策略地思考如何将文本数据转换为数值特征。有几个办法可以做到，我们开始学习吧。

#### 词袋法
scikit-learn有一个`feature_extraction`模块，非常方便。顾名思义，它能以机器学习算法支持的方法提取数据的特征，包括文本数据。这个模块包括处理文本时需要使用的一些方法。

接下来，我们可能会将文本数据称为语料库（corpus），尤其是指文本内容或文档的集合。

将语料库转换为数值表示（也就是向量化）的常见方法是词袋（bag of words），其背后的基本思想是：通过单词的出现来描述文档，完全忽略单词在文档中的位置。在它最简单的形式中，用一个袋子表示文本，不考虑语法和词序，并将这个袋子视作一个集合，其中重复度高的单词更重要。词袋的3个步骤是：

1. 分词（tokenizing）；
2. 计数（counting）；
3. 归一化（normalizing）。

首先介绍分词。分词过程是用空白和标点将单词分开，将其变为词项。每个可能出现的词项都有一个整数ID。然后是计数。简单地计算文档中词项的出现次数。最后是归一化。将词项在大多数文档中的重要性按逆序排列。下面了解另外几个向量化方法。

#### `CountVectorizer`
`CountVectorizer`是将文本数据转换为其向量表示的最常用办法。和虚拟变量类似，`CountVectorizer`将文本列转换为矩阵，其中的列是词项，单元值是每个文档中每个词项的出现次数。这个矩阵叫文档-词矩阵（document-termmatrix），因为每行代表一个文档（在本例中是一条推文），每列代表一个词（一个单词）。

我们用一个新的数据集展示`CountVectorizer`的工作原理。Twitter情感分析数据集包括1578627条分类后的推文，每行标记为1或0：前者代表正面情绪，后者代表负面情绪。

我们用Pandas的`read_csv`方法读入数据。注意我们指定了可选参数`encoding`，这是为了保证所有的特殊字符都可以正常处理。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155134.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155150.png)

我们只关注Sentiment和SentimentText列，所以删除ItemID列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155213.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155225.png)

现在可以导入CountVectorizer，更好地理解这些文本：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155239.png)

然后设置X和y：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155317.png)

`CountVectorizer`和我们一直使用的自定义转换器非常类似，也有操作数据的`fit_trans-form`函数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155334.png)

用`CountVectorizer`转换后，数据有99989行和105849列。

`CountVectorizer`有很多参数，可以控制构建特征的数量。下面研究其中的一些，更好地了解如何构建特征。

##### `CountVectorizer`的参数
我们会介绍以下几个参数：
1. stop_words
2. min_df
3. max_df
4. ngram_range
5. analyzer

`stop_words`参数很常用。如果向其传入字符串`english`，那么CountVectorizer会使用内置的英语停用词列表。你也可以自定义停用词列表。这些词会从词项中删除，不会表示为特征。

例如：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160023.png)

可以看见，使用英语停用词后，特征列从105849下降到105545。停用词的意义在于消除特征的噪声，去掉在模型中意义不大的常用词。

另一个参数叫`min_df`。它通过忽略在文档中出现频率低于阈值的词，减少特征的数量。

使用`min_df`的CountVectorizer如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160117.png)

这极为有效地减少了特征数。

还有一个参数叫`max_df`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160146.png)

这类似于试图理解文档中有哪些停用词。

接下来看看`ngram_range`参数。这个参数接收一个元组，表示`n`值的范围（代表要提取的不同`n-gram`的数量）上下界。`n-gram`代表短语：若`n= 1`，则其是一个词项；若`n= 2`，则其代表相邻的两个词项。可以预想到，这个方法会显著地增加特征集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160313.png)

冒出来了3219557个特征。因为短语可能有其他含义，所以调整这个参数会对建模有帮助。

`CountVectorizer`还可以设置分析器作为参数，以判断特征是单词还是短语。默认是单词：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160451.png)

因为默认就是划分为单词，所以特征列结果变化不大。

我们甚至可以创建自定义分析器。理论上说，单词是由词根或词干构建而来的，所以可以据此写一个自己的分析器。

`CountVectorizer`是一个非常有用的工具，不仅可以扩展特征，还可以将文本转换为数值特征。

#### TF-IDF向量化器
TF-IDF向量化器由两部分组成：表示词频的TF部分，以及表示逆文档频率的IDF部分。TF-IDF是一个用于信息检索和聚类的词加权方法。

对于语料库中的文档，TF-IDF会给出其中单词的权重，表示重要性。我们把每个部分拆开来看。

1. TF（term frequency，词频）：衡量词在文档中出现的频率。由于文档的长度不同，词在长文中的出现次数有可能比在短文中出现的次数多得多。因此，一般会对词频进行归一化，用其除以文档长度或文档的总词数。
2. IDF（inverse document frequency，逆文档频率）：衡量词的重要性。在计算词频时，我们认为所有的词都同等重要。但是某些词（如is、of和that）有可能出现很多次，但这些词并不重要。因此，我们需要减少常见词的权重，加大稀有词的权重。

再次强调，`TfidfVectorizer`和`CountVectorizer`相同，都从词项构造了特征，但是`TfidfVectorizer`进一步将词项计数按照在语料库中出现的频率进行了归一化。我们看一个例子。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161339.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161349.png)

还是之前的代码，用`CountVectorizer`生成文档-词矩阵：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161533.png)

按此设置`TfidfVectorizer`：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161552.png)

可以看到，两个向量化器输出的行列数相同，但是里面的值不同。这是因为虽然`TfidfVectorizer`和`CountVectorizer`都可以把文本数据转换为定量数据，但是填充单元值的方法不同。

#### 在机器学习流水线中使用文本
当然，向量化器的最终目标都是让机器学习流水线理解文本数据。因为`CountVectorizer`和`TfidfVectorizer`与本书中的其他转换器一样，所以要使用scikit-learn流水线保证机器学习流水线的准确率和诚实度。本例要处理大量的列（数十万），所以我们使用在这种情况下更高效的分类器——朴素贝叶斯（naive Bayes）模型：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161739.png)

在开始构建流水线之前，取响应列的空准确率（0是负面情绪，1是正面情绪）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161754.png)

要让准确率超过56.5%。我们分两步创建流水线：

1. 用`CountVectorizer`将推文变成特征；
2. 用朴素贝叶斯模型`MultiNomialNB`进行正负面情绪的分类。

首先设置流水线的参数，然后实例化网格搜索：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161840.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161923.png)

结果是75.6%，很不错！现在进一步调优，加入`TfidfVectorizer`。这次我们尝试新的做法，比简单地利用TF-IDF建立流水线高级一些。scikit-learn有一个`FeatureUnion`模块，可以水平（并排）排列特征。这样，在一个流水线中可以使用多种类型的文本特征构建器。

例如，可以构建一个`featurizer`对象，在推文上使用`TfidfVectorizer`和`CountVecto-rizer`，并且并排排列推文（行数相同，增加列数）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162020.png)

然后可以看见数据的变化情况：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162110.png)

可以看到，结合两个特征构建器后的数据集行数相同，但是因为`TfidfVectorizer`和`CountVectorizer`并排，所以列数加倍。这样做可以让机器学习模型同时从两组数据中学习。我们稍稍改变`featurizer`对象的参数，看看效果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162135.png)

我们建立一个更完整的流水线，包括两个向量化器的特征结合：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162152.png)

这比单独使用`CountVectorizer`好多了。值得注意的是，`CountVectorizer`的最佳`ngram_range`是(1, 2)，而`TfidfVectorizer`的是(1, 1)，代表单个词的出现没有2个单词的短语那么重要。

至此，我们知道以下方法可以让流水线更加复杂：

1. 对向量化器的几十个参数使用网格搜索；
2. 在流水线上添加步骤，例如多项式特征构造。

## 特征选择：对坏属性说不
本书过半，我们处理了十余个数据集，并学习了大量特征选择方法。作为数据科学家和机器学习工程师，我们可以在工作和生活中利用这些方法，以保证充分利用预测模型。目前为止，在处理数据方面，我们已经使用了如下方法。
1. 特征理解：理解数据的等级。
2. 特征增强：填充缺失值。
3. 特征标准化和正则化。

每个方法在数据流水线中都有一席之地，更常见的现象是，我们会串联两个或多个处理方法。

本书剩余部分将着重介绍特征工程中更涉及数学、更加复杂的其他一些方法。随着工作流程的增长，我们会尽量不去讲解每个统计测试的内部原理，而是关注大局，让你理解测试要达到的目标。

讨论特征时经常遇到噪声问题。通常，我们手上的特征有可能预测性不高，有时甚至会阻碍模型的预测性能。我们使用过标准化和正则化等方法来减轻其危害，但是总有一天需要解决这种问题。

本章会讨论特征工程的一个子集，称为特征选择。特征选择是从原始数据中选择对于预测流水线而言最好的特征的过程。更正式地说，给定`n`个特征，我们搜索其中包括`k`（`k＜ n`）个特征的子集来改善机器学习流水线的性能。一般来说，我们的意思是：

特征选择尝试剔除数据中的噪声。

这个定义包括两个需要解决的问题：
1. 找到k特征子集的办法；
2. 在机器学习中对“更好”的定义。

本章的大部分内容着重讲解寻找这类子集的方法，及其工作原理的基础。本章将特征选择的方法分为两大类：基于统计的特征选择，以及基于模型的特征选择。这种分类也许不能100%捕捉到特征选择在科学性和艺术性上的复杂程度，但是可以推动机器学习流水线输出真实、可应用的结果。

### 在特征工程中实现更好的性能
在本书中，当我们讨论特征工程的方法时，需要对“更好”下定义。实际上，我们的目标是实现更好的预测性能，而且仅使用简单的指标进行测量，例如分类任务的准确率和回归任务的均方根误差（大部分是准确率）。我们还可以测量和跟踪其他指标，以评估预测的性能。例如，分类任务可以使用如下指标：

1. 真阳性率和假阳性率；
2. 灵敏度（真阳性率）和特异性；
3. 假阴性率和假阳性率。

回归任务则可以使用：

1. 平均绝对误差；
2. R2。

这个列表还可以继续延长。虽然我们不会放弃用以上指标量化性能的想法，但是也可以测量其他元指标。元指标是指不直接与模型预测性能相关的指标，它们试图衡量周遭的性能，包括：

1. 模型拟合/训练所需的时间；
2. 拟合后的模型预测新实例的时间；
3. 需要持久化（永久保存）的数据大小。

这补充了更好的定义，因为这些指标在预测性能之外涵盖了机器学习流水线的更多方面。为了跟踪这些指标，我们可以创建一个函数，通用到足以评估若干模型，同时精细到可以提供每个模型的指标。我们会利用`get_best_model_and_accuracy`函数，完成以下任务：

1. 搜索所有给定的参数，优化机器学习流水线；
2. 输出有助于评估流水线质量的指标

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508163736.png)

这个函数的总体目标是给出一个基线数据，因为我们会用这个函数评估每个特征选择方法，带来一种标准化的感觉。虽然本质上和之前的工作没什么区别，但是这次把工作形式化成函数，而且用另外的指标为机器学习流水线和特征选择模块打分，而不是只看准确率。

#### 案例分析：信用卡逾期数据集
特征选择算法可以智能地从数据中提取最重要的信号并忽略噪声，达到以下两个结果。

1. 提升模型性能：在删除冗余数据后，基于噪声和不相关数据做出错误决策的情况会减少，而且模型可以在重要的特征上练习，提高预测性能。
2. 减少训练和预测时间：因为拟合的数据更少，所以模型一般在拟合和训练上有速度提升，让流水线的整体速度更快。

为了更好地理解噪声以及为什么噪声有妨碍作用，我们介绍一个新的数据集：信用卡逾期数据集。我们使用23个特征和一个响应变量。这个变量是一个布尔值，可以是`True`（真）或`False`（假）。我们想知道，能否在23个特征中找出对机器学习流水线有帮助和有害的特征。用以下代码导入数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164655.png)

先导入两个常见模块numpy和pandas，并设置随机数种子，保持运行结果一致。然后，用以下代码导入数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164708.png)

先进行基本的探索性数据分析。检查一下数据集的大小，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164723.png)

数据有30000行（观察值）和24列（1个响应，23个特征）。我们先使用传统的统计方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164751.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164812.png)

default payment next month（下个月逾期）是响应，其他都是特征，或者说是潜在的预测变量。很明显，特征的尺度迥异，这会是我们选择数据处理方法和模型时需要考虑的因素。在前面的章节中，我们使用`StandardScalar`和归一化解决了这些问题，而本章会忽略这些问题，以便集中处理更相关的问题。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165003.png)

太好了，没有缺失值。我们在之后的案例分析中会再次处理缺失值，但是现在有更重要的事情要做。接下来为机器学习流水线设置变量，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165034.png)

和往常一样创建X和y变量。X矩阵有30000行和23列，而y是长度为30000的Pandas Series。因为要执行分类任务，所以取一个空准确率，确保机器学习的性能比基准更好。代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165055.png)

本例需要击败77.88%这个准确率，也就是没有逾期者的比例（0代表没有逾期）。

### 创建基准机器学习流水线
在前几章里，我们都提供了一个全章通用的机器学习模型。在本章中，我们会做一些工作，寻找最符合我们需求的机器学习模型，然后通过特征选择来增强模型。先导入4种模型：

1. 逻辑回归；
2. K最近邻（KNN）；
3. 决策树；
4. 随机森林。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165729.png)

导入后执行`get_best_model_and_accuracy`函数，取得每个模型处理原始数据的基准。需要先建立一些变量，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165753.png)

因为我们通过函数设置模型，而这会调用一个网格搜索模型，所以只需要创建空白模型即可，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165816.png)

我们在所有的模型上运行评估函数，了解一下效果的好坏。记住，我们要击败的精确率是0.7788，也就是基线空准确率。运行模型的代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508170321.png)

可以看见，逻辑回归只用原始数据就打败了空准确率。它拟合训练集平均需要0.6s，而只用20ms就可以得出结果。这其实是有道理的：要拟合数据，scikit-learn的逻辑回归需要在内存中创建一个巨大的矩阵，但是预测时只需要相乘并做一点标量计算。

我们在KNN上做同样的处理：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508170358.png)

不出所料，KNN在拟合时间上表现得更好。因为在拟合时，KNN只需要按方便检索和及时处理的方法存储数据。注意，这里的准确率甚至不如空准确率！你有可能在考虑原因是什么。如果你想到“等等，KNN是按照欧几里得距离进行预测的，在非标准数据上可能会失效，但是其他3个算法不会受此影响”，那么你是对的。

KNN是基于距离的模型，使用空间的紧密度衡量，假定所有的特征尺度相同，但是我们知道数据并不是这样。因此对于KNN，我们需要更复杂的流水线，以更准确地评估基准性能。代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508170800.png)

首先注意，在用`StandardScalar`进行z分数标准化处理后，这个流水线的准确率至少比空准确率要高，但是这也严重影响了预测时间，因为多了一个预处理步骤。目前，逻辑回归依然领先：准确率更高，速度更快。我们继续讨论两个基于树的模型，从更简单的决策树开始：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508171906.png)

真厉害！现在决策树的准确率是第一，而且拟合和预测的速度也很快。实际上，决策树的拟合速度比逻辑回归快，预测速度比KNN快。我们最后测试一下随机森林，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508171940.png)

比逻辑回归和KNN好得多，但是没有决策树好。我们汇总一下结果，看看应该使用哪个模型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508172004.png)

决策树的准确率最高，并且预测时间和逻辑回归并列第一，而带缩放的KNN拟合最快。总体而言，决策树应该是最适合下一步采用的模型，因为它在两个最重要的指标上领先：

1. 我们想要最高的准确率，以保证预测的准确性；
2. 考虑到实时生产环境，预测时间低大有裨益。

我们使用的办法是在选择特征之前选择模型。虽然不必要，但是在时间有限的情况下这样做一般很省时。你可以尝试多种模型，不必拘泥于一个模型。

既然知道了要使用决策树，那么：
1. 要击败的新基线准确率是0.8203，即拟合整个数据集的准确率；
2. 不再需要`StandardScaler`了，因为决策树不受其影响。

### 特征选择的类型
选择特征是为了提高预测能力，降低时间成本。所以这里介绍两种类型：基于统计和基于模型的特征选择。基于统计的特征选择很大程度上依赖于机器学习模型之外的统计测试，以便在流水线的训练阶段选择特征。基于模型的特征选择则依赖于一个预处理步骤，需要训练一个辅助的机器学习模型，并利用其预测能力来选择特征。

这两种类型都试图从原始特征中选择一个子集，减少数据大小，只留下预测能力最高的特征。我们可以依靠自己的智慧来选择特征，但实际上通过每种方法的例子对相应的流水线性能进行测量也很有效。

首先，我们研究如何依靠统计测试从数据集中选择可行的特征。

#### 基于统计的特征选择
通过统计数据，我们可以快速、简便地解释定量和定性数据。前几章使用了一些统计方法来获取关于数据的新知识和新看法，特别是我们认识到，均值和标准差是计算z分数和数据缩放的指标。本章会使用两个新概念帮我们选择特征：

1. 皮尔逊相关系数（Pearson correlations）；
2. 假设检验。

这两个方法都是单变量方法。意思是，如果为了提高机器学习流水线性能而每次选择单一特征以创建更好的数据集，这种方法最简便。

##### 使用皮尔逊相关系数

我们其实已经见过相关系数了，但并非用于特征选择。我们已经知道，可以这样计算相关系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174151.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174201.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174253.png)

皮尔逊相关系数（是Pandas默认的）会测量列之间的线性关系。该系数在-1～1变化，0代表没有线性关系。相关性接近-1或1代表线性关系很强。

值得注意的是，皮尔逊相关系数要求每列是正态分布的（我们没有这样假设）。在很大程度上，我们也可以忽略这个要求，因为数据集很大（超过500的阈值）

Pandas的`.corr()`方法会为所有的列计算皮尔逊相关系数。这个24×24的矩阵很难读，我们用热图优化一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174348.png)

注意，`heatmap`函数会自动选择最相关的特征进行展示。不过，我们目前关注特征和响应变量的相关性。我们假设，和响应变量越相关，特征就越有用。不太相关的特征应该没有什么用。

也可以用相关系数确定特征交互和冗余变量。发现并删除这些冗余变量是减少机器学习过拟合问题的一个关键方法。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174526.png)

最后一行可以忽略，因为这是响应变量和自己的相关性。我们寻找相关系数接近-1或1的特征，因为这些特征应该会对预测有用。我们用Pandas过滤出相关系数超过正负0.2的特征。

先定义一个Pandas mask作为过滤器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174603.png)

上面Pandas Series中的False代表特征的相关系数在-0.2～0.2，`True`则代表相关系数超过了正负0.2。我们用下面的代码结合这个mask：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174626.png)

highly_correlated_features变量会存储与响应变量高度相关的特征，但是需要删掉响应列的名称，因为在机器学习流水线中包括这列等于作弊：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174746.png)

留下原始数据集的5个特征，用于预测响应变量。我们尝试一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174802.png)

我们的准确率比要击败的准确率0.8203略差，但是拟合时间快了大概20倍。我们的模型只需要5个特征就可以学习整个数据集，而且速度快得多。

接下来回顾一下scikit-learn流水线，将相关性选择作为预处理阶段的一部分。我们需要创建一个自定义转换器调用刚才的逻辑，并封装为流水线可以使用的类。

将这个类命名为`CustomCorrelationChooser`，它会实现一个拟合逻辑和一个转换逻辑。

1. 拟合逻辑：从特征矩阵中选择相关性高于阈值的列。
2. 转换逻辑：对数据集取子集，只包含重要的列。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175644.png)

运行下面的代码试试新的相关特征选择器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175702.png)

这个类的选择和之前一样。我们在X矩阵上应用转换，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175728.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175735.png)

我们看见`transform`方法删除了其他列，只保留大于0.2阈值的列。现在在流水线中把一切组装起来：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175859.png)

哇！第一次的特征选择就已经打败了目标（虽然只高一点点）。我们的流水线显示，如果把阈值设为0.1，就足以消除噪声以提高准确性，并缩短拟合时间（之前是0.158s）。下面看看选择器保留了哪些列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508180322.png)

选择器保留了我们找到的5列，以及LIMIT_BAL和PAY_6列。这就是scikit-learn中自动化网格搜索的好处，让模型达到最优，解放我们的劳动力。

##### 使用假设检验
假设检验是一种统计学方法，可以对单个特征进行复杂的统计检验。在特征选择中使用假设检验可以像之前的自定义相关选择器一样，尝试从数据集中选择最佳特征，但是这里的检验更依赖于形式化的统计方法，并通过所谓的p值进行检验。

作为一种统计检验，假设检验用于在给定数据样本时确定可否在整个数据集上应用某种条件。假设检验的结果会告诉我们是否应该相信或拒绝假设（并选择另一个假设）。基于样本数据，假设检验会确定是否应拒绝零假设。我们通常会用p值（一个上限为1的非负小数，由显著性水平决定）得出结论。

在特征选择中，假设测试的原则是：“特征与响应变量没有关系”（零假设）为真还是假。我们需要在每个特征上进行检验，并决定其与响应变量是否有显著关系。在某种程度上说，我们的相关性检测逻辑也是这样运作的。我们的意思是，如果某个特征与响应变量的相关性太弱，那么认为“特征与响应变量没有关系”这个假设为真。如果相关系数足够强，那么拒绝该假设，认为特征与响应变量有关。

在将其用于数据之前，需要定义新模块`SelectKBest`和`f_classif`，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508183849.png)

`SelectKBest`基本上就是包装了一定数量的特征，而这些特征是根据某个标准保留的前几名。在这里，我们使用假设检验的p值作为排名依据。
###### 理解p值
p值是介于0和1的小数，代表在假设检验下，给定数据偶然出现的概率。简而言之，p值越低，拒绝零假设的概率越大。在我们的例子中，p值越低，这个特征与响应变量有关联的概率就越大，我们应该保留这个特征。

需要注意的是，`f_classif`函数在每个特征上单独（单变量测试由此得名）执行一次ANOVA测试（一种假设检验类型），并分配一个p值。`SelectKBest`会将特征按p值排列（越小越好），只保留我们指定的k个最佳特征。下面我们用Python试验一下。
###### p值排列
首先实例化一个`SelectKBest`模块。我们手动设定k是5，代表只希望保留5个最佳的特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508190924.png)

然后可以像之前使用自定义选择器那样，拟合并转化X矩阵，选择需要的特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508190939.png)

如果想直接查看p值并检查选择了哪些特征，可以深入观察`k_best`变量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191145.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191205.png)

可以看到，我们的选择器认为PAY_X是最重要的特征。观察p值，我们可以看见，这些特征的p值极小，几乎为0。p值的一个常见阈值是0.05，意思是可以认为p值小于0.05的特征是显著的。对于我们的测试，这些列是极其重要的。我们可以用Pandas的过滤方法，查看所有p值小于0.05的特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191246.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191259.png)

大部分特征的p值都很低，但并不是全部。用下面的代码看看哪些列的p_value较高：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191314.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191324.png)

有3个特征的p值较高。我们可以在流水线中应用`SelectKBest`，看看是否效果更好：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508191349.png)

`SelectKBest`模块和自定义转换器的准确率差不多，但是快了一点。用下面的代码查看选择了哪些特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508194057.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508194106.png)

看起来和之前统计方法的选择相同。我们的统计方法有可能只是按顺序选了这7个特征。

在ANOVA之外，还有其他的测试能用于回归任务，例如卡方检验等。这些测试在scikit-learn的文档中有所涉及。

在开始基于模型的特征选择前，我们可以进行一次快速的完整性检查，以确保路线正确。到目前为止，为了取得最佳准确率，我们已经用了特征选择的两种统计方法，每次选择的7个特征都一样。如果选择这7个特征之外的所有特征呢？是不是流水线的准确率会下降，流水线会劣化？我们来确认一下。下面的代码会进行完整性测试：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508194211.png)

因此，如果不选择之前的7个特征，不仅准确性会变差（几乎和空准确率一样差），而且拟合时间也会变慢。现在我们可以继续了解下一种特征选择方法了——基于模型的方法。

#### 基于模型的特征选择
##### 再议自然语言处理
`CountVectorizer`有很多参数，在搜索最佳流水线时可以调整。具体来说，有以下几个内置的特征选择参数。

1. `max_features`：整数，设置特征构建器可以记忆的最多特征数量。要记忆的特征是由一个排名系统决定的，它依照词项在语料库中的出现次数进行排序。
2. `min_df`：浮点数，为词项在语料库中出现的频率设定下限；如果低于该值，则不进行标记。
3. `max_df`：浮点数，和min_df类似，设定词项的频率上限。
4. `stop_words`：按照内置静态列表对词项类型进行限制。如果词项在stop_words中，那么即使频率在min_df和max_df允许的范围内，也会被省略。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508194531.png)

我们先创建一个特征和一个响应变量。回忆一下，因为我们处理的是文本，所以特征变量是文本列，而不是二维矩阵：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508194601.png)

可以建立流水线，并用本章使用过的函数进行评估，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508194620.png)

数据不错（空准确率是0.564），但是上一章使用`FeatureUnion`模块来组合`TfidfVectorizer`和`CountVectorizer`，比这次的分数还高。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508195026.png)

看起来`SelectKBest`对于文本数据效果不好。如果没有`FeatureUnion`，我们不能达到上一章的准确率。值得注意的是，无论使用何种方式，拟合和预测的时间都很长：这是因为统计单变量方法在大量特征（例如从文本向量化中获取的特征）上表现不佳。

##### 使用机器学习选择特征
在文本处理中，`CountVectorizer`内置的特征选择工具表现不错，但是一般处理的是已经有行列结构的数据。我们已经看到了基于纯统计方法的特征选择非常强大，现在研究如何使用这些方法让机器学习变得更好。本节主要使用的两类模型是基于树的模型和线性模型。这两类模型都有特征排列的功能，在对特征划分子集时很有用。

在进一步研究前，我们需要强调，虽然这些方法的逻辑并不相同，但是目标一致：找到最佳的特征子集，优化机器学习流水线。我们要介绍的第一个方法会涉及拟合训练数据时算法（例如训练决策树和随机森林）内部指标的重要性。

###### 特征选择指标——针对基于树的模型
在拟合决策树时，决策树会从根节点开始，在每个节点处贪婪地选择最优分割，优化节点纯净度指标。默认情况下，scikit-learn每步都会优化基尼指数（gini metric）。每次分割时，模型会记录每个分割对整体优化目标的帮助。因此，在树形结构中，这些指标对特征重要性有作用。

为了进一步说明，我们拟合一个决策树，并输出特征重要性，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508195612.png)

拟合后，可以用`feature_importances_`属性展示特征对于拟合树的重要性：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508195642.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508195720.png)

上表显示，拟合中最重要的特征是PAY_0，和上一章统计模型的结果相匹配。更值得注意的是第2、第3和第5个特征，这3个特征在进行统计测试前没有显示出重要性。这意味着，这种特征选择方法有可能带来一些新的结果。

回想一下，之前我们使用scikit-learn内置的包装器`SelectKBest`，基于排序函数（例如ANOVA的p值）取前k个特征。下面会引入一个新的包装器`SelectFromModel`，和`SelectKBest`一样选取最重要的前k个特征。但是，它会使用机器学习模型的内部指标来评估特征的重要性，不使用统计测试的p值。我们用下面的代码定义`SelectFromModel`:

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508230737.png)

`SelectFromModel`和`SelectKBest`相比最大的不同之处在于不使用k（需要保留的特征数）：`SelectFromModel`使用阈值，代表重要性的最低限度。通过这种方式，这种基于模型的选择器可以脱离人工筛选的过程，只保留与流水线所需同等数量的特征。我们实例化这个类:

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508231606.png)

然后在`SelectFromModel`上拟合数据，调用`transform`方法，观察数据选择后的子集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508231633.png)

了解了模块的基本原理后，我们可以在流水线中应用选择功能。我们需要击败的准确率是0.8206，之前的相关性选择器和ANOVA都得到了这个准确率（因为选择的特征相同）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508231738.png)

首先注意，我们可以用一些保留字作为阈值参数的一部分，并不是必须选择表示最低重要性的浮点数。例如，mean的阈值只选择比均值更重要的特征，median的阈值只选择比中位数更重要的特征。我们还可以用这些保留字的倍数，例如2.*mean代表比均值重要两倍的特征。

现在查看基于决策树的选择器选出了哪些特征，可以使用`SelectFromModel`的`get_support()`方法。这个方法会返回一个数组，其中的每个布尔值代表一个特征，从而告诉我们保留了哪些特征，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508232024.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508232032.png)

这棵树选择了除了两个特征外的所有其他特征，但是和什么都不选的树性能没什么区别。

##### 线性模型和正则化
`SelectFromModel`可以处理任何包括`feature_importances_`或`coef_`属性的机器学习模型。基于树的模型会暴露前者，线性模型则会暴露后者。在拟合后，线性回归、逻辑回归、支持向量机（SVM，support vector machine）等线性模型会将一个系数放在特征的斜率（重要性）前面。`SelectFromModel`会认为这个系数等同于重要性，并根据拟合时的系数选择特征。

然而，在使用模型之前，我们需要引入正则化的概念，以选择真正有用的特征。

###### 正则化简介
在线性模型中，正则化是一种对模型施加额外约束的方法，目的是防止过拟合，并改进数据泛化能力。正则化通过对需要优化的损失函数添加额外的条件来完成，意味着在拟合时，正则化的线性模型有可能严重减少甚至损坏特征。有两种广泛使用的正则化方法：L1和L2正则化。这两种技术都基于向量的$L_{p}$范数.

 L1正则化也称为lasso正则化，会使用L1范数将向量条目绝对值的和加以限制，使系数可以完全消失。如果系数降为0，那么这个特征在预测时就没有任何意义，而且肯定不会被`SelectFromModel`选择。

L2正则化也称为岭正则化，施加惩罚L2范数（向量条目的平方和），让系数不会变成0，但是会非常小。

正则化也有助于解决多重共线性的问题，也就是说，数据中有多个线性相关的特征。L1惩罚可以强制其他线性相关特征的系数为0，保证选择器不会选择这些线性相关的特征，有助于解决过拟合问题。

###### 另一个特征重要性指标：线性模型参数
和之前用树的方法一样，我们可以用L1和L2正则化为特征选择寻找最佳系数。我们用逻辑回归模型作为选择器，在L1和L2范数上进行网格搜索：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508232619.png)

现在的准确率终于超过统计测试选择器了。再次使用`SelectFromModel`的`get_support()`方法，列出选择的特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508234001.png)

非常好！基于逻辑回归的选择器选择了大部分的PAY_X特征，也发现了性别、教育和婚姻状况可以帮助预测。接下来用`SelectFromModel`模块在支持向量机分类器上进行测试。

支持向量机是分类模型，在空间中绘制线性边界，对二分数据进行分类。这些线性边界叫作支持向量。目前看来，逻辑回归分类器和支持向量分类器（SVC）的最大区别在于，后者会最大优化二分类项目的准确性，而前者对属性的建模更好。像之前决策树和逻辑回归一样，我们用scikit-learn实现一个线性SVC模型，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508234051.png)

太棒了！SVC达到了最高的准确率。可以看见拟合时间受到了影响，但是如果能把最快的预测和最好的准确率结合，那么机器学习流水线就会很出色了：基于SVC，利用正则化为决策树分类器找到最佳特征。下面看看选择器选择了哪些特征来达到目前的最佳准确率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508234132.png)

与逻辑回归比，唯一的区别是PAY_4特征。但是可以看到，移除单个特征不会影响流水线的性能。

### 选用正确的特征选择方法
现在你有可能感到本章的信息过多，难以消化。我们演示了几种选择特征的方法，其中一部分基于统计学，另一部分基于机器学习模型的二次输出。一个很自然的问题是：应该如何选用特征选择方法？理论上说，最理想的状况是，你可以像本章这样多次尝试，但我们知道这样是不可行的。下面是一些经验，可以在判断特征选择方法的优劣时参考。

1. 如果特征是分类的，那么从`SelectKBest`开始，用卡方或基于树的选择器。
2. 如果特征基本是定量的（例如本例），用线性模型和基于相关性的选择器一般效果更好。
3. 如果是二元分类问题，考虑使用`SelectFromModel`和`SVC`，因为`SVC`会查找优化二元分类任务的系数。
4. 在手动选择前，探索性数据分析会很有益处。不能低估领域知识的重要性。

## 特征转换：数学显神通
到目前为止，我们似乎已经从数据的所有角度应用了特征工程工具。从通过分析表格数据以确定数据的等级，到通过统计方法构建并选择列以优化机器学习流水线，我们为处理数据中的特征做了很多了不起的事。

再次提醒：有很多方法可以增强机器学习的效果。我们通常认为，最主要的两个特征是准确率和预测/拟合时间。这意味着，如果利用特征工程工具后，机器学习流水线的准确率在交叉验证中有所提高，或者拟合/预测的速度加快，那就代表特征工程成功了。当然，我们的终极目标是既优化准确率又优化时间，构建出更好的流水线。

在前面的5章中，我们了解了所谓的经典特征工程。目前，我们已经讨论了特征工程的5个主要类别/步骤。

1. 探索性数据分析：在应用机器学习流水线，甚至在使用机器学习算法或特征工程工具之前，我们理应对数据集进行一些基本的描述性统计，并进行可视化操作，以便更好地理解数据的性质。
2. 特征理解：在了解了数据的大小和形状后，应该进一步仔细观察数据集的每一列（如果有可能的话）和大致特点，包括数据的等级，因为这会决定如何清洗数据。
3. 特征增强：这个阶段是关于改变数据值和列的，我们根据数据的等级填充缺失值，并按需执行虚拟变量转换和缩放操作。
4. 特征构建：在拥有可以得到的最好数据集之后，可以考虑构建新的列，以便理解特征交互情况。
5. 特征选择：在选择阶段，用所有原始和新构建的列进行（通常是单变量）统计测试，选取性能最佳的特征，以消除噪声影响、加速计算。

下图总结了这个过程，并展示了其中的每个步骤。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509003242.png)

上图举例说明了一个使用上述方法的机器学习流水线，包括5个主要步骤：分析、理解、增强、构建和选择。在随后的章节中，我们会重点介绍一种转换数据的新方法，与之前的体系有所不同。

读到这里，读者应该可以对现实世界数据集的性能有合理的信心和期望。本章和第7章会专注于特征工程的两个子集，它们依赖大量的编程和数学方法，特别是线性代数。和之前一样，我们会尽力解释用到的所有代码，只在必要时对数学进行解释。

本章会涉及特征转换，这是一套改变数据内部结构的算法，以产生数学上更优的超级列（super-column）。下一章则重点介绍使用非参数方法（不依赖数据的形状）的特征学习，以自动学习新的特征。本书的最后一章是几个经过细致研究的案例，旨在展示特征工程的端到端过程，以及特征工程对机器学习流水线的影响。

我们首先讨论特征转换。上面说到，特征转换是一组矩阵算法，会在结构上改变数据，产生本质上全新的数据矩阵。其基本思想是，数据集的原始特征是数据点的描述符/特点，也应该能创造一组新的特征，用更少的列来解释数据点，并且效果不变，甚至更好。

想象一个简单的长方形房间。房间是空的，只在中间有一个人体模型。人体模型永远不会移动，而且永远面向一个方向。你的任务是全天候监控这个房间。你会想到在房间里安装摄像头，确保房内的所有活动都被捕获并记录下来。可以把摄像头装在房间角落，对准人体模型的面部，并且尽可能包括房间的大部分区域。用一台摄像头就基本上可以看见整个房间的所有区域。不过问题是摄像头有盲区，比如看不到摄像头的正下方（物理缺陷），而且看不见人体模型的后面（视野被遮挡）。那么，可以在人体模型后面再放一个摄像头，弥补第一个摄像头的盲区。坐在监控室里面，用两个摄像头可以看见房间内99%以上的区域。

在这个例子中，房间表示数据的原始特征空间，人体模型表示特征空间特定区域上的数据点。正式地说，你是在思考一个带有单一数据点的三维特征空间：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509003359.png)

用单个摄像头捕获数据点时，就像把数据集压入一个维度，即该摄像头看见的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509003413.png)

但是只用一个维度很可能无法充分描述数据，因为一个摄像头有盲区。增加一个摄像头：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509003425.png)

这两个摄像头就是特征转换形成的新维度，以一种新的方式捕获数据，但是只需要两列而非三列就可以提供足够的信息。在特征转换中，最棘手的部分是一开始就不认为原始特征空间是最好的。我们需要接受一个事实：可能有其他的数学坐标轴和系统能用更少的特征描述数据，甚至可以描述得更好。

### 维度缩减：特征转换、特征选择与特征构建
刚才，我们提到了如何压缩数据集，用全新的方法以更少的列描述数据。听起来和特征选择的概念很类似：从原始数据集中删除列，通过消除噪声和增强信号列来创建一个不同而且更好的数据集。虽然特征选择和特征转换都是降维的好办法，但是它们的方法迥然不同。

特征选择仅限于从原始列中选择特征；特征转换算法则将原始列组合起来，从而创建可以更好地描述数据的特征。因此，特征选择的降维原理是隔离信号列和忽略噪声列。

特征转换方法使用原始数据集的隐藏结构创建新的列，生成一个全新的数据集，结构与之前不同。这些算法创建的新特征非常强大，只需要几个就可以准确地解释整个数据集。

特征转换的原理是生成可以捕获数据本质的新特征，这一点和特征构造的本质类似：都是创建新特征，捕捉数据的潜在结构。需要注意，这两个不同的过程方法截然不同，但是结果类似。

特征构造用几个列之间的简单操作（加法和乘法等）构造新的列。意思是，经典特征构造过程构造出的任何特征都只能用原始数据集中的几个列生成。如果我们的目标是创造足够多的特征，捕获所有可能的特征交互，那么也许会生成大量额外的列。例如，如果数据集有1000个甚至更多特征，我们要捕获所有特征交互的一个子集，就需要几万个列来构建足够多的特征。

特征转换方法可以用每个列中的一点点特征创建超级列，所以不需要创建很多新特征就可以捕获所有潜在的特征交互。因为特征转换算法涉及矩阵和线性代数，所以不会创造出比原有列更多的列，而且仍能提取出原始列中的结构。

特征转换算法可以选择最佳的列，将其与几个全新的列进行组合，从而构建新的特征。我们可以认为，特征转换在本书中最强大的算法之列。下面介绍首先要用到的算法和数据集：主成分分析和鸢尾花数据集。

### 主成分分析
主成分分析（PCA，principal components analysis）是将有多个相关特征的数据集投影到相关特征较少的坐标系上。这些新的、不相关的特征（之前称为超级列）叫主成分。主成分能替代原始特征空间的坐标系，需要的特征少、捕捉的变化多。在摄像头的例子中，主成分就是摄像头本身。

换句话说，PCA的目标是识别数据集中的模式和潜在结构，以创建新的特征，而非使用原始特征。和特征选择类似，如果原始数据是n×d的（n是观察值数，d是原始的特征数），那么我们会将这个数据集投影到n×k（k＜d）的矩阵上。

主成分会产生新的特征，最大化数据的方差。这样，每个特征都会解释数据的形状。主成分按可以解释的方差来排序，第一个主成分最能解释数据的方差，第二个其次。我们希望用尽可能多的成分来优化机器学习任务，无论是监督学习还是无监督学习。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509004742.png)

PCA本身是无监督任务，意思是PCA不使用响应列进行投影/转换。这点很重要，因为我们的第二个特征转换算法是有监督的，而且会利用响应变量来用不同的方式创建超级列，优化预测任务。

#### PCA的工作原理
PCA利用了协方差矩阵的特征值分解。PCA的数学原理首次发表于20世纪30年代，涉及一点多变量微积分和线性代数。出于本书的目的，我们跳过数学部分，直接看应用。

PCA也可以在相关矩阵上使用。如果特征的尺度类似，那么可以使用相关矩阵；尺度不同时，应该使用协方差矩阵。一般建议在缩放数据上使用协方差矩阵。

这个过程分为4步：
1. 创建数据集的协方差矩阵；
2. 计算协方差矩阵的特征值；
3. 保留前k个特征值（按特征值降序排列）；
4. 用保留的特征向量转换新的数据点。

#### 鸢尾花数据集的PCA——手动处理
鸢尾花数据集（iris）有150行和4列。每行（观察值）代表一朵花，每列（特征）代表花的4种定量特点。数据集的目标是拟合一个分类器，尝试在给定4个特征后，在3种花中预测。花的类型分别是山鸢尾（setosa）、变色鸢尾（versicolor）和维吉尼亚鸢尾（virginica）。

1.  我们先加载模块，将数据集存储到变量iris：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509005049.png)

2. 然后将数据矩阵和响应变量存储到iris_X和iris_y中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509005104.png)

3. 先看一下要预测的花的名称：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509005118.png)

4. 除了花的名称，我们还可以查看用于预测的特征名称：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509005145.png)

5. 为了理解数据，我们写一点代码，查看一下其中的两个特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509005239.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509005251.png)

然后在数据集上执行PCA，获得主成分。回忆一下，这包括4个步骤。

##### 创建数据集的协方差矩阵
为了计算鸢尾花数据集的协方差矩阵，我们先计算特征的均值向量（后面使用），然后用NumPy计算协方差矩阵。

协方差矩阵是d×d的正方形矩阵（特征数与行数、列数均相等），表示特征间的交互。它和相关系数矩阵很相似：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509010041.png)

现在cov_mat变量是4×4的协方差矩阵。
##### 计算协方差矩阵的特征值
NumPy有一个方便的函数，可以计算特征向量和特征值，以获得鸢尾花数据集的主成分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509010355.png)

##### 按降序保留前k个特征值
我们有4个特征值，需要选择合适的数量进行保留。如果我们愿意，可以保留完整的4个，但是一般希望选择的比原始特征数更少。多少合适呢？虽然可以进行暴力搜索，但是我们的工具库中有一个新工具——碎石图（screeplot）。

碎石图是一种简单的折线图，显示每个主成分解释数据总方差的百分比。要绘制碎石图，需要对特征值进行降序排列，绘制每个主成分和之前所有主成分方差的和。在鸢尾花数据集上，我们的碎石图有4个点，每个点代表一个主成分。每个主成分解释了总方差的某个百分比，相加后，所有主成分应该解释了数据集中总方差的100%。

我们取每个特征向量（主成分）的特征值，将其除以所有特征值的和，计算每个特征向量解释方差的百分比：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509010657.png)

可以看到，4个主成分解释的部分有很大差异。作为单个特征（列），第一个主成分可以解释方差的92%以上。太惊人了！理论上，这个超级列可以完成4个原始列的绝大部分工作。

下面对碎石图进行可视化，图中的x轴上有4个主成分，y轴是累积方差。每个数据点代表到这个主成分为止可以解释的方差百分比：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509010722.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509010731.png)

上图告诉我们，前两个主成分就占了原始方差的近98%，意味着几乎可以只用前两个特征向量作为新的主成分。我们可以将数据集缩小一半（从4列缩小到2列），而且保持了特征的完整性、加速了性能。下面会进一步研究机器学习的例子，并对理论加以验证。

特征值分解总是会产生和特征一样多的特征向量。我们需要在计算完毕后选择希望使用的主成分数量。这表示PCA和本书中大部分算法一样是半监督的，需要一些人为输入。

##### 使用保留的特征向量转换新的数据点
在做出保留两个主成分的决定后（这个数字是靠网格搜索还是分析碎石图得到的无关紧要），我们必须能用这些主成分转换新的样本数据点。首先隔离这两个特征向量，存储在`top_2_eigenvectors`变量中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509011035.png)

数组代表了两个特征向量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509011736.png)

通过这些向量，我们可以将iris_X和top_2_eigenvectors两个矩阵相乘，将数据投影到改进后的超级数据集中。下图显示了这个过程。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509011843.png)

矩阵相乘后，我们将原始数据集投影到这个新的二维空间：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509011900.png)

这样就完成了。我们将四维的鸢尾花数据集转换成了只有两列的新矩阵，而这个新矩阵可以在机器学习流水线中代替原始数据集。

####  scikit-learn的PCA
1. 从scikit-learn的分解模块中导入：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509011946.png)

2. 为了模仿鸢尾花数据集的操作过程，我们实例化有两个组件的PCA对象：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509011959.png)

3. 现在可以用PCA拟合数据了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509012018.png)

4.  查看一下PCA对象的属性，看看是不是和手动计算的结果匹配。检查components_属性是不是和前面的top_2_eigenvectors变量匹配：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509012054.png)

5. 这两个主成分几乎完美匹配之前的top_2_eigenvectors变量。我们说“几乎”完美，是因为第二个主成分是之前计算值的负数。然而这在数学上而言没有问题，因为两个特征都100%有效，而且也是不相关的。

6. 到目前为止，这个过程比前面的简单得多。要完成这个过程，我们用PCA对象的transform方法，将数据投影到新的二维平面上：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509012227.png)

注意，这里投影后的数据和之前不同，因为scikit-learn的PCA会在预测阶段自动将数据中心化，从而改变结果。

7.  我们可以改变一行，模仿之前的效果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509012914.png)

8. 绘制鸢尾花数据集，比较一下投影到新坐标系之前和之后的样子：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509012935.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509012942.png)

在原始数据集中，我们可以在前两列的原始特征空间中看见这些鸢尾花。但是在投影后的空间内，各种花分离得更远，而且旋转了一点。数据聚类看上去更突出了。这是因为我们的主成分尽可能捕捉了数据的方差，并在图中展示。我们可以像手动的例子一样，提取每个主成分解释的方差量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509013012.png)

现在我们可以用scikit-learn的PCA实现所有基本功能了，下面用这些信息展示PCA的主要优点之一：消除相关特征。

本质上，在特征值分解时，得到的所有主成分都互相垂直，意思是彼此线性无关。

因为很多机器学习模型和预处理技术会假设输入的特征是互相独立的，所以消除相关特征好处很大。我们可以用PCA确保这一点。

为了说明，我们创建鸢尾花数据集的相关矩阵，找出特征间的平均线性相关参数。然后对PCA投影后的数据集执行同样的操作，并对值进行比较。我们认为，投影后数据集的平均相关系数应该更接近0，也就是说所有的特征都是线性独立的。

首先计算鸢尾花数据集的相关矩阵。

1. 这个矩阵是4×4的，每个值代表两个特征间的相关性系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509013648.png)

2. 我们提取对角线上的1，计算特征间的平均相关性：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014021.png)

3. 最后取数组的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014033.png)

4. 平均相关系数是0.16，虽然很小，但肯定不是0。我们做一个完整的PCA，提取所有主成分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014047.png)

5. 然后用老办法计算（应该是线性独立的）新列的平均相关系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014100.png)

投影到主成分的数据相关性极小，在机器学习中总体而言是有帮助的。

#### 中心化和缩放对PCA的影响
和前面用过的很多转换方法一样，特征的缩放对于转换往往极其重要。PCA也不例外。上面说到，scikit-learn的PCA在预测阶段会将数据进行中心化（centering），但为什么不是在拟合时进行？如果scikit-learn的PCA要在预测时添加一步数据中心化的操作，那为什么不在计算特征向量时就完成？我们的假设是：将数据中心化不会影响主成分。下面进行验证。

1. 导入scikit-learn的`StandardScaler`模块，对鸢尾花数据集进行中心化：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014704.png)

2.  查看一下中心化后的数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014723.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014731.png)

3. 用之前实例化的PCA类（n_components设为2）拟合中心化后的数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014750.png)

4. 之后调用PCA类的components_属性，和原始鸢尾花数据集的结果进行比较：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014806.png)

5. 看起来中心化后的主成分和之前的完全相同。为了明确解释，我们用PCA类对中心化后的数据进行转换，查看前5行是否和之前的投影一样：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509014838.png)

6. 结果是一样的！投影后的中心化数据和被解释的方差比例也匹配：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015033.png)

关于解释方差的百分比，我们可以这样看：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015056.png)

这是因为，原始矩阵和中心化后矩阵的协方差矩阵相同。如果两个矩阵的协方差矩阵相同，那么它们的特征值分解也相同。因此，scikit-learn的PCA不会对数据进行中心化，因为无论是否进行中心化操作，结果都一样。那么为什么要加上这个步骤呢？

我们观察一下，用标准z分数进行缩放时，主成分的变化程度：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015128.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015136.png)

在经过缩放的新数据上应用PCA模块，看一下主成分的变化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015322.png)

和之前的主成分不同，这次连数字也不一样。PCA不改变数据的尺度，所以数据的尺度会影响主成分。注意，这里缩放的意思是对数据进行中心化，并除以标准差。我们把数据集投影到新的主成分上，确定新的投影数据已经有了变化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015355.png)

最后，查看一下解释方差的百分比：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015409.png)

有意思。在特征工程或机器学习中，特征缩放一般来说都是好的，我们在大多数情况下会推荐这种操作。但是为什么第一个主成分解释方差的比例比之前低得多？

这是因为对数据进行缩放后，列与列的协方差会更加一致，而且每个主成分解释的方差会变得分散，而不是集中在一个主成分中。在实践和生产环境下，我们会建议进行缩放，但应该在缩放和未缩放的数据上都进行性能测试。

回顾一下这个部分，看看缩放后数据的投影：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015435.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015443.png)

区别不明显，但是如果你仔细观察并与之前投影后的原始和中心化数据进行比较，可以看见一点微妙的差异。

#### 深入解释主成分
在介绍第二个特征转换算法前，我们需要研究一下如何解释主成分。

1. 鸢尾花数据集是一个150×4的矩阵。当我们把n_components设置为2时，得到的矩阵是2×4的：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509015526.png)

2. 和手动计算特征向量时一样，`components_`属性可以用矩阵乘法计算投影。我们将原始数据和`components_`矩阵的转置相乘：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509152558.png)

3. 为了让行列数对应，我们对矩阵进行了转置。其底层的原理是，对于每行，计算原始行和每个主成分的点积。点积的结果是新的行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509152758.png)

4. 可以利用内置的转换方法进行操作：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509152931.png)

换句话说，每个主成分都是原始列的组合。这样，我们的第一个主成分是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509153121.png)

第一个缩放后的花是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509153337.png)

要获取投影矩阵第一行的第一个元素，可以用公式：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509153401.png)

实际上，对于任何花的数据（坐标是(a, b, c, d)：按iris.feature_names的描述，a是萼片长度，b是萼片宽度，c是花瓣长度，d是花瓣宽度），新坐标系的第一个值可以如此计算：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509153417.png)

我们进一步处理，对主成分进行可视化。我们截断原始数据，只保留两个原始特征，即萼片长度和萼片宽度。这样做的原因是使可视化更加简单，不需要关心4个维度：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509153433.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509153440.png)

第一个主成分PCA 1表示了大部分差异，所以投影后的数据主要在x轴上分布。注意x轴的区间是-3～3，而y轴的区间是-0.4～0.6。为了进一步说明，我们用下面的代码绘制原始和投影后的鸢尾花散点图，在两个坐标系上面覆盖twodim_pca的主成分。

我们的目标是将主成分理解成引导向量，展示数据如何移动，以及这些向量如何变成垂直坐标系：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509165430.png)

下面两幅图展示了原始鸢尾花数据集和使用了PCA的投影数据集。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509165445.png)

上方的图表示原始坐标系中的主成分。这些主成分不是垂直的，指向数据自然遵循的方向。可以看到，两个向量中较长的那个就是第一个主成分，其方向明显是鸢尾花数据最符合的对角线方向。

第二个主成分是方差的方向，解释一部分的数据形状，但不能全部解释。下方的图显示了鸢尾花数据如何投影到新的主成分上，而新的主成分变成了直角坐标系，也就是新的x轴和y轴。

PCA是一种特征转换工具，能以原始特征的线性组合构建出全新的超级特征。我们看见，这些新的主成分表示了最大的方差，变成了数据的新坐标系。下一个特征转换算法与其类似，也是从数据中提取特征，不过是以机器学习的方式来做的。

### 线性判别分析
线性判别分析（LDA，linear discriminant analysis）是特征变换算法，也是有监督分类器。LDA一般用作分类流水线的预处理步骤。和PCA一样，LDA的目标是提取一个新的坐标系，将原始数据集投影到一个低维空间中。和PCA的主要区别在于，LDA不会专注于数据的方差，而是优化低维空间，以获得最佳的类别可分性。意思是，新的坐标系在为分类模型查找决策边界时更有用，非常适合用于构建分类流水线。

LDA极为有用的原因在于，基于类别可分性的分类有助于避免机器学习流水线的过拟合，也叫防止维度诅咒。LDA也会降低计算成本。

#### LDA的工作原理
LDA和PCA一样可以作为降维工具使用，但并不会计算整体数据的协方差矩阵的特征值，而是计算类内（within-class）和类间（between-class）散布矩阵的特征值和特征向量。LDA分为5个步骤：

1. 计算每个类别的均值向量；
2. 计算类内和类间的散布矩阵；
3. 计算![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509165744.png)
的特征值和特征向量；
4. 降序排列特征值，保留前k个特征向量；
5. 使用前几个特征向量将数据投影到新空间。

##### 计算每个类别的均值向量
首先计算每个类别中每列的均值向量，分别是setosa、versicolor和virginica：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509171702.png)

##### 计算类内和类间的散布矩阵
我们先计算类内的散布矩阵，定义如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509171727.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509171741.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509171825.png)

类内和类间的散布矩阵是对ANOVA测试中一个步骤的概括（上一章有涉及）。此处的想法是把鸢尾花数据分成两个不同的部分。

计算矩阵之后，我们可以进入下一步，用矩阵代数提取线性判别式。

##### 计算特征值和特征向量
和PCA的操作类似，我们需要对特定矩阵进行特征值分解。在LDA中，我们会分解矩阵![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172053.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172102.png)

注意第三个和第四个特征值几乎是0，这是因为LDA的工作方式是在类间划分决策边界。考虑到鸢尾花数据中只有3个类别，我们可能只需要2个决策边界。通常来说，用LDA拟合n个类别的数据集，最多只需要n-1次切割。

##### 降序排列特征值，保留前k个特征向量
和PCA一样，只保留最有用的特征向量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172353.png)

用每个特征值除以特征值的和，可以查看每个类别（线性判别式）解释总方差的比例：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172411.png)

看起来第一个判别式做了绝大部分的工作，拥有超过99%的信息。

##### 使用前几个特征向量投影到新空间
现在我们有了所有的线性判别式，先用特征向量将鸢尾花数据集投影到新空间，然后用plot函数绘制投影数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172457.png)

注意在图中，数据几乎完全突出出来了（甚至比PCA的投影效果还好），因为LDA会绘制决策边界，提供特征向量/线性判别式，从而帮助机器学习模型尽可能分离各种花。这有助于将数据投影到每个类别都尽可能分散的空间中。

#### 在scikit-learn中使用LDA
scikit-learn中有LDA的实现，可以避免这个费时费力的过程。导入很简单：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172547.png)

然后可以拟合并转换原始的鸢尾花数据，绘制新的投影，以便和PCA的结果进行比较。注意在下面的代码中，`fit`函数需要两个输入。

回忆一下，我们说过，LDA其实是伪装成特征转换算法的分类器。和PCA的无监督计算（不需要响应变量）不同，LDA会尝试用响应变量查找最佳坐标系，尽可能优化类别可分性。这意味着，LDA只在响应变量存在时才可以使用。使用时，我们把响应变量作为第二个参数输入`fit`，让LDA进行计算：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509172626.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173049.png)

上图是手动执行LDA时投影的镜像，看起来还可以。回想一下，在PCA中我们有过符号相反的特征向量，但这不会影响机器学习流水线。在LDA模块中，我们需要注意一些差别。LDA有一个`scalings_`属性，没有`components_`，但是二者的行为基本相同：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173122.png)

两个线性判别式解释的方差和之前计算的比例完全相同。注意我们忽略了第三个和第四个特征值，因为它们几乎是0。

然而，这些判别式乍看之下和之前手动计算的特征向量完全不同。这是因为scikit-learn计算特征向量的方式虽然得到了相同的结果，但是会进行标量缩放，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173153.png)

scikit-learn计算的线性判别式是手动计算结果的标量乘法，意味着它们都是正确的特征向量。唯一的区别在于投影数据的缩放问题。

这些特征被组织成4×2的矩阵，而不是PCA中2×4的矩阵。这是开发模块时的选择，在数学上没有影响。LDA和PCA都不改变数据尺度，所以缩放非常重要。

我们在LDA上拟合缩放后的鸢尾花数据，看看差异：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173506.png)

`scalings_`属性（和PCA的`components_`属性类似）显示了不同的数组，代表投影也是不一样的。为了完成对LDA的描述，我们再用一次之前的代码，像对待PCA的`components_`属性一样解释`scalings_`。

先在截断后的数据集上用LDA拟合并转换，只保留前两个特征：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173604.png)

看看数据集投影的前5行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173818.png)

`scalings_`矩阵现在是2×2的（2行2列），列是判别式（和PCA的行是主成分不同）。要进行调整，可以建立一个叫`components`的变量，保存`scalings_`的转置：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173910.png)

我们看见，`components`变量和PCA中`components_`的使用方式相同。这意味着，和PCA一样，投影是原始列的一个线性组合。还要注意，LDA也会和PCA一样去除特征的相关性。为了证明这一点，我们计算原始截断数据和投影数据的相关矩阵：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509173941.png)

注意，原始数据每个矩阵的右上角，特征都是高度相关的，但是LDA投影后数据的特征高度独立（相关系数接近0）。在利用PCA和LDA开始真正的机器学习前，我们来总结一下对LDA的解释。和PCA一样，查看LDA中`scalings_`属性的图像：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509174005.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509174013.png)

注意判别式并不与数据的方差一致，而是基本与之垂直：其实符合类别的分离情况。另外，它与左右两侧鸢尾花之间的间隔几乎平行。LDA在试图捕捉两个类别的分离情况。

上图中，我们可以看见原始数据集的`scalings_`向量覆盖在数据点上。较长的向量几乎与左下的山鸢尾（setosa）和右上的其他鸢尾花之间的间隔平行。这表明LDA在尝试指出原始坐标系中分离鸢尾花类别的最佳方向。

需要注意的是，LDA的`scalings_`属性并不像PCA的那样1∶1对应坐标系。因为`scalings_`的目的不是创造新的坐标系，而是指向可以优化类别可分性的最佳方向。我们不会像介绍PCA时那样详细说明坐标系的计算。我们只需要知道，PCA和LDA的主要区别在于：PCA是无监督方法，捕获整个数据的方差；而LDA是有监督方法，通过响应变量来捕获类别可分性。

LDA等有监督特征转换的局限性在于，不能像PCA那样处理聚类任务。这是因为聚类是无监督的任务，没有LDA需要的响应变量。

### LDA与PCA：使用鸢尾花数据集
我们终于接近了尾声，可以在机器学习流水线中尝试PCA和LDA了。因为本章一直在使用鸢尾花数据集，所以继续用这个数据集展示LDA和PCA作为有监督和无监督机器学习特征转换预处理步骤的实用性。

我们从有监督的机器学习开始，建立一个分类器，从4个定量特征中识别鸢尾花的种类。

1. 从scikit-learn中导入3个模块：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220045.png)
我们用K最近邻（KNN）作为有监督模型，用流水线模块将KNN模型和特征转换工具结合起来，创建一个可以使用`cross_val_score`模块进行交叉验证的机器学习流水线。我们会尝试几个不同的机器学习流水线，并记录性能。
2. 创建3个变量，其中一个代表LDA，一个代表PCA，最后一个代表KNN模型：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220246.png)
3. 不用做任何转换，调用KNN模型，来取一个基线准确率。我们将用它来对比两个特征转换算法：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220318.png)
4. 要击败的基线准确率是98.04%。我们用LDA，只保留最好的线性判别式：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220336.png)
5. 看来一个线性判别式不够击败基线准确率。下面试试PCA。我们的猜测是，PCA不会优于LDA，因为PCA不会像LDA那样优化类别可分性：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220402.png)
毫无疑问，表现最差。
试试加一个LDA判别式是否有用：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220439.png)
用两个判别式就可以达到原始的准确率！不错，但是我们希望做得更好。看看上一章的特征选择模块是否有帮助。我们导入`SelectKBest`模块，看看统计特征选择能否让LDA模块做到最好：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220528.png)
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509220549.png)
到目前为止，拥有两个判别式的LDA暂时领先。在生产中，联合使用有监督和无监督的特征转换是很常见的。我们设置一个`GridSearch`模块，找到下列参数的最佳组合：
1. 缩放数据（用或不用均值/标准差）；
2. PCA主成分；
3. LDA判别式；
4. KNN邻居。

下面的代码会建立一个`get_best_model_and_accuracy`函数，向其传入一个模型（scikit-learn或其他模型）、一个字典形式的参数网，以及X和y数据集，会输出网格搜索模块的结果。输出是模型的最佳表现（准确率）、获得最佳表现时的最好参数、平均拟合时间，以及平均预测时间：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509221043.png)

设置好接收模型和参数的函数后，我们可以组合使用缩放、PCA、LDA和KNN对流水线进行测试了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200509221101.png)

最好的准确率（接近99%）结合了缩放、PCA和LDA。在流水线中结合使用这3种算法并且用超参数进行微调是很常见的。因此，在生产环境下，最好的机器学习流水线实际上是多种特征工程工具的组合。

### 小结
总结一下，PCA和LDA都是特征转换工具，用于找出最优的新特征。LDA特别为类别分离进行了优化，而PCA是无监督的，尝试用更少的特征表达方差。一般来说，这两个算法在流水线中会一同使用，像上面的例子那样。在最后一章中，我们会研究两个案例，在文本聚类和面部识别软件中利用PCA和LDA。

PCA和LDA都是很强大的工具，但也有局限性。这两个工具都是线性转换，所以只能创建线性的边界，表达数值型数据。它们也是静态转换。无论输入什么，LDA和PCA的输出都是可预期的，而且是数学的。如果数据不适合PCA或LDA（数据有非线性特征，例如是圆形的），那么无论我们怎么进行网格搜索，这些算法都不会有什么帮助。

下一章介绍特征学习算法。可以说，特征学习算法是最强大的特征工程算法。这些算法可以从输入的数据中学习新特征，不必像PCA或LDA那样对数据特性有所假设。我们还会使用包括神经网络在内的复杂结构，实现最高级别的特征工程>




