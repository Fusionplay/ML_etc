<!-- TOC -->

- [特征工程入门与实践](#特征工程入门与实践)
  - [特征工程简介](#特征工程简介)
    - [评估监督学习算法](#评估监督学习算法)
    - [评估无监督学习算法](#评估无监督学习算法)
    - [特征理解：我的数据集里有什么](#特征理解我的数据集里有什么)
    - [特征增强：清洗数据](#特征增强清洗数据)
    - [特征选择：对坏属性说不](#特征选择对坏属性说不)
    - [特征构建：能生成新特征吗](#特征构建能生成新特征吗)
    - [特征转换：数学显神通](#特征转换数学显神通)
    - [特征学习：以AI促AI](#特征学习以ai促ai)
    - [小结](#小结)
  - [特征理解：我的数据集里有什么](#特征理解我的数据集里有什么-1)
    - [数据结构的有无](#数据结构的有无)
      - [非结构化数据的例子：服务器日志](#非结构化数据的例子服务器日志)
    - [定量数据和定性数据](#定量数据和定性数据)
      - [按工作分类的工资](#按工作分类的工资)
    - [数据的4个等级](#数据的4个等级)
      - [定类等级](#定类等级)
        - [可以执行的数学操作](#可以执行的数学操作)
      - [定序等级](#定序等级)
        - [可以执行的数学操作](#可以执行的数学操作-1)
      - [定距等级](#定距等级)
        - [可以执行的数学操作](#可以执行的数学操作-2)
          - [在定距等级绘制两列数据](#在定距等级绘制两列数据)
      - [定比等级](#定比等级)
        - [可以执行的数学操作](#可以执行的数学操作-3)
    - [数据等级总结](#数据等级总结)
  - [特征增强：清洗数据](#特征增强清洗数据-1)
    - [识别数据中的缺失值](#识别数据中的缺失值)
      - [探索性数据分析](#探索性数据分析)
    - [处理数据集中的缺失值](#处理数据集中的缺失值)
      - [删除有害的行](#删除有害的行)
      - [填充缺失值](#填充缺失值)
      - [在机器学习流水线中填充值](#在机器学习流水线中填充值)
  - [标准化和归一化](#标准化和归一化)
    - [z分数标准化](#z分数标准化)
    - [min-max标准化](#min-max标准化)
    - [行归一化](#行归一化)
    - [整合起来](#整合起来)
  - [特征构建：我能生成新特征吗](#特征构建我能生成新特征吗)
    - [检查数据集](#检查数据集)
    - [填充分类特征](#填充分类特征)
      - [自定义填充器](#自定义填充器)
      - [自定义分类填充器](#自定义分类填充器)
      - [自定义定量填充器](#自定义定量填充器)
    - [编码分类变量](#编码分类变量)
      - [定类等级的编码](#定类等级的编码)
      - [定序等级的编码](#定序等级的编码)
      - [将连续特征分箱](#将连续特征分箱)
      - [创建流水线](#创建流水线)
    - [扩展数值特征](#扩展数值特征)
      - [根据胸部加速度计识别动作的数据集](#根据胸部加速度计识别动作的数据集)
      - [多项式特征](#多项式特征)
        - [参数](#参数)
        - [探索性数据分析](#探索性数据分析-1)
    - [针对文本的特征构建](#针对文本的特征构建)
      - [词袋法](#词袋法)
      - [`CountVectorizer`](#countvectorizer)
        - [`CountVectorizer`的参数](#countvectorizer的参数)
      - [TF-IDF向量化器](#tf-idf向量化器)
      - [在机器学习流水线中使用文本](#在机器学习流水线中使用文本)
  - [特征选择：对坏属性说不](#特征选择对坏属性说不-1)
    - [在特征工程中实现更好的性能](#在特征工程中实现更好的性能)
      - [案例分析：信用卡逾期数据集](#案例分析信用卡逾期数据集)
    - [创建基准机器学习流水线](#创建基准机器学习流水线)
    - [特征选择的类型](#特征选择的类型)
      - [基于统计的特征选择](#基于统计的特征选择)
        - [使用皮尔逊相关系数](#使用皮尔逊相关系数)
        - [使用假设检验](#使用假设检验)

<!-- /TOC -->
# 特征工程入门与实践
## 特征工程简介
### 评估监督学习算法
当进行预测建模（即监督学习）时，性能直接与模型利用数据结构的能力，以及使用数据结构进行恰当预测的能力有关。一般而言，可以将监督学习分为两种更具体的类型：分类（预测定性响应）和回归（预测定量响应）。

评估分类问题时，直接用5折交叉验证计算逻辑回归模型的准确率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000754.png)

与之类似，对于回归问题，我们用线性回归的均方误差（MSE，mean squared error）进行评估，同样使用5折交叉验证：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000814.png)

我们用这两个线性模型，而不是出于速度和低方差的考虑使用更新、更高级的模型。这样可以更加确定，性能的增长直接与特征工程相关，而不是因为模型可以发现隐藏的模式。
### 评估无监督学习算法
这个问题比较棘手。因为无监督学习不做出预测，所以不能直接根据模型预测的准确率进行评估。尽管如此，如果我们进行了聚类分析（例如之前的市场细分例子），通常会利用轮廓系数（silhouette coefficient，这是一个表示聚类分离性的变量，在-1和1之间）加上一些人工分析来确定特征工程是提升了性能还是在浪费时间。

下面的例子用Python和scikit-learn导入并计算了一些假数据的轮廓系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507000954.png)

>需要记住，之所以对评估的算法和指标进行标准化，是因为要展示特征工程的强大，而且要让你成功复现我们的过程。实践中，你优化的性能有可能不是准确率，例如真阳性率（true positive rate），想用决策树而不是逻辑回归。这样很好，我们鼓励这样做。始终记住，要按步骤评估特征工程的结果，并将特征工程后的结果与基准性能进行对比。

大体上，我们会在3个领域内对特征工程的好处进行量化。

1. 监督学习：也叫预测分析
   1.  回归——预测定量数据:主要使用均方误差作为测量指标
   2.  分类——预测定性数据:主要使用准确率作为测量指标
2. 无监督学习：聚类——将数据按特征行为进行分类:主要用轮廓系数作为测量指标
3. 统计检验：用相关系数、$t$检验、卡方检验，以及其他方法评估并量化原始数据和转换后数据的效果

###  特征理解：我的数据集里有什么
在繁杂的切入点中，我们将着眼于以下几个方面：
1. 结构化数据与非结构化数据；
2. 数据的4个等级；
3. 识别数据的缺失值；
4. 探索性数据分析；
5. 描述性统计；
6. 数据可视化。

### 特征增强：清洗数据
我们探索的主题包括以下这些。
1. 对非结构化数据进行结构化。
2. 数据填充——在原先没有数据的位置填充（缺失）数据。
3. 数据归一化： 
   1. 标准化（也称为$z$分数标准化）；
   2. 极差法（也称为$min-max$标准化）；
   3. $L1$和$L2$正则化（将数据投影到不同的空间，很有趣）。

### 特征选择：对坏属性说不
这些过程包括：
1. 相关系数；
2. 识别并移除多重共线性；
3. 卡方检验；
4. 方差分析；
5. 理解$p$值；
6. 迭代特征选择；
7. 用机器学习测量熵和信息增益。

### 特征构建：能生成新特征吗
### 特征转换：数学显神通
这一章将开始着眼于自动创建特征，因为这些特征适用于数学维度。如果把数据理解为一个$n$维空间中的向量（$n$是列数），那么我们可以考虑，能不能创建一个$k$维（$k＜n$）的子集，完全或几乎完全表示原数据，从而提升机器学习速度或性能？这里的目标是，创建一个维度更低、比原有高维度数据集性能更好的数据集。

一个值得注意的例子是主成分分析（PCA，principal component analysis），我们会花一些时间深入探讨。这种转换将数据分成3个完全不同的数据集，然后可以用这些结果创造全新的数据集，让其性能超过原先的数据集！

### 特征学习：以AI促AI
我们会主要关注基于神经网络（节点和权重）的算法。这些算法在数据上增加的特征虽然有时不为人类所理解，但是机器会大受裨益。这章的主题包括：
1. 受限玻尔兹曼机（RBM，restricted Boltzmann machine）；
2. Word2vec/GloVe等词嵌入（word embedding）算法。

Word2vec和GloVe算法可以将高维度数据嵌入文本的词项（token）中。例如，Word2vec算法的可视化结果可能如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507002418.png)

在欧几里得空间中将单词表示为向量后，就可以得到数学样式的结果。在上面的例子中，加入自动生成的特征后，可以在Word2vec算法的帮助下通过计算单词的向量表示来对单词进行加减。然后可以得到有趣的结论，例如国王-男+女=女王。

### 小结
1. 特征理解：学习如何识别定量数据和定性数据。
2. 特征增强：清洗和填充缺失值，最大化数据集的价值。
3. 特征选择：通过统计方法选择一部分特征，以减少数据噪声。
4. 特征构建：构建新的特征，探索特征间的联系。
5. 特征转换：提取数据中的隐藏结构，用数学方法转换数据集、增强效果。
6. 特征学习：利用深度学习的力量，以全新的视角看待数据，从而揭示新的问题，并予以解决。

## 特征理解：我的数据集里有什么
### 数据结构的有无
拿到一个新的数据集后，首要任务是确认数据是结构化还是非结构化的。

1. 结构化（有组织）数据：可以分成观察值和特征的数据，一般以表格的形式组织（行是观察值，列是特征）。
2. 非结构化（无组织）数据：作为自由流动的实体，不遵循标准组织结构（例如表格）的数据。通常，非结构化数据在我们看来是一团数据，或只有一个特征（列）。

下面两个例子展示了结构化和非结构化数据的区别：
1. 以原始文本格式存储的数据，例如服务器日志和推文，是非结构化数据；
2. 科学仪器报告的气象数据是高度结构化的，因为存在表格的行列结构。

#### 非结构化数据的例子：服务器日志
我们从公共数据中提取了一些服务器日志，放在文本文件中，作为非结构化数据的例子。可以看看这种数据的样子，方便日后识别：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507002925.png)

我们创建了一个叫作logs的Pandas DataFrame，用于存放服务器日志。可以用`.head()`方法看一下前几行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003019.png)

logs DataFrame中数据的前5行如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003201.png)

可以发现，表中每行代表一篇日志，而且只有一列：日志文本。这个文本并不是特征，只是来自服务器的原始日志。这个例子很好地代表了非结构化数据。通常，文本形式的数据都是非结构化的。

### 定量数据和定性数据
为了完成对数据的判断，我们从区分度最高的顺序开始。在处理结构化的表格数据时（大部分时候都是如此），第一个问题一般是：数据是定量的，还是定性的？

- 定量数据本质上是数值，应该是衡量某样东西的数量。
- 定性数据本质上是类别，应该是描述某样东西的性质。

基本示例：
1. 以华氏度或摄氏度表示的气温是定量的；
2. 阴天或晴天是定性的；
3. 白宫参观者的名字是定性的；
4. 献血的血量是定量的。

有时，数据可以同时是定量和定性的。例如，餐厅的评分（1～5星）虽然是数，但是这个数也可以代表类别。如果餐厅评分应用要求你用定量的星级系统打分，并且公布带小数的平均分数（例如4.71星），那么这个数据是定量的。如果该应用问你的评价是讨厌、还行、喜欢、喜爱还是特别喜爱，那么这些就是类别。由于定量数据和定性数据之间的模糊性，我们会使用一个更深层次的方法进行处理，称为数据的4个等级。

#### 按工作分类的工资
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003915.png)

然后导入第一个数据集，探索在旧金山做不同工作的工资。这个数据集可以公开获得，随意使用。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507003944.png)

可以看到表格有很多列，而且已经能发现其中一些是定性或定量的。我们用`.info()`方法了解一下数据有多少行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004026.png)

可以看到，数据有1356个条目（行）和13列。`.info()`方法也会报告每列的非空（non-null）项目数。这点非常重要，因为缺失数据是特征工程中最常见的问题之一。在Pandas包中有很多方法可以识别和处理缺失值，其中计算缺失值数量最快的方法是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004050.png)

数据中看起来没有缺失值，可以（暂时）松一口气了。接下来用`describe`方法查看一些定量数据的描述性统计（应该有定量列）。注意，`describe`方法默认描述定量列，但是如果没有定量列，也会描述定性列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004122.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004131.png)

Pandas认为，数据只有3个定量列：Step、Union Code和Extended Step（步进、工会代码和增强步进）。先不说步进和增强步进，很明显工会代码不是定量的。虽然这一列是数，但这些数不代表数量，只代表某个工会的代码。因此需要做一些工作来理解我们感兴趣的特征。最值得注意的特征是一个定量列Biweekly High Rate（双周最高工资）和一个定性列Grade（工作种类）。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004201.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004209.png)

我们清理一下数据，移除工资前面的美元符号，保证数据类型正确。当处理定量数据时，一般使用整数或浮点数作为类型（最好使用浮点数）；定性数据则一般使用字符串或Unicode对象。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004250.png)

我们可以用Pandas的`map`功能，将函数映射到整个数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004316.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004329.png)

最后，将Biweekly High Rate列中的数据转换为浮点数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004344.png)

同时，将Grade列中的数据转换为字符串：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507004455.png)

可以看见，我们共有：1356行（和开始时相同）,2列（我们选择的）

- 双周最高工资：
  1. 定量列，代表某个部门的平均最高工资
  2. 此列是定量的，因为其中的值是数，代表某人每两周的工资
  3. 数据类型是浮点数，因为进行了强制转换
- 工作种类：
  1. 工资对应的部门
  2. 此列肯定是定性的，因为代码代表一个部门，而不是数量
  3. 数据类型是对象，Pandas会把字符串归为此类
  4. 为了进一步研究定量数据和定性数据，我们开始研究数据的4个等级。

### 数据的4个等级
我们已经可以将数据分为定量和定性的，但是还可以继续分类。数据的4个等级是：

1. 定类等级（nominal level）
2. 定序等级（ordinal level）
3. 定距等级（interval level）
4. 定比等级（ratio level）

每个等级都有不同的控制和数学操作等级。了解数据的等级十分重要，因为它决定了可以执行的可视化类型和操作。

#### 定类等级
定类等级是数据的第一个等级，其结构最弱。这个等级的数据只按名称分类。例如，血型（A、B、O和AB型）、动物物种和人名。这些数据都是定性的。

##### 可以执行的数学操作
对于每个等级，我们都会简要介绍可以执行的数学操作，以及不可以执行的数学操作。在这个等级上，不能执行任何定量数学操作，例如加法或除法。这些数学操作没有意义。因为没有加法和除法，所以在此等级上找不到平均值。当然了，没有“平均名”或“平均工作”这种说法。

但是，我们可以用Pandas的`value_counts`方法进行计数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005040.png)

出现最多的工作种类是00000，意味着这个种类是众数，即最多的类别。因为能在定类等级上进行计数，所以可以绘制图表（如条形图）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005107.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005116.png)

#### 定序等级
定类等级为我们提供了很多进一步探索的方法。向上一级就到了定序等级。定序等级继承了定类等级的所有属性，而且有重要的附加属性：

1. 定序等级的数据可以自然排序；
2. 这意味着，可以认为列中的某些数据比其他数据更好或更大。

和定类等级一样，定序等级的天然数据属性仍然是类别，即使用数来表示类别也是如此。

##### 可以执行的数学操作
和定类等级相比，定序等级多了一些新的能力。在定序等级，我们可以像定类等级那样进行计数，也可以引入比较和排序。因此，可以使用新的图表了。不仅可以继续使用条形图和饼图，而且因为能排序和比较，所以能计算中位数和百分位数。对于中位数和百分位数，我们可以绘制茎叶图和箱线图。

我们引入一个新的数据集来解释定序等级的数据。这个数据集表示多少人喜欢旧金山国际机场。

现在，我们关注Q7A_ART这一列。如数据字典所述，Q7A_ART是关于艺术品和展览的。可能的选择是0、1、2、3、4、5、6，每个数字都有含义。

- 1：不可接受
- 2：低于平均
- 3：平均
- 4：不错
- 5：特别好
- 6：从未有人使用或参观过
- 0：空

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005506.png)

Pandas把这个列当作数值处理了，因为这个列充满数。然而我们需要知道，虽然这些值是数，但每个数其实代表的是类别，所以该数据是定性的，更具体地说，是属于定序等级。如果删除0和6这两个类别，剩下的5个有序类别类似于餐厅的评分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507005540.png)

然后将这些值转换为字符串：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010008.png)

现在定序数据的格式是正确的，可以进行可视化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010051.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010059.png)

#### 定距等级
我们开始加大火力了。在定类和定序等级，我们一直在处理定性数据。即使其内容是数，也不代表真实的数量。在定距等级，我们摆脱了这个限制，开始研究定量数据。在定距等级，数值数据不仅可以像定序等级的数据一样排序，而且值之间的差异也有意义。这意味着，在定距等级，我们不仅可以对值进行排序和比较，而且可以加减。

例子：定距等级的一个经典例子是温度。如果美国得克萨斯州的温度是32℃，阿拉斯加州的温度是4℃，那么可以计算出32-4=28℃的温差。这个例子看上去简单，但是回首之前的两个等级，我们从未对数据执行过这种操作。

##### 可以执行的数学操作
请记住，在定距等级上可以进行加减，这改变了整个游戏规则。既然可以把值加在一起，就能引入两个熟悉的概念：算术平均数（就是均值）和标准差。为了举例说明，我们引入一个新的数据集，它是关于气候变化的：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010243.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010254.png)

这个数据集有860万行，每行代表某个城市某月的平均温度，上溯到18世纪。请注意，只看前5行，我们已经可以注意到有数据缺失了。把这些数据删除，美化一下结果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010323.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010331.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010341.png)

我们关注的是AverageTemperature（平均温度）列。温度数据属于定距等级，这里不能使用条形图或饼图进行可视化，因为值太多了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010359.png)

对111994个值绘图非常奇怪，当然也没有必要，因为我们知道这些数是定量的。从这个级别开始，最常用的图是直方图。直方图是条形图的“近亲”，用不同的桶包含不同的数据，对数据的频率进行可视化。

对世界平均温度画一个直方图，从整体的角度看温度分布：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010424.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010431.png)

可以看到，平均温度约为20℃。下面确认一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010447.png)

很接近，均值大概是17℃。我们继续处理数据，加入year（年）和century（世纪）两列，只观察美国的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010754.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010818.png)

我们用新的century列，对每个世纪画直方图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010952.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507010959.png)

这4幅直方图显示AverageTemperature随时间略微上升。确认一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011031.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011041.png)

因为差值在这个等级是有意义的，所以我们可以回答美国从18世纪至今平均温度上升多少这个问题。先把随世纪变化的温度数据存储到Pandas的Series对象中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011115.png)

现在可以对这个Series进行切片，用21世纪的数据减去18世纪的数据，得到温差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011148.png)

###### 在定距等级绘制两列数据
定距及更高等级的一大好处是，我们可以使用散点图：在两个轴上绘制两列数据，将数据点可视化为图像中真正的点。在气候变化数据集中，year和averageTemperature列都属于定距等级，因为它们的差值是有意义的。因此可以对美国每月的温度绘制散点图，其中x轴是年份，y轴是温度。我们希望可以看见之前折线图表示的升温趋势：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507011734.png)

好像不怎么好看。和预期一样，里面有很多噪声。考虑到每年每个城镇都会报告好几个平均气温，在图上每年有很多点也是理所应当的。

我们用`groupby`清除年份的大部分噪声：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012515.png)

好多了！可以看出气温随年份上升的趋势，但是可以再用滑动均值（rolling mean）平滑一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012539.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507012548.png)

我们在定距等级同时绘制两列数据，重新确认了之前用折线图表示的内容：美国的平均气温总体的确有上升的趋势。

#### 定比等级
最终，我们到达了最高的等级：定比等级。在这个等级上，可以说我们拥有最高程度的控制和数学运算能力。和定距等级一样，我们在定比等级上处理的也是定量数据。这里不仅继承了定距等级的加减运算，而且有了一个绝对零点的概念，可以做乘除运算。

##### 可以执行的数学操作
在定比等级，我们可以进行乘除运算。虽然看起来没什么大不了，但是这些运算可以让我们对这个等级上的数据进行独特的观察，而这在低等级上是无法做到的。我们先看几个例子，了解一下这意味着什么。

当处理金融数据时，我们几乎肯定要计算一些货币的值。货币处于定比等级，因为“零资金”这个概念可以存在。那么我们就可以说：

1. $100是$50的两倍，因为100/50=2；
2. 0mg青霉素是20mg青霉素的一半，因为10/20=0.5。

因为存在0这个概念，所以这种比较是有意义的。

我们一般认为，温度属于定距等级，而不是定比等级，因为100℃比50℃高两倍这种说法没有意义，并不合理。温度是主观的，不是客观正确的。

回到旧金山的工资数据，可以看到Biweekly High Rate列处于定比等级，因而可以进行新的观察。先看一下最高的工资：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015042.png)

会发现，工资最高的是公共交通部总经理（General Manager, Public Transportation Dept.）。我们用同样的办法查看工资最低的工作：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015142.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015152.png)

对照可知，工资最低的是集会助理（Camp Assistant）。

因为金钱处于定比等级，所以可以计算最高工资和最低工资的比值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015224.png)

### 数据等级总结
理解数据的不同等级对于特征工程是非常必要的。当需要构建新特征或修复旧特征时，我们必须有办法确定如何处理每一列。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015549.png)

下表展示了每个等级上可行与不可行的统计类型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015612.png)

最后这张表显示了每个等级上可以或不可以绘制的图表。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015636.png)

当你拿到一个新的数据集时，下面是基本的工作流程。
1. 数据有没有组织？数据是以表格形式存在、有不同的行列，还是以非结构化的文本格式存在？
2. 每列的数据是定量的还是定性的？单元格中的数代表的是数值还是字符串？
3. 每列处于哪个等级？是定类、定序、定距，还是定比？
4. 我可以用什么图表？条形图、饼图、茎叶图、箱线图、直方图，还是其他？

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507015726.png)

## 特征增强：清洗数据
1. 识别数据中的缺失值；
2. 删除有害数据；
3. 输入（填充）缺失值；
4. 对数据进行归一化/标准化；

### 识别数据中的缺失值
特征增强的第一种方法是识别数据的缺失值，这可以让我们更好地明白如何使用真实世界中的数据。通常，数据集会因为各种原因有所缺失，例如调查时没有记录某些观察值等。分析数据并了解缺失的数据是什么至关重要，这样才可以决定下一步如何处理这些缺失值。首先，我们深入了解一下本章要使用的数据集——皮马印第安人糖尿病预测数据集。

皮马印第安人糖尿病预测数据集,数据有9列，共768个数据点（行）。这个数据集希望通过体检结果细节，预测21岁以上的女性皮马印第安人5年内是否会患糖尿病。

这个数据集和机器学习的二分类（两个类别）问题相对应。意思是，我们希望知道，这个人5年内会不会得糖尿病？数据每列的含义（按顺序）如下：
1. 怀孕次数；
2. 口服葡萄糖耐量试验中的2小时血浆葡萄糖浓度；
3. 舒张压（mmHg）；
4. 三头肌皮褶厚度（mm）；
5. 2小时血清胰岛素浓度（μU/ml）；
6. 体重指数[BMI，即体重（kg）除以身高（m）的平方]；
7. 糖尿病家族函数；
8. 年龄（岁）；
9. 类变量（0或1，代表无或有糖尿病）。

对于这个数据集，我们的目标是向机器学习函数输入8个特征值，来预测最后一列类变量的值，即此人是否患有糖尿病。

采用这个数据集有两点很重要的原因：
1. 我们必须应对缺失值；
2. 所有特征都是定量的。

因为我们的目的就是研究缺失值。本章只处理定量特征，因为目前没有足够的工具处理缺失的定性特征。

#### 探索性数据分析
首先进行探索性数据分析（EDA，exploratory data analysis）来识别缺失的值。我们会使用Pandas和NumPy这两个得力的Python包来存储数据并进行一些简单的计算，还会使用流行的可视化工具来观察数据的分布情况。开始写一点代码吧。
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022037.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022100.png)

看起来不大对劲：表格没有列名。源数据的CSV文件肯定没有内置列的标题。没关系，可以用数据源网站的信息手动添加标题，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022125.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022137.png)

看起来好多了，可以用列名做一些基本的统计、选择和可视化操作。先算一下空准确率:

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022216.png)

可以用列名做一些基本的统计、选择和可视化操作。先算一下空准确率(空准确率是指当模型总是预测频率较高的类别时达到的正确率)：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022757.png)

既然终极目标是研究数据的规律以预测是否会患糖尿病，那么可以对糖尿病患者和健康人的区别进行可视化。希望直方图可以显示一些规律，或者这两类之间的显著差异：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022934.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507022947.png)

看起来患者和常人的血浆葡萄糖浓度（plasma_glucose_concentration）有很大的差异。我们继续按此绘制其他列的直方图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023007.png)

上面的代码会输出3张直方图。第一张直方图是两类（正常人和患者）的身体质量指数（BMI，body massindex）分布：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023026.png)

下一张直方图也表明两类人有显著区别，这次是在舒张压方面：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023042.png)

最后的直方图是血浆葡萄糖浓度方面的区别：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023058.png)

观察几张直方图后，可以看出两类人存在明显区别。例如，对于最终患有糖尿病的患者，其血浆葡萄糖浓度会有很大的增长。我们可以用线性相关矩阵来量化这些变量间的关系。用本章一开始导入的Seaborn作为可视化工具：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023118.png)

下图是数据集的相关矩阵，显示了皮马人数据集中不同列的相关性。输出如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023138.png)

相关矩阵显示，plasma_glucose_concentration（血浆葡萄糖浓度）和onset_diabetes（糖尿病）有很强的相关性。我们进一步研究onset_diabetes列的相关性数值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023247.png)

而目前探索性数据分析提示，plasma_glucose_concentration是预测糖尿病的重要变量。

我们要看看数据集中是否有数据点是空的（缺失值）。用Pandas DataFrame内置的`isnull()`方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023320.png)

数据中没有缺失值。继续探索数据，首先用`shape`方法看看数据的行数和列数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023416.png)

我们确定数据有9列（包括要预测的变量）和768个观察值（行）。用下面的代码看看糖尿病的发病率：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023430.png)


65%的人没有糖尿病，35%的人有糖尿病。`DataFrame`内置的`describe`方法可以提供数据基本的描述性统计：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023455.png)

表格里有基本的统计量，例如均值、标准差，以及一些百分位数据。但是，请注意：BMI的最小值是0。这是有悖医学常识的，肯定事出有因。也许数据中缺失或不存在的点都用0填充了。继续观察，我们发现以下列的最小值都是0：

1. times_pregnant
2. plasma_glucose_concentration
3. diastolic_blood_pressure
4. triceps_thickness
5. serum_insulin
6. bmi
7. onset_diabetes

因为onset_diabetes中的0代表没有糖尿病，人也可以怀孕0次，所以可以得出结论，下面这些列中的缺失值用0填充了：
1. plasma_glucose_concentration
2. diastolic_blood_pressure
3. triceps_thickness
4. serum_insulin
5. bmi

数据中还是存在缺失值的！我们已经知道缺失的数据用0填充过了，真不走运。作为数据科学家，你必须时刻保持警惕，尽可能地了解数据集，以便找到使用其他符号填充的缺失数据。务必阅读公开数据集的所有文档，里面有可能提到了缺失数据的问题。

如果数据集没有文档，缺失值的常见填充方法有：
1. 0（数值型）
2. unknown或Unknown（类别型）
3. ？（类别型）

### 处理数据集中的缺失值
在处理数据时，数据科学家遇到的最常见问题之一就是存在缺失值。最常见的情况是某个单元格（行列交叉点）是空白的，数据出于某种原因没有被收集到。缺失值会引发很多问题，最重要的是，大部分（不是全部的）学习算法不能处理缺失值。

因此，数据科学家和机器学习工程师有很多处理缺失值的办法和技巧。虽然办法有很多变种，但是两个最主要的处理方法是：

1. 删除缺少值的行；
2. 填充缺失值。

这两种办法都会清洗我们的数据集，让算法可以处理，但是每种办法都各有优缺点。

在进一步处理前，先用Python中的`None`填充所有的数字0，这样Pandas的`fillna`和`dropna`方法就可以正常工作了。我们可以手动将每列的0替换成`None`，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023840.png)

既可以手动一列列地操作，也可以用for循环和内置的`replace`方法加速，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507023939.png)

如果现在用`isnull`方法计算缺失值的数量，应该可以看见正确的结果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024004.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024709.png)

现在数据有意义多了。我们可以看见其中5列有缺失值，缺失程度有所不同。有些列，例如plasma_glucose_concentration只缺少5个值，但是serum_insulin差不多一半的值都是缺失的。

我们已经将缺失值正确插入了数据集，缺失数据不再是原来的占位符0。这样探索性数据分析也会更准确：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507024749.png)

注意describe方法不包括有缺失值的列。尽管不理想，但还是可以对某些列取均值和标准差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025007.png)

#### 删除有害的行
在处理缺失数据的两种办法中，最常见也最容易的方法大概是直接删除存在缺失值的行。通过这种操作，我们会留下具有数据的完整数据点。可以在Pandas中利用dropna方法获取新的DataFrame，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025213.png)

当然，现在的问题是我们丢失了一些行。用下面的代码检查删除了多少行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025235.png)

我们丢失了原始数据集中大约51%的行！从机器学习的角度考虑，尽管数据都有值、很干净，但是我们没有利用尽可能多的数据，忽略了一半以上的观察值。就像医生在研究心脏病发作原理时，忽略了一半以上的患者。

下面对数据集做进一步的探索性数据分析，比较一下丢弃缺失值前后的统计数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507025732.png)

看一下丢弃数据后的统计数据，如下面代码所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030019.png)

在大刀阔斧的转换后，二元响应看起来没什么变化。我们用`pima.mean`函数比较一下转换前后列的均值，看看数据的形状，结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030052.png)

用`pima_dropped.mean()`函数同样看一下丢弃缺失值后的统计数据，结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030112.png)

为了更好地了解这些数的变化，我们创建一个新图表，将每列均值变化的百分比可视化。首先创建一个表格，列出每列均值变化的百分比，如下方代码所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030206.png)

现在用条形图对这些变化进行可视化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030225.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030236.png)

可以看到，times_pregnant（怀孕次数）的均值在删除缺失值后下降了14%，变化很大！pedigree_function（糖尿病血系功能）也上升了11%，也是个飞跃。可以看到，删除行（观察值）会严重影响数据的形状，所以应该保留尽可能多的数据。在介绍下一个处理方法前，我们先进行一些机器学习。

类似于下面的代码块（稍后会一行行地讲解）将在本书中多次出现。这段代码基于目前的特征描述并实现了机器学习在多个参数上的一次拟合，希望取得最佳模型：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030437.png)

我们会使用scikit-learn的K最近邻（KNN，k-nearest neighbor）分类模型，以及一个网格搜索模块。这个模块会自动找到最适合我们模型的、交叉验证准确率最好的KNN参数组合（暴力搜索）。接下来，用删除缺失值后的数据集为预测模型创建一个X和一个y变量。我们从X（特征矩阵）开始,

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507030549.png)

现在已经看出问题了：机器学习算法使用的数据比一开始拿到的数据少得多。然后创建一个y（响应变量）：

有了X和y变量，就可以为网格搜索创建需要的参数和实例了。为了简便起见，我们将params（参数）的数量设成7。对于要尝试的每一种数据清洗和特征工程方法（删除行，填充数据），都用1～7个邻居进行拟合。然后实例化一个网格搜索模块，如以下代码所示，并用特征矩阵和响应变量进行拟合。拟合后，代码会打印出最佳准确率，以及相应的最佳参数，

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031315.png)

看来，最好的邻居数是7个，此时KNN模型的准确率是74.5%（比空准确率65%好）。但是要记住，这个模型只用了49%的数据，如果能用到所有数据，会不会更好一些？

很明显，虽然删除脏数据并不完全是特征工程，但这种操作的确是一种数据清洗技术，有助于清洗机器学习流水线的输入。接下来尝试一个稍难的办法。

#### 填充缺失值
填充数据是处理缺失值的一种更复杂的方法。填充指的是利用现有知识/数据来确定缺失的数量值并填充的行为。我们有几个选择，最常见的是用此列其余部分的均值填充缺失值，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031411.png)

看一下plasma_glucose_concentration列的5个缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031441.png)

现在可以用内置的`fillna`方法，将所有的`None`填充为plasma_glucose_concentration列其余数据的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031546.png)

如果查看各列，应该会发现原来的`None`被121.68取代了，这个值就是该列的均值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031617.png)

这样有点麻烦。我们用scikit-learn预处理类的`Imputer`模块,称它为“填充器”名副其实。可以这样导入：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031659.png)

和大部分scikit-learn模块一样，我们有几个新的参数可以调节，但是主要关注`strategy`。这个参数可以调节如何填充缺失值。对于定量值，可以使用内置的均值或中位数策略来填充值。为了使用`Imputer`，必须先实例化对象，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031725.png)

然后调用fit_transform方法创建新对象：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031747.png)

我们有个小问题要处理。`Imputer`的输出值不是Pandas的`DataFrame`，而是NumPy数组：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031812.png)

解决方法很简单，因为我们可以将任何数组直接变成`DataFrame`，方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031835.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507031845.png)

我们检查一下plasma_glucose_concentration列，确保填充的值和之前手动计算的值相同：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033001.png)

Imputer在很大程度上解决了填充缺失值的琐事。我们尝试填充一些别的值，看看对KNN模型的影响。首先尝试一种更简单的填充方法，用0替代所有的缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033046.png)

如果用0填充，准确率会低于直接删掉有缺失值的行。目前，我们的目标是建立一个可以从全部768行中学习的机器学习流水线，而且比仅用392行的结果还好。也就是说，我们的结果要好于0.745，即74.5%。

#### 在机器学习流水线中填充值
用`Imputer`类填充值的时候不使用流水线其实是很不恰当的，因此流水线尤其重要。这是因为学习算法的目标是泛化训练集的模式并将其应用于测试集。如果在划分数据集和应用算法之前直接对整个数据集填充值，我们就是在作弊，模型其实学不到任何模式。为了将这个概念可视化，我们对训练集和测试集进行一次划分，在交叉验证中可能会进行多次划分。

复制一份皮马印第安人数据集，从scikit-learn中导入一个划分模块：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033228.png)

现在进行一次划分。在划分前，对整个数据集填充变量X的均值，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033258.png)

用KNN模型拟合训练集和测试集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033358.png)

注意我们没有进行任何网格搜索，只做了简单的拟合。可以看见，模型的准确率是66%（并不好，但这不是重点）。重点是，训练集和测试集都是用整个X矩阵的均值填充的。这违反了机器学习流程的核心原则。当预测测试集的响应值时，不能假设我们已经知道了整个数据集的均值。简而言之，我们的KNN模型利用了测试集的信息以拟合训练集，所以亮红灯了。

现在用恰当的方法再做一遍。我们先计算出训练集的均值，然后用它填充测试集的缺失值。这个过程再一次测试了模型用训练数据的均值预测未知测试集的能力：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033434.png)

这里不取整个X矩阵的均值，而是用训练集的均值填充训练集和测试集的缺失值：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033507.png)

最后，给在同一个数据集上正确填充缺失值的KNN模型打分：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033522.png)


准确率的确低得多，但是至少更诚实地代表了模型的泛化能力，即从训练集的特征中学习并将所学应用到未知隐藏数据上的能力。通过为机器学习流水线的各个步骤提供结构和顺序，scikit-learn让搭建流水线变得更加容易。下面看看如何结合使用scikit-learn的`Pipeline`和`Imputer`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033608.png)
有几件事需要注意。第一，我们的Pipeline分两步：
1. 拥有`strategy='mean'`的`Imputer`；
2. KNN类型的分类器。
第二，要为网格搜索重新定义`param`字典，因为必须明确`n_neighbors`参数所属的步骤：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033807.png)

除此之外，其他一切都很平常。Pipeline类会替我们处理大部分流程：可以恰当地从多个训练集取值并用其填充测试集的缺失值，还可以正确测试KNN的泛化能力，最终输出性能最佳的模型。本例中的准确率是0.73，略微低于我们的目标0.745。了解这一语法后，我们可以重写一遍代码，但是略作修改，如下所示：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033848.png)

这里唯一的区别是，我们的流水线尝试了一种不同的填充策略：用剩余值的中位数填充缺失值。重申一下，这里的准确率可能比完全删除存在缺失值的行更差，但是我们的训练数据是之前的两倍！况且，这个办法还是比一开始所有数据都是0要好。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507033910.png)

如果只看准确率，最好的办法似乎是删掉有缺失值的行。也许只靠scikit-learn的Pipeline和Imputer还不够。如果有可能，我们还是希望利用全部768行的实现类似的性能，甚至更好。为此，我们引入全新的特征工程技巧：标准化和归一化。

## 标准化和归一化
到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过4种不同的方式处理数据集，最佳的KNN交叉验证准确率是0.745。如果回头看之前的探索性数据分析，会发现一些特征的性质：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034026.png)

到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过4种不同的方式处理数据集，最佳的KNN交叉验证准确率是0.745。如果回头看之前的探索性数据分析，会发现一些特征的性质：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034506.png)

用标准直方图查看所有9列的分布情况，指定一个图像大小：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034531.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034540.png)

每列的均值、最小值、最大值和标准差差别很大。通过`describe`方法也可以看到很明显的差别：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034635.png)

这有什么关系呢？因为一些机器学习模型受数据尺度（scale）的影响很大。这意味着如果diastolic_blood_pressure列的舒张压在24～122，但是年龄是21～81，那么算法不会达到最优化状态。我们可以在直方图方法中调用可选的sharex和sharey参数，在同一比例下查看每个图表：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034733.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507034741.png)

很明显，所有的数据尺度都不同。数据工程师可以选用某种归一化操作，在机器学习流水线上处理该问题。归一化操作旨在将行和列对齐并转化为一致的规则。例如，归一化的一种常见形式是将所有定量列转化为同一个静态范围中的值（例如，所有数都位于0～1）。我们也可以使用数学规则，例如所有列的均值和标准差必须相同，以便在同一个直方图上显示（和上面皮马人的直方图不同）。标准化通过确保所有行和列在机器学习中得到平等对待，让数据的处理保持一致。

我们将重点关注3种数据归一化方法：
1. z分数标准化；
2. min-max标准化；
3. 行归一化。

前两个办法特别用于调整特征，而第三个办法虽然操作行，但效果和前两个相当。

### z分数标准化
z分数标准化是最常见的标准化技术，利用了统计学里简单的z分数（标准分数）思想。z分数标准化的输出会被重新缩放，使均值为0、标准差为1。通过缩放特征、统一化均值和方差（标准差的平方），可以让KNN这种模型达到最优化，而不会倾向于较大比例的特征。公式很简单，对于每列，用这个公式替换单元格：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042059.png)

在这个公式中：
1. z是新的值（z分数）；
2. x是单元格原来的值；
3. μ是该列的均值；
4. σ是列的标准差。

以plasma_glucose_concentration列的缩放为例：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042149.png)

用下面的代码手动计算列的z分数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042225.png)

可以看到，该列中的每个值都会被替换，而且某些值是负的。这是因为该值代表到均值的距离，如果它最初低于该列的均值，z分数就是负数。当然，在scikit-learn中有内置的对象帮助我们计算，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042249.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042257.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042314.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042322.png)

可以看见该列处理前的分布情况。现在应用z分数标准化，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042345.png)

在应用缩放后，均值下降到0，标准差为1。下面进一步查看缩放后数据的分布情况：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042427.png)

我们观察到x轴更紧密了，y轴则没有变化。注意，数据的形状没有变化。在对每一列都进行z分数转换后，我们观察一下`DataFrame`的直方图。操作时，`StandardScaler`会对每列单独计算均值和标准差：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042554.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042606.png)

注意整个数据集的x轴都更加紧密了。现在将`StandardScaler`插入之前的机器学习流水线中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042649.png)

有几件事需要注意。我们在网格搜索中加入了一些新的参数，填充缺失值。现在我们正在寻找KNN中策略和邻居数的最佳组合，目前的得分是0.742，这是至今为止的最佳得分，接近目标0.745。这条流水线从整个768行中学习。我们现在看看另一种标准化方法。

### min-max标准化
min-max标准化和z分数标准化类似，因为它也用一个公式替换列中的每个值。此处的公式是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042739.png)

在这个公式中：
1. m是新的值；
2. x是单元格原来的值；
3. xmin是该列的最小值；
4. xmax是该列的最大值。

使用这个公式可以看到，每列所有的值都会位于0～1。我们用scikit-learn的内置模块试试：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042824.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042840.png)

注意，最小值都是0，最大值都是1。进一步注意，这种缩放的副作用是标准差都非常小。这有可能不利于某些模型，因为异常值的权重降低了。我们将新的标准化方法插入机器学习流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042857.png)

这是至今使用包括缺失数据的全部768行的最好结果。看起来min-max缩放对KNN有很大帮助！我们现在试试第三种标准化，把关注点从列转移到行上。

### 行归一化
最后这个标准化方法是关于行，而不是关于列的。行归一化不是计算每列的统计值（均值、最小值、最大值等），而是会保证每行有单位范数（unit norm），意味着每行的向量长度相同。想象一下，如果每行数据都在一个n维空间内，那么每行都有一个向量范数（长度）。也就是说，我们认为每行都是空间内的一个向量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507042956.png)

在皮马人数据集中n为8，每个特征一个（不包括响应），那么范数的计算方法是：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043040.png)

这是L2范数。其他类型的范数也存在，但是不在这里讨论。我们关心的是，让每行都有相同的范数。在使用文本数据或聚类算法时，这非常方便。

在开始之前，先看看用均值填充后的输入矩阵的平均范数，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043119.png)

我们用下面的代码引入行归一化：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043137.png)

在归一化后，所有行的范数都是1了。看看这个方法在流水线上表现如何：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043234.png)

不怎么样，但值得一试。现在我们知道了3种不同的数据标准化方法，把它们整合在一起，看看如何处理这个数据集吧。

很多算法会受尺度的影响，下面就是其中一些流行的学习算法：
1. KNN——因为依赖欧几里得距离；
2. K均值聚类——和KNN的原因一样；
3. 逻辑回归、支持向量机、神经网络——如果使用梯度下降来学习权重；
4. 主成分分析——特征向量将偏向较大的列。

### 整合起来
处理了数据集的各种问题（包括识别隐藏为0的缺失值，填充缺失值，以及按不同比例标准化数据），现在可以列出所有的得分，看看哪种办法最好。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507043347.png)

看来我们终于可以用均值填充和min-max标准化的方法得到最好的结果了，而且依旧使用全部的768列。不错！

特征增强的意义是，识别有问题的区域，并确定哪种修复方法最有效。我们的主要想法应该是用数据科学家的眼光看数据。我们应该考虑如何用最好的方法解决问题，而不是删除了事。一般来说，机器学习算法最终会因此取得让我们欣慰的表现。

## 特征构建：我能生成新特征吗
需要注意的是，之前使用的特征都是定量的。我们现在就开始转而研究分类数据。我们的主要目的是，使用现有特征构建全新的特征，让模型从中学习。

有很多方法可以构建新特征：最简单的办法是用Pandas将现有的特征扩大几倍；我们也会研究一些更依靠数学的方法，并使用scikit-learn包的很多部分；我们还会编写自己的类。稍后真正写代码时会进行深入研究。

我们会探讨如下主题：
1. 检查数据集；
2. 填充分类特征；
3. 编码分类变量；
4. 扩展数值特征；
5. 针对文本的特征构建。

### 检查数据集
为了进行演示，本章会使用我们自己创建的数据集，以便展示不同的数据等级和类型。我们先设置数据的DataFrame。

用Pandas创建要使用的DataFrame，这也是Pandas的主要数据结构。这样做的优点是可以用很多属性和方法操作数据，从而对数据进行符合逻辑的操作，以深入了解我们使用的数据，以及如何最好地构建机器学习模型。

1. 我们用Pandas的`DataFrame`方法创建表格数据结构（带行和列的表格）。这个方法可以接受不同类型的数据（例如，NumPy数组和字典等）。在本例中，我们传入一个字典，其键是列标题、值是列表，每个列表代表一列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234321.png)

2. ![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234332.png)

观察每一列，并识别每列的类型和等级。
1. boolean（布尔值）：此列是二元分类数据（是/否），定类等级。
2. city（城市）：此列是分类数据，也是定类等级。
3. ordinal_column（顺序列）：顾名思义，此列是顺序数据，定序等级。
4. quantitative_column（定量列）：此列是整数，定比等级。

### 填充分类特征
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234508.png)

有3列存在缺失值。接下来当然要填充这些值。

你应该还记得，上一章实现了scikit-learn的`Imputer`类，用于填充数值数据。`Imputer`的确有一个`most_frequent`方法可以用在定性数据上，但是只能处理整数型的分类数据。

我们不一定想这样做，因为这种转换会改变我们对分类数据的解释方式。因此我们要写一个自己的转换器，也就是一个填充每列缺失值的方法。

实际上，本章将构建好几个自定义转换器。这些转换器对转换数据帮助很大，而且可以进行Pandas和scikit-learn不支持的操作。

我们先从定性列city开始。对于数值数据，可以通过计算均值的方法填充缺失值；而对于分类数据，我们也有类似的处理方法：计算出最常见的类别用于填充。

为此，需要找出city列中最常见的类别。

>注意，要对这个列使用`value_counts`方法。这样会返回一个对象，由高到低包含列中的各个元素——第一个元素就是最常出现的。

我们只需要对象中的第一个元素：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234704.png)

我们注意到，tokyo是最频繁出现的城市。知道了应该用哪个值来填充，就可以开始处理了。`fillna`函数可以指定填充缺失值的方式：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234724.png)

city列现在是这样的：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507234811.png)

现在city列没有缺失值了。不过我们的另一个列boolean依然存在缺失值。我们不再使用同样的方法，而是构建一个自定义填充器，用来处理分类数据的填充。

#### 自定义填充器
在写代码之前，快速回顾一下机器学习流水线：
1. 我们可以用流水线按顺序应用转换和最终的预测器；
2. 流水线的中间步骤只能是转换，这意味着它们必须实现`fit`和`transform`方法；
3. 最终的预测器只需要实现`fit`方法。

流水线的目的是将几个可以交叉验证的步骤组装在一起，并设置不同的参数。在为每个需要填充的列构建好自定义转换器后，就可以把它们传入流水线，一口气转换好数据。

#### 自定义分类填充器
首先，用scikit-learn的`TransformerMixin`基类创建我们的自定义分类填充器。这个转换器（以及本章中的其他转换器）会作为流水线的一环，实现`fit`和`transform`方法。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235740.png)

1. 首先是一条`import`语句：
   
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235805.png)

2. 继承scikit-learn的`TransformerMixin`类，它包括一个`.fit_transform`方法，会调用我们创建的`.fit`和`.transform`方法。这能让我们的转换器和scikit-learn的转换器保持结构一致。我们初始化这个自定义的类：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235846.png)

3.  我们已经对这个自定义类进行了实例化，并用`__init__`方法对属性进行了初始化。在这里，只需要初始化一个实例属性`self.cols`（就是我们指定为参数的列）。现在可以构建`fit`和`transform`方法了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200507235932.png)

4.  上面是`transform`方法，它接收一个`DataFrame`。首先将这个`DataFrame`复制一份，命名为X。然后遍历`cols`参数指定的列，填充缺失值。`fillna`部分看起来很眼熟，因为这个函数在一开始的例子里用过。我们使用同样的函数并进行设置，这样一个填充器可以在很多列上同时工作。缺失值填充完毕后，返回`DataFrame`。然后是`fit`方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000605.png)

我们的`fit`方法只有`return self`一句话，和scikit-learn的标准`.fit`方法相同。

5. 我们在两列分类数据city和boolean上试验：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000642.png)

6. 我们初始化了一个自定义分类填充器，现在需要在数据集上调用`fit_transform`函数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000705.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508000714.png)

我们的city和boolean列都没有缺失值了。不过定量列还是有缺失值。既然默认的填充器不能选择列，我们再来自定义一个。

#### 自定义定量填充器
我们使用的结构和自定义分类填充器类似。主要的区别在于，此处用scikit-learn的`Imputer`类实现一个自定义的转换器，对列进行转换：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001138.png)

对于`CustomQuantitativeImputer`，我们添加了一个`strategy`参数，指定如何填充定量数据里的缺失值。这里用均值填充缺失值，依然使用`transform`和`fit`方法。

还是用`fit_transform`函数填充数据，这次我们指定要填充的列和`strategy`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001227.png)

也可以不分别调用并用`fit_transform`拟合转换`CustomCategoryImputer`和`Custom-QuantitativeImputer`，而是把它们放在流水线中。方法如下所示。

1. 写`import`语句：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001336.png)

2. 导入自定义填充器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001354.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508001402.png)

### 编码分类变量
我们目前已经填充了数据集——包括定量列和定性列。你可能在想：如何让机器学习算法利用分类数据呢？

需要将分类数据转换为数值数据。到目前为止，我们已经用最常见的类别对缺失值进行了填充。现在需要进行进一步操作。

任何机器学习算法，无论是线性回归还是利用欧几里得距离的KNN算法，需要的输入特征都必须是数值。有几种办法可以将分类数据转换为数值数据。

#### 定类等级的编码
我们从定类等级开始。主要方法是将分类数据转换为虚拟变量（dummy variable），有两种选择：

1. 用Pandas自动找到分类变量并进行编码；
2. 创建自定义虚拟变量编码器，在流水线中工作。

虚拟变量的取值是1或0，代表某个类别的有无。虚拟变量是定性数据的代理，或者说是数值的替代。

考虑一个简单的工资回归分析问题。假设给定了性别（定性数据）和工龄（定量数据）。为了考察性别对工资的影响，我们用虚拟变量：female = 0代表男性，female = 1代表女性。

当使用虚拟变量时，需要小心虚拟变量陷阱。虚拟变量陷阱的意思是，自变量有多重共线性或高度相关。简单地说，这些变量能依据彼此来预测。在这个例子中，如果设置female和male两个虚拟变量，它们都可以取值为1或0，那么就出现了重复的类别，陷入了虚拟变量陷阱。我们可以直接推断female = 0代表男性。

为了避免虚拟变量陷阱，我们需要忽略一个常量或者虚拟类别。被忽略的虚拟变量可以作为基础类别，和其他变量进行比较。

回到数据集中，用第一种选择将分类数据编码成虚拟变量。Pandas有个很方便的`get_dummies`方法，可以找到所有的分类变量，并将其转换为虚拟变量：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002515.png)

我们必须指定需要应用虚拟化的列，因为Pandas也会编码定序等级的列，这就没有意义了。稍后会提及为什么这种操作没有意义。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002615.png)

另一种选择是创建一个自定义虚拟化器，从而在流水线中一口气转换整个数据集。

再次使用之前两个自定义填充器的结构。在这里，我们的`transform`方法会利用`Pandas`的`get_dummies`方法，为指定的列创建虚拟变量。该自定义虚拟化器中唯一的参数是`cols`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002657.png)

我们的自定义虚拟化器模仿了scikit-learn的`OneHotEncoding`，但是可以在整个`DataFrame`上运行。

最后实例化自定义虚拟化器，保证后面的代码可以运行。运行如下代码：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508002737.png)

#### 定序等级的编码
现在我们关注定序等级的列。这个等级上仍然存在有用的信息，然而我们需要将字符串转换为数值数据。在定序等级，由于数据的顺序有含义，使用虚拟变量是没有意义的。为了保持顺序，我们使用标签编码器。

标签编码器是指，顺序数据的每个标签都会有一个相关数值。在我们的例子中，这意味着顺序列的值（dislike、somewhat like和like）会用0、1、2来表示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508111224.png)

这里创建了一个列表，用于对标签排序。这一步是关键，我们会用其索引将标签转换为数值数据。

在列上实现一个`map`函数，允许我们指定需要在列上实现的函数。我们指定该函数使用`lambda`匿名函数，即不绑定到某个名称：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508113435.png)

这行代码会创建一个函数，将列表的索引ordering分配到各个元素上。现在将其映射到顺序列上：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508113450.png)

顺序列现在变成了带标签的数据。

注意，我们没有使用scikit-learn的`LabelEncoder`，因为这个方法不能像上面的代码那样对顺序进行编码（0表示dislike，1表示somewhat like，2表示like）。它默认是一个排序方法，而我们不想这么做。

还是将自定义标签编码器放进流水线中：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508113613.png)

我们保留了之前其他自定义转换器的结构。此处用上面详述的`map`和`lambda`函数对特定的列进行转换。注意，关键参数是`ordering`，它会指定将标签编码成什么数值。

调用我们的自定义编码器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114620.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114657.png)

顺序列已经被编码了。到这里，我们已经转换了如下这些列。
1. boolean和city：虚拟变量编码。
2. ordinal_column：标签编码。

#### 将连续特征分箱
有时，如果数值数据是连续的，那么将其转换为分类变量可能是有意义的。例如你的手上有年龄，但是年龄段可能会更有用。

Pandas有一个有用的函数叫作cut，可以将数据分箱（binning），亦称为分桶（bucketing）。意思就是，它会创建数据的范围。

我们在quantitative_column列上看看它的作用：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114843.png)

对于我们的定量列，cut函数的输出如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508114924.png)

当指定的bins为整数的时候（bins = 3），会定义X范围内的等宽分箱数。然而在本例中，X的范围向两边分别扩展了0.1%，以包括最小值和最大值。

也可以将标签设置为`False`，这将只返回分箱的整数指示器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115045.png)

quantitative_column列的整数指示器如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115059.png)

利用`cut`函数的属性，可以为流水线定义自己的`CustomCutter`。再次仿照之前转换器的结构。我们的`transform`方法会利用`cut`，所以需要bins和labels作为参数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115158.png)

注意，labels的默认值是`False`。我们可以初始化`CustomCutter`，输入需要转换的列和分箱数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508115643.png)

经过`CustomCutter`转换后的`quantitative_column`列如下表中所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120039.png)

注意，现在quantitative_column列处于定序等级，不需要引入虚拟变量。

#### 创建流水线
回顾一下，我们对数据集里的列进行了以下这些转换。
1. boolean和city：虚拟变量编码。
2. ordinal_column：标签编码。
3. quantitative_column：分箱。

回顾一下，我们对数据集里的列进行了以下这些转换。
1. boolean和city：虚拟变量编码。
2. ordinal_column：标签编码。
3. quantitative_column：分箱。

既然已经转换了所有的列，就可以组装流水线了。我们从scikit-learn的`Pipeline`开始：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120421.png)

把每列的自定义转换器放在一起。我们流水线的顺序是：

1. 用`imputer`填充缺失值；
2. 用虚拟变量填充分类列；
3. 对ordinal_column进行编码；
4. 将quantitative_column分箱。

这样设置流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120521.png)

为了观察流水线对数据的完整转换，我们先看看尚未转换的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120541.png)

转换前的数据如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120557.png)

我们可以对流水线进行拟合：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120634.png)

创建流水线对象后，可以转换`DataFrame`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120651.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508120700.png)

### 扩展数值特征
有多种办法可以从数值特征中创建扩展特征。之前我们研究了如何将连续的数值数据转换为顺序数据，现在开始进一步扩展数值特征。

#### 根据胸部加速度计识别动作的数据集
这个数据集来自佩戴在胸部的加速度计，它收集了15名参与者的7种动作。采样频率是52Hz，加速度计数据未校准。数据集按参与者划分，包含以下内容：
1. 序号；
2. x轴加速度；
3. y轴加速度；
4. z轴加速度；
5. 标签。

标签是数字，每个数字代表一种动作（activity），如下所示：

1. 在电脑前工作；
2. 站立、走路和上下楼梯；
3. 站立；
4. 走路；
5. 上下楼梯；
6. 与人边走边聊；
7. 站立着讲话。

我们开始研究数据。首先加载CSV文件，并设置每列标题：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121432.png)

用`.head`方法查看前几行。如果没有特殊设置，默认查看前5行。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121453.png)

这个数据集的目的是训练模型，以便根据智能手机等设备上加速度计的x、y、z读数识别用户的当前动作。根据上述网站可知，activity列的数字有如下意义。

1. 1：在电脑前工作
2. 2：站立、走路和上下楼梯
3. 3：站立
4. 4：走路
5. 5：上下楼梯
6. 6：与人边走边聊
7. 7：站立着讲话

我们的目标是预测activity列。首先确定要击败的空准确率。调用`value_counts`方法，将`normalize`选项设为`True`，以百分比的形式列出最常见的动作：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121635.png)

空准确率是51.54%，意味着如果我们猜7（站立着讲话），正确率就超过一半了。现在开始进行机器学习，一步步建立模型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121659.png)

你可能已经熟悉上一章中用过的这些语句了。我们还是用scikit-learn的KNN分类模型。依旧采用网格搜索模块，自动找到最适合数据的KNN参数组合，以达到最佳的交叉验证准确率。然后，为预测模型创建一个特征矩阵（X）和一个响应变量（y）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121910.png)

设定好X和y之后，就可以引入网格搜索所需的变量和实例了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508121926.png)

然后，我们实例化一个KNN模型和一个网格搜索模块，并且用特征矩阵和响应变量拟合：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508122644.png)

现在可以打印出最佳准确率和学习到的参数了：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508122658.png)

使用5个邻居作为参数时，KNN模型准确率达到了72.08%，比51.54%的空准确率高得多。也许还有别的办法可以进一步提高准确率。

#### 多项式特征
在处理数值数据、创建更多特征时，一个关键方法是使用scikit-learn的Polynomial-Features类。这个构造函数会创建新的列，它们是原有列的乘积，用于捕获特征交互。

更具体地说，这个类会生成一个新的特征矩阵，里面是原始数据各个特征的多项式组合，阶数小于或等于指定的阶数。意思是，如果输入是二维的，例如[a, b]，那么二阶的多项式特征就是[1, a, b, a^2, ab, b^2]。

##### 参数
在实例化多项式特征时，需要了解3个参数：
1. degree
2. interaction_only
3. include_bias

degree是多项式特征的阶数，默认值是2。

interaction_only是布尔值：如果为真，表示只生成互相影响/交互的特征，也就是不同阶数特征的乘积。interaction_only默认为false。

include_bias也是布尔值：如果为真（默认），会生成一列阶数为0的偏差列，也就是说列中全是数字1。

我们先导入这个多项式特征类，并设置参数来实例化。首先看看将`interaction_only`设成`False`时的数据：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132326.png)

然后调用`fit_transform`函数，拟合多项式特征，并观察扩展后数据集的形状：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132656.png)

把数据放进`DataFrame`，将列标题设置为feature_names，查看前几行：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132724.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132735.png)

##### 探索性数据分析
现在可以进行一些探索性数据分析了。因为多项式特征的目的是更好地理解原始数据的特征交互情况，所以最好的可视化办法是关联热图。

Matplotlib和Seaborn都是流行的数据可视化工具。我们可以用如下方法创建关联热图：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508132826.png)

`.corr`是一个可以在`DataFrame`上的调用的函数，返回相关性矩阵。我们看看特征交互情况，如下图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152327.png)

目前，interaction_only参数是`False`。我们不重新建立变量，将参数设成`True`试试。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152520.png)

矩阵有162501行和6列。仔细观察一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152620.png)

`DataFrame`如下表所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152637.png)

因为`interaction_only`为真，x0^2、x1^2和x2^2都消失了，因为这几列不和其他列交互。我们看看相关性矩阵：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152658.png)

可以看见特征是如何互相影响的。我们还可以用新的多项式特征对KNN模型进行网格搜索，这也可以在流水线中进行。

1. 先设置流水线参数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152756.png)

2. 然后实例化流水线：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152810.png)

3. 最后设置网格搜索，打印最佳准确率和学习到的参数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508152902.png)

现在的准确率是72.12%，比之前不用多项式扩展的准确率有所提高。

### 针对文本的特征构建
到目前为止，我们一直在处理分类数据和数值数据。虽然分类数据是字符串，但是里面的文本仅仅是某个类别。我们现在进一步探索更长的文本数据。这种文本数据比单个类别的文本复杂得多，因为长文本包括一系列类别，或称为词项（token）。

在进一步研究之前，我们要保证对文本有了充分的理解。考虑一下商户点评服务：用户在平台上撰写对餐厅和商家的评论，分享对自己体验的看法。这些评论都是文本格式的，包含可用于机器学习的大量有用信息，例如预测最应该去的餐厅。

总体来说，在当今世界中，我们沟通方式的很大一部分还是基于书面文本，无论使用的是聊天服务、社会媒体，还是电子邮件。通过建模，我们可以从中获得海量信息，例如用Twitter数据进行情绪分析。

这种工作叫作自然语言处理（NLP，natural language processing）。这个领域主要涉及计算机与人类的交流，特别是对计算机进行编程，以处理自然语言。

之前提到过，所有的机器学习模型都需要数值输入。因此处理文本时需要有创造性，有策略地思考如何将文本数据转换为数值特征。有几个办法可以做到，我们开始学习吧。

#### 词袋法
scikit-learn有一个`feature_extraction`模块，非常方便。顾名思义，它能以机器学习算法支持的方法提取数据的特征，包括文本数据。这个模块包括处理文本时需要使用的一些方法。

接下来，我们可能会将文本数据称为语料库（corpus），尤其是指文本内容或文档的集合。

将语料库转换为数值表示（也就是向量化）的常见方法是词袋（bag of words），其背后的基本思想是：通过单词的出现来描述文档，完全忽略单词在文档中的位置。在它最简单的形式中，用一个袋子表示文本，不考虑语法和词序，并将这个袋子视作一个集合，其中重复度高的单词更重要。词袋的3个步骤是：

1. 分词（tokenizing）；
2. 计数（counting）；
3. 归一化（normalizing）。

首先介绍分词。分词过程是用空白和标点将单词分开，将其变为词项。每个可能出现的词项都有一个整数ID。然后是计数。简单地计算文档中词项的出现次数。最后是归一化。将词项在大多数文档中的重要性按逆序排列。下面了解另外几个向量化方法。

#### `CountVectorizer`
`CountVectorizer`是将文本数据转换为其向量表示的最常用办法。和虚拟变量类似，`CountVectorizer`将文本列转换为矩阵，其中的列是词项，单元值是每个文档中每个词项的出现次数。这个矩阵叫文档-词矩阵（document-termmatrix），因为每行代表一个文档（在本例中是一条推文），每列代表一个词（一个单词）。

我们用一个新的数据集展示`CountVectorizer`的工作原理。Twitter情感分析数据集包括1578627条分类后的推文，每行标记为1或0：前者代表正面情绪，后者代表负面情绪。

我们用Pandas的`read_csv`方法读入数据。注意我们指定了可选参数`encoding`，这是为了保证所有的特殊字符都可以正常处理。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155134.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155150.png)

我们只关注Sentiment和SentimentText列，所以删除ItemID列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155213.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155225.png)

现在可以导入CountVectorizer，更好地理解这些文本：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155239.png)

然后设置X和y：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155317.png)

`CountVectorizer`和我们一直使用的自定义转换器非常类似，也有操作数据的`fit_trans-form`函数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508155334.png)

用`CountVectorizer`转换后，数据有99989行和105849列。

`CountVectorizer`有很多参数，可以控制构建特征的数量。下面研究其中的一些，更好地了解如何构建特征。

##### `CountVectorizer`的参数
我们会介绍以下几个参数：
1. stop_words
2. min_df
3. max_df
4. ngram_range
5. analyzer

`stop_words`参数很常用。如果向其传入字符串`english`，那么CountVectorizer会使用内置的英语停用词列表。你也可以自定义停用词列表。这些词会从词项中删除，不会表示为特征。

例如：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160023.png)

可以看见，使用英语停用词后，特征列从105849下降到105545。停用词的意义在于消除特征的噪声，去掉在模型中意义不大的常用词。

另一个参数叫`min_df`。它通过忽略在文档中出现频率低于阈值的词，减少特征的数量。

使用`min_df`的CountVectorizer如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160117.png)

这极为有效地减少了特征数。

还有一个参数叫`max_df`：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160146.png)

这类似于试图理解文档中有哪些停用词。

接下来看看`ngram_range`参数。这个参数接收一个元组，表示`n`值的范围（代表要提取的不同`n-gram`的数量）上下界。`n-gram`代表短语：若`n= 1`，则其是一个词项；若`n= 2`，则其代表相邻的两个词项。可以预想到，这个方法会显著地增加特征集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160313.png)

冒出来了3219557个特征。因为短语可能有其他含义，所以调整这个参数会对建模有帮助。

`CountVectorizer`还可以设置分析器作为参数，以判断特征是单词还是短语。默认是单词：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508160451.png)

因为默认就是划分为单词，所以特征列结果变化不大。

我们甚至可以创建自定义分析器。理论上说，单词是由词根或词干构建而来的，所以可以据此写一个自己的分析器。

`CountVectorizer`是一个非常有用的工具，不仅可以扩展特征，还可以将文本转换为数值特征。

#### TF-IDF向量化器
TF-IDF向量化器由两部分组成：表示词频的TF部分，以及表示逆文档频率的IDF部分。TF-IDF是一个用于信息检索和聚类的词加权方法。

对于语料库中的文档，TF-IDF会给出其中单词的权重，表示重要性。我们把每个部分拆开来看。

1. TF（term frequency，词频）：衡量词在文档中出现的频率。由于文档的长度不同，词在长文中的出现次数有可能比在短文中出现的次数多得多。因此，一般会对词频进行归一化，用其除以文档长度或文档的总词数。
2. IDF（inverse document frequency，逆文档频率）：衡量词的重要性。在计算词频时，我们认为所有的词都同等重要。但是某些词（如is、of和that）有可能出现很多次，但这些词并不重要。因此，我们需要减少常见词的权重，加大稀有词的权重。

再次强调，`TfidfVectorizer`和`CountVectorizer`相同，都从词项构造了特征，但是`TfidfVectorizer`进一步将词项计数按照在语料库中出现的频率进行了归一化。我们看一个例子。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161339.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161349.png)

还是之前的代码，用`CountVectorizer`生成文档-词矩阵：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161533.png)

按此设置`TfidfVectorizer`：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161552.png)

可以看到，两个向量化器输出的行列数相同，但是里面的值不同。这是因为虽然`TfidfVectorizer`和`CountVectorizer`都可以把文本数据转换为定量数据，但是填充单元值的方法不同。

#### 在机器学习流水线中使用文本
当然，向量化器的最终目标都是让机器学习流水线理解文本数据。因为`CountVectorizer`和`TfidfVectorizer`与本书中的其他转换器一样，所以要使用scikit-learn流水线保证机器学习流水线的准确率和诚实度。本例要处理大量的列（数十万），所以我们使用在这种情况下更高效的分类器——朴素贝叶斯（naive Bayes）模型：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161739.png)

在开始构建流水线之前，取响应列的空准确率（0是负面情绪，1是正面情绪）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161754.png)

要让准确率超过56.5%。我们分两步创建流水线：

1. 用`CountVectorizer`将推文变成特征；
2. 用朴素贝叶斯模型`MultiNomialNB`进行正负面情绪的分类。

首先设置流水线的参数，然后实例化网格搜索：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161840.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508161923.png)

结果是75.6%，很不错！现在进一步调优，加入`TfidfVectorizer`。这次我们尝试新的做法，比简单地利用TF-IDF建立流水线高级一些。scikit-learn有一个`FeatureUnion`模块，可以水平（并排）排列特征。这样，在一个流水线中可以使用多种类型的文本特征构建器。

例如，可以构建一个`featurizer`对象，在推文上使用`TfidfVectorizer`和`CountVecto-rizer`，并且并排排列推文（行数相同，增加列数）：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162020.png)

然后可以看见数据的变化情况：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162110.png)

可以看到，结合两个特征构建器后的数据集行数相同，但是因为`TfidfVectorizer`和`CountVectorizer`并排，所以列数加倍。这样做可以让机器学习模型同时从两组数据中学习。我们稍稍改变`featurizer`对象的参数，看看效果：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162135.png)

我们建立一个更完整的流水线，包括两个向量化器的特征结合：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508162152.png)

这比单独使用`CountVectorizer`好多了。值得注意的是，`CountVectorizer`的最佳`ngram_range`是(1, 2)，而`TfidfVectorizer`的是(1, 1)，代表单个词的出现没有2个单词的短语那么重要。

至此，我们知道以下方法可以让流水线更加复杂：

1. 对向量化器的几十个参数使用网格搜索；
2. 在流水线上添加步骤，例如多项式特征构造。

## 特征选择：对坏属性说不
本书过半，我们处理了十余个数据集，并学习了大量特征选择方法。作为数据科学家和机器学习工程师，我们可以在工作和生活中利用这些方法，以保证充分利用预测模型。目前为止，在处理数据方面，我们已经使用了如下方法。
1. 特征理解：理解数据的等级。
2. 特征增强：填充缺失值。
3. 特征标准化和正则化。

每个方法在数据流水线中都有一席之地，更常见的现象是，我们会串联两个或多个处理方法。

本书剩余部分将着重介绍特征工程中更涉及数学、更加复杂的其他一些方法。随着工作流程的增长，我们会尽量不去讲解每个统计测试的内部原理，而是关注大局，让你理解测试要达到的目标。

讨论特征时经常遇到噪声问题。通常，我们手上的特征有可能预测性不高，有时甚至会阻碍模型的预测性能。我们使用过标准化和正则化等方法来减轻其危害，但是总有一天需要解决这种问题。

本章会讨论特征工程的一个子集，称为特征选择。特征选择是从原始数据中选择对于预测流水线而言最好的特征的过程。更正式地说，给定`n`个特征，我们搜索其中包括`k`（`k＜ n`）个特征的子集来改善机器学习流水线的性能。一般来说，我们的意思是：

特征选择尝试剔除数据中的噪声。

这个定义包括两个需要解决的问题：
1. 找到k特征子集的办法；
2. 在机器学习中对“更好”的定义。

本章的大部分内容着重讲解寻找这类子集的方法，及其工作原理的基础。本章将特征选择的方法分为两大类：基于统计的特征选择，以及基于模型的特征选择。这种分类也许不能100%捕捉到特征选择在科学性和艺术性上的复杂程度，但是可以推动机器学习流水线输出真实、可应用的结果。

### 在特征工程中实现更好的性能
在本书中，当我们讨论特征工程的方法时，需要对“更好”下定义。实际上，我们的目标是实现更好的预测性能，而且仅使用简单的指标进行测量，例如分类任务的准确率和回归任务的均方根误差（大部分是准确率）。我们还可以测量和跟踪其他指标，以评估预测的性能。例如，分类任务可以使用如下指标：

1. 真阳性率和假阳性率；
2. 灵敏度（真阳性率）和特异性；
3. 假阴性率和假阳性率。

回归任务则可以使用：

1. 平均绝对误差；
2. R2。

这个列表还可以继续延长。虽然我们不会放弃用以上指标量化性能的想法，但是也可以测量其他元指标。元指标是指不直接与模型预测性能相关的指标，它们试图衡量周遭的性能，包括：

1. 模型拟合/训练所需的时间；
2. 拟合后的模型预测新实例的时间；
3. 需要持久化（永久保存）的数据大小。

这补充了更好的定义，因为这些指标在预测性能之外涵盖了机器学习流水线的更多方面。为了跟踪这些指标，我们可以创建一个函数，通用到足以评估若干模型，同时精细到可以提供每个模型的指标。我们会利用`get_best_model_and_accuracy`函数，完成以下任务：

1. 搜索所有给定的参数，优化机器学习流水线；
2. 输出有助于评估流水线质量的指标

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508163736.png)

这个函数的总体目标是给出一个基线数据，因为我们会用这个函数评估每个特征选择方法，带来一种标准化的感觉。虽然本质上和之前的工作没什么区别，但是这次把工作形式化成函数，而且用另外的指标为机器学习流水线和特征选择模块打分，而不是只看准确率。

#### 案例分析：信用卡逾期数据集
特征选择算法可以智能地从数据中提取最重要的信号并忽略噪声，达到以下两个结果。

1. 提升模型性能：在删除冗余数据后，基于噪声和不相关数据做出错误决策的情况会减少，而且模型可以在重要的特征上练习，提高预测性能。
2. 减少训练和预测时间：因为拟合的数据更少，所以模型一般在拟合和训练上有速度提升，让流水线的整体速度更快。

为了更好地理解噪声以及为什么噪声有妨碍作用，我们介绍一个新的数据集：信用卡逾期数据集。我们使用23个特征和一个响应变量。这个变量是一个布尔值，可以是`True`（真）或`False`（假）。我们想知道，能否在23个特征中找出对机器学习流水线有帮助和有害的特征。用以下代码导入数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164655.png)

先导入两个常见模块numpy和pandas，并设置随机数种子，保持运行结果一致。然后，用以下代码导入数据集：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164708.png)

先进行基本的探索性数据分析。检查一下数据集的大小，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164723.png)

数据有30000行（观察值）和24列（1个响应，23个特征）。我们先使用传统的统计方法：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164751.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508164812.png)

default payment next month（下个月逾期）是响应，其他都是特征，或者说是潜在的预测变量。很明显，特征的尺度迥异，这会是我们选择数据处理方法和模型时需要考虑的因素。在前面的章节中，我们使用`StandardScalar`和归一化解决了这些问题，而本章会忽略这些问题，以便集中处理更相关的问题。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165003.png)

太好了，没有缺失值。我们在之后的案例分析中会再次处理缺失值，但是现在有更重要的事情要做。接下来为机器学习流水线设置变量，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165034.png)

和往常一样创建X和y变量。X矩阵有30000行和23列，而y是长度为30000的Pandas Series。因为要执行分类任务，所以取一个空准确率，确保机器学习的性能比基准更好。代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165055.png)

本例需要击败77.88%这个准确率，也就是没有逾期者的比例（0代表没有逾期）。

### 创建基准机器学习流水线
在前几章里，我们都提供了一个全章通用的机器学习模型。在本章中，我们会做一些工作，寻找最符合我们需求的机器学习模型，然后通过特征选择来增强模型。先导入4种模型：

1. 逻辑回归；
2. K最近邻（KNN）；
3. 决策树；
4. 随机森林。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165729.png)

导入后执行`get_best_model_and_accuracy`函数，取得每个模型处理原始数据的基准。需要先建立一些变量，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165753.png)

因为我们通过函数设置模型，而这会调用一个网格搜索模型，所以只需要创建空白模型即可，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508165816.png)

我们在所有的模型上运行评估函数，了解一下效果的好坏。记住，我们要击败的精确率是0.7788，也就是基线空准确率。运行模型的代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508170321.png)

可以看见，逻辑回归只用原始数据就打败了空准确率。它拟合训练集平均需要0.6s，而只用20ms就可以得出结果。这其实是有道理的：要拟合数据，scikit-learn的逻辑回归需要在内存中创建一个巨大的矩阵，但是预测时只需要相乘并做一点标量计算。

我们在KNN上做同样的处理：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508170358.png)

不出所料，KNN在拟合时间上表现得更好。因为在拟合时，KNN只需要按方便检索和及时处理的方法存储数据。注意，这里的准确率甚至不如空准确率！你有可能在考虑原因是什么。如果你想到“等等，KNN是按照欧几里得距离进行预测的，在非标准数据上可能会失效，但是其他3个算法不会受此影响”，那么你是对的。

KNN是基于距离的模型，使用空间的紧密度衡量，假定所有的特征尺度相同，但是我们知道数据并不是这样。因此对于KNN，我们需要更复杂的流水线，以更准确地评估基准性能。代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508170800.png)

首先注意，在用`StandardScalar`进行z分数标准化处理后，这个流水线的准确率至少比空准确率要高，但是这也严重影响了预测时间，因为多了一个预处理步骤。目前，逻辑回归依然领先：准确率更高，速度更快。我们继续讨论两个基于树的模型，从更简单的决策树开始：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508171906.png)

真厉害！现在决策树的准确率是第一，而且拟合和预测的速度也很快。实际上，决策树的拟合速度比逻辑回归快，预测速度比KNN快。我们最后测试一下随机森林，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508171940.png)

比逻辑回归和KNN好得多，但是没有决策树好。我们汇总一下结果，看看应该使用哪个模型。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508172004.png)

决策树的准确率最高，并且预测时间和逻辑回归并列第一，而带缩放的KNN拟合最快。总体而言，决策树应该是最适合下一步采用的模型，因为它在两个最重要的指标上领先：

1. 我们想要最高的准确率，以保证预测的准确性；
2. 考虑到实时生产环境，预测时间低大有裨益。

我们使用的办法是在选择特征之前选择模型。虽然不必要，但是在时间有限的情况下这样做一般很省时。你可以尝试多种模型，不必拘泥于一个模型。

既然知道了要使用决策树，那么：
1. 要击败的新基线准确率是0.8203，即拟合整个数据集的准确率；
2. 不再需要`StandardScaler`了，因为决策树不受其影响。

### 特征选择的类型
选择特征是为了提高预测能力，降低时间成本。所以这里介绍两种类型：基于统计和基于模型的特征选择。基于统计的特征选择很大程度上依赖于机器学习模型之外的统计测试，以便在流水线的训练阶段选择特征。基于模型的特征选择则依赖于一个预处理步骤，需要训练一个辅助的机器学习模型，并利用其预测能力来选择特征。

这两种类型都试图从原始特征中选择一个子集，减少数据大小，只留下预测能力最高的特征。我们可以依靠自己的智慧来选择特征，但实际上通过每种方法的例子对相应的流水线性能进行测量也很有效。

首先，我们研究如何依靠统计测试从数据集中选择可行的特征。

#### 基于统计的特征选择
通过统计数据，我们可以快速、简便地解释定量和定性数据。前几章使用了一些统计方法来获取关于数据的新知识和新看法，特别是我们认识到，均值和标准差是计算z分数和数据缩放的指标。本章会使用两个新概念帮我们选择特征：

1. 皮尔逊相关系数（Pearson correlations）；
2. 假设检验。

这两个方法都是单变量方法。意思是，如果为了提高机器学习流水线性能而每次选择单一特征以创建更好的数据集，这种方法最简便。

##### 使用皮尔逊相关系数

我们其实已经见过相关系数了，但并非用于特征选择。我们已经知道，可以这样计算相关系数：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174151.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174201.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174253.png)

皮尔逊相关系数（是Pandas默认的）会测量列之间的线性关系。该系数在-1～1变化，0代表没有线性关系。相关性接近-1或1代表线性关系很强。

值得注意的是，皮尔逊相关系数要求每列是正态分布的（我们没有这样假设）。在很大程度上，我们也可以忽略这个要求，因为数据集很大（超过500的阈值）

Pandas的`.corr()`方法会为所有的列计算皮尔逊相关系数。这个24×24的矩阵很难读，我们用热图优化一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174348.png)

注意，`heatmap`函数会自动选择最相关的特征进行展示。不过，我们目前关注特征和响应变量的相关性。我们假设，和响应变量越相关，特征就越有用。不太相关的特征应该没有什么用。

也可以用相关系数确定特征交互和冗余变量。发现并删除这些冗余变量是减少机器学习过拟合问题的一个关键方法。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174526.png)

最后一行可以忽略，因为这是响应变量和自己的相关性。我们寻找相关系数接近-1或1的特征，因为这些特征应该会对预测有用。我们用Pandas过滤出相关系数超过正负0.2的特征。

先定义一个Pandas mask作为过滤器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174603.png)

上面Pandas Series中的False代表特征的相关系数在-0.2～0.2，`True`则代表相关系数超过了正负0.2。我们用下面的代码结合这个mask：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174626.png)

highly_correlated_features变量会存储与响应变量高度相关的特征，但是需要删掉响应列的名称，因为在机器学习流水线中包括这列等于作弊：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174746.png)

留下原始数据集的5个特征，用于预测响应变量。我们尝试一下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508174802.png)

我们的准确率比要击败的准确率0.8203略差，但是拟合时间快了大概20倍。我们的模型只需要5个特征就可以学习整个数据集，而且速度快得多。

接下来回顾一下scikit-learn流水线，将相关性选择作为预处理阶段的一部分。我们需要创建一个自定义转换器调用刚才的逻辑，并封装为流水线可以使用的类。

将这个类命名为`CustomCorrelationChooser`，它会实现一个拟合逻辑和一个转换逻辑。

1. 拟合逻辑：从特征矩阵中选择相关性高于阈值的列。
2. 转换逻辑：对数据集取子集，只包含重要的列。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175644.png)

运行下面的代码试试新的相关特征选择器：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175702.png)

这个类的选择和之前一样。我们在X矩阵上应用转换，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175728.png)

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175735.png)

我们看见`transform`方法删除了其他列，只保留大于0.2阈值的列。现在在流水线中把一切组装起来：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508175859.png)

哇！第一次的特征选择就已经打败了目标（虽然只高一点点）。我们的流水线显示，如果把阈值设为0.1，就足以消除噪声以提高准确性，并缩短拟合时间（之前是0.158s）。下面看看选择器保留了哪些列：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508180322.png)

选择器保留了我们找到的5列，以及LIMIT_BAL和PAY_6列。这就是scikit-learn中自动化网格搜索的好处，让模型达到最优，解放我们的劳动力。

##### 使用假设检验
假设检验是一种统计学方法，可以对单个特征进行复杂的统计检验。在特征选择中使用假设检验可以像之前的自定义相关选择器一样，尝试从数据集中选择最佳特征，但是这里的检验更依赖于形式化的统计方法，并通过所谓的p值进行检验。

作为一种统计检验，假设检验用于在给定数据样本时确定可否在整个数据集上应用某种条件。假设检验的结果会告诉我们是否应该相信或拒绝假设（并选择另一个假设）。基于样本数据，假设检验会确定是否应拒绝零假设。我们通常会用p值（一个上限为1的非负小数，由显著性水平决定）得出结论。

在特征选择中，假设测试的原则是：“特征与响应变量没有关系”（零假设）为真还是假。我们需要在每个特征上进行检验，并决定其与响应变量是否有显著关系。在某种程度上说，我们的相关性检测逻辑也是这样运作的。我们的意思是，如果某个特征与响应变量的相关性太弱，那么认为“特征与响应变量没有关系”这个假设为真。如果相关系数足够强，那么拒绝该假设，认为特征与响应变量有关。

在将其用于数据之前，需要定义新模块`SelectKBest`和`f_classif`，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200508183849.png)

`SelectKBest`基本上就是包装了一定数量的特征，而这些特征是根据某个标准保留的前几名。在这里，我们使用假设检验的p值作为排名依据。









