<!-- TOC -->

- [机器学习概述](#机器学习概述)
  - [何谓机器学习](#何谓机器学习)
    - [机器学习应用开发步骤](#机器学习应用开发步骤)
      - [定义问题](#定义问题)
      - [数据采集](#数据采集)
      - [数据清洗](#数据清洗)
      - [特征选择与处理](#特征选择与处理)
      - [训练模型](#训练模型)
      - [模型评估与调优](#模型评估与调优)
      - [模型使用](#模型使用)
  - [集成学习发展与XGBoost提出](#集成学习发展与xgboost提出)
    - [集成学习](#集成学习)
      - [Boosting](#boosting)
      - [Bagging](#bagging)
      - [Stacking](#stacking)
    - [XGBoost](#xgboost)
- [机器学习算法基础](#机器学习算法基础)
  - [决策树](#决策树)
    - [构造决策树](#构造决策树)
    - [特征选择](#特征选择)
      - [ID3](#id3)
        - [信息增益](#信息增益)
        - [ID3算法过程](#id3算法过程)
      - [C4.5](#c45)
      - [CART](#cart)
    - [决策树剪枝](#决策树剪枝)
    - [决策树解决肿瘤分类问题](#决策树解决肿瘤分类问题)
- [XGBoost小试牛刀](#xgboost小试牛刀)
  - [XGBoost实现原理](#xgboost实现原理)
  - [二分类问题](#二分类问题)
  - [多分类问题](#多分类问题)
  - [回归问题](#回归问题)
    - [`objective`](#objective)
    - [`eval_metric`](#eval_metric)
  - [排序问题](#排序问题)
    - [排序学习算法](#排序学习算法)
      - [Pointwise方法](#pointwise方法)
      - [Pairwise方法](#pairwise方法)
      - [Listwise方法](#listwise方法)
    - [排序评价指标](#排序评价指标)
      - [精确率、召回率和F-Score](#精确率召回率和f-score)
      - [AUC](#auc)
      - [MAP](#map)
      - [NDCG](#ndcg)
  - [其他常用功能](#其他常用功能)
    - [将DMatrix保存为二进制文件](#将dmatrix保存为二进制文件)
    - [](#)

<!-- /TOC -->
# 机器学习概述
## 何谓机器学习
### 机器学习应用开发步骤
#### 定义问题
#### 数据采集
#### 数据清洗
通过数据采集得到的原始数据可能并不规范，需对数据进行清洗才能满足使用需求。例如，去掉数据集中的重复数据、噪声数据，修正错误数据等，最后将数据转换为需要的格式，以方便后续处理。
#### 特征选择与处理
特征选择是在原始特征中选出对模型有用的特征，去除数据集中与模型预测无太大关系的特征。通过分析数据，可以人工选择贡献较大的特征，也可以采用类似PCA等算法进行选择。此外，还要对特征进行相应处理，如对数值型特征进行标准化，对类别型特征进行one-hot编码等。
#### 训练模型
特征数据准备完成后，即可根据具体任务选择合适的模型并进行训练。对于监督学习，一般会将数据集分为训练集和测试集，通过训练集训练模型参数，然后通过测试集测试模型精度。而无监督学习则不需对算法进行训练，而只需通过算法发现数据的内在结构，发现其中的隐藏模式即可。
#### 模型评估与调优
#### 模型使用
## 集成学习发展与XGBoost提出
集成学习是目前机器学习领域最热门的研究方向之一，近年来许多机器学习竞赛的冠军均使用了集成学习，而XGBoost是集成学习中集大成者。
### 集成学习
集成学习的基本思想是把多个学习器通过一定方法进行组合，以达到最终效果的提升。虽然每个学习器对全局数据的预测精度不高，但在某一方面的预测精度可能比较高，俗话说“三个臭皮匠顶个诸葛亮”，将多个学习器进行组合，通过优势互补即可达到强学习器的效果。集成学习最早来自于Valiant提出的PAC（Probably ApproximatelyCorrect）学习模型，该模型首次定义了弱学习和强学习的概念：识别准确率仅比随机猜测高一些的学习算法为弱学习算法；识别准确率很高并能在多项式时间内完成的学习算法称为强学习算法。该模型提出给定任意的弱学习算法，能否将其提升为强学习算法的问题。1990年，Schapire对其进行了肯定的证明。这样一来，我们只需要先找到一个弱学习算法，再将其提升为强学习算法，而不用一开始就找强学习算法，因为强学习算法比弱学习算法更难找到。目前集成学习中最具代表性的方法是：Boosting、Bagging和Stacking。
#### Boosting
简单来讲，Boosting会训练一系列的弱学习器，并将所有学习器的预测结果组合起来作为最终预测结果，在学习过程中，后期的学习器更关注先前学习器学习中的错误。1995年，Freund等人提出了AdaBoost，成为了Boosting代表性的算法。AdaBoost继承了Boosting的思想，并为每个弱学习器赋予不同的权值，将所有弱学习器的权重和作为预测的结果，达到强学习器的效果。Gradient Boosting是Boosting思想的另外一种实现方法，由Friedman于1999年提出。与AdaBoost类似，Gradient Boosting也是将弱学习器通过一定方法的融合，提升为强学习器。与AdaBoost不同的是，它将损失函数梯度下降的方向作为优化的目标，新的学习器建立在之前学习器损失函数梯度下降的方向，代表算法有GBDT、XGBoost等。一般认为，Boosting可以有效提高模型的准确性，但各个学习器之间只能串行生成，时间开销较大。
#### Bagging
Bagging（Bootstrap Aggregating）对数据集进行有放回采样，得到多个数据集的随机采样子集，用这些随机子集分别对多个学习器进行训练（对于分类任务，采用简单投票法；对于回归任务采用简单平均法），从而得到最终预测结果。随机森林是Bagging最具代表性的应用，将Bagging的思想应用于决策树，并进行了一定的扩展。一般情况下，Bagging模型的精度要比Boosting低，但其各学习器可并行进行训练，节省大量时间开销。
#### Stacking
Stacking的思想是通过训练集训练好所有的基模型，然后用基模型的预测结果生成一个新的数据，作为组合器模型的输入，用以训练组合器模型，最终得到预测结果。组合器模型通常采用逻辑回归。
### XGBoost
虽然这些年神经网络（尤其是深度神经网络）变得越来越流行，但XGBoost仍旧在训练样本有限、训练时间短、调参知识缺乏的场景下具有独特的优势。相比深度神经网络，XGBoost能够更好地处理表格数据，并具有更强的可解释性，另外具有易于调参、输入数据不变性等优势。
XGBoost是Gradient Boosting的实现，相比其他实现方法，XGBoost做了很多优化，在模型训练速度和精度上都有明显提升，其优良特性如下。
1. 将正则项加入目标函数中，控制模型的复杂度，防止过拟合。
2. 对目标函数进行二阶泰勒展开，同时用到了一阶导数和二阶导数。
3. 实现了可并行的近似直方图算法。
4. 实现了缩减和列采样（借鉴了GBDT和随机森林）。
5. 实现了快速直方图算法，引入了基于loss-guide的树构建方法（借鉴了LightGBM）。
6. 实现了求解带权值的分位数近似算法（weighted quantile sketch）。
7. 可根据样本自动学习缺失值的分裂方向，进行缺失值处理。
8. 数据预先排序，并以块（block）的形式保存，有利于并行计算。
9. 采用缓存感知访问、外存块计算等方式提高数据访问和计算效率。
10. 基于Rabit实现分布式计算，并集成于主流大数据平台中。
11. 除CART作为基分类器外，XGBoost还支持线性分类器及LambdaMART排序模型等算法。
12. 实现了DART，引入Dropout技术。
# 机器学习算法基础
## 决策树
决策树是XGBoost模型的基本构成单元，因此通过本节的学习可以为深入理解XGBoost打下坚实的基础。1966年Hunt提出的CLS算法是最早的决策树算法。Quinlan在1986年提出的ID3、1993年提出的C4.5以及Breiman在1984年提出的CART，是迄今为止最具影响力的决策树算法。

决策树是一种树形结构，可用于解决分类问题和回归问题。每个叶子节点代表预测结果，从根节点到叶子节点的路径则代表判定规则。

假如有一批人的特征数据，特征包括头发长度、体重、身高。现在需要通过这些特征判定这些人的性别，此时便可以采用决策树。图3-9是根据数据设计的决策树，图中叶子节点的值表示节点上样本为男性的概率。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200415203759.png)

图3-9中，如果一个人的头发长度小于12cm并且体重大于55kg，那这个人有80%的概率是一名男性，有20%的概率是一名女性。可以看到，决策树具有天然的可解释性，可以非常直观地展示整个决策过程。此外，我们也可以很容易地将其转化为规则，直接通过代码实现。

### 构造决策树
决策树的训练目标，其实就是得到一种分类规则，使数据集中的所有样本都能被划分到正确的类别。这样的决策树可能有多个，也可能没有，所以我们的目标是找到一棵能将大部分样本正确分类并且具有较好泛化能力的树。决策树学习的损失函数一般是正则化的极大似然函数。从所有的决策树中选出最优的决策树是NP完全问题，一般采用启发式算法近似求解，因此生成决策树过程中的每一步都会采用当前最优的决策。

首先，为根节点选择一个最优特征对数据集进行划分，然后分别对其子节点进行最优划分，即每一步求局部最优解，直至该子集的所有样本都被正确地分类，则生成分类对应的叶子节点。如果子集中还存在没有被正确分类的样本，则继续划分，直至生成一棵完整的决策树。

以上述判定性别的问题为例，决策树生成过程如图3-10所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200415204055.png)

这样生成的决策树虽然可以对现有样本进行很好的分类，但是泛化能力不一定好，因此可能需要进行剪枝。另外，如果特征很多，则需要进行特征选择。

### 特征选择
特征选择是指选择出那些有分类能力的特征，作为决策树划分的特征。好的特征选择能够提升模型性能，帮助使用者理解数据样本的特点和结构，有利于进一步优化模型或算法。采用特征选择主要有如下益处：

- 简化模型，缩短模型训练时间；
- 避免维度灾难；
- 减少过拟合，提高模型泛化能力。

那怎么衡量特征是否有分类能力呢？通常在特征选择中会使用信息增益、信息增益比等。不同的决策树算法选择不同的方法作特征选择，常用的决策树算法有ID3、C4.5、CART等。

#### ID3
ID3算法由Ross Quinlan于1986年提出。该算法根据最大信息增益的原则对数据集进行划分。从根节点开始，遍历所有未被使用的特征，计算特征的信息增益，选取信息增益最大的特征作为节点分裂的特征，然后为该特征的每一个可能取值分别生成子节点，再对子节点递归调用上述方法，直到所有特征的信息增益均小于某阈值或没有特征可以选择。最终，对于每个叶子节点，将其中样本数量最多的类作为该节点的类标签。

##### 信息增益
信息是一个比较抽象的东西，如何度量其包含的信息量大小呢？1948年香农提出了信息熵的概念，解决了信息的度量问题。信息熵，简称熵（entropy），是信息的期望值，表示随机变量不确定性的度量，记作：

$Entropie (P)=-\sum_{i=0}^{n} p_{i} \times \log p_{i}$

其中，$p_{i}$为事件$i$发生的概率。熵只依赖于随机变量的分布，而不依赖其取值，熵越大，表示随机变量的不确定性越大。

假设样本集合为$D$,$|D|$表示集合中样本的数量，样本集合共有$k$个分类，每个分类的概率为$\frac{\left|C_{k}\right|}{|D|}$,其中$\left|C_{k}\right|$表示属于第$k$类的样本数量，则该样本集合的熵为：

$Entropie (D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log \frac{\left|C_{k}\right|}{|D|}$

在介绍信息增益之前，还需要介绍另外一个概念——条件熵（conditional entropy）。设有随机变量$(X, Y)$，条件熵$Entropie (Y | X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，条件熵$Entropie (Y | X)$定义为在给定条件$X$下，$Y$的条件概率分布的熵对$X$的数学期望：

$Entropie(Y | X)=\sum_{i=1}^{n} p_{i}$ Entropie $\left(Y | X=x_{i}\right)$

其中，$p_{i}=P\left(X=x_{i}\right), i=1,2, \ldots, n$

在决策树学习的过程中，信息增益是特征选择的一个重要指标。由前面信息熵的介绍可知，熵可以表示样本集合的不确定性（熵越大，样本的不确定性就越大），因此当通过某一特征对样本集合进行划分时，可通过划分前后熵的差值来衡量该特征对样本集合划分的好坏，差值越大表示不确定性降低越多，则信息增益越大。假设划分前样本集合的熵为$Entropie (D)$,按特征$A$划分后样本集合的熵为$Entropie(D|A)$，则信息增益为：

$info gain=Entropie (D) -Entropie (D|A)$

信息增益表示用特征$A$对样本集合进行划分时不确定性的减少程度。换句话说，按照特征$A$对样本进行分类，分类后数据的确定性是否比之前更高。通过计算信息增益，我们可以选择合适的特征作为决策树节点分裂的依据。下面通过例子进一步说明。

表3-1是根据天气情况判断是否进行户外活动的数据表。该数据表包含户外天气、温度、湿度和是否有风几个特征，类别标签为是否户外活动。各特征的具体信息如下。

- 户外天气：晴朗，阴天，下雨；
- 温度：凉爽，适中，热；
- 湿度：高，适中；
- 是否有风：是，否；

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200415232023.png)

下面计算每个特征的信息增益。为便于说明，用$D$表示该数据集，用$F_{1}$、$F_{2}$、$F_{3}$、$F_{4}$分别表示户外天气、温度、湿度、是否有风这4个特征。

首先计算数据集的熵$Entropie (D)$:

Entropie $(D)=-\frac{9}{14} \log _{2} \frac{9}{14}-\frac{5}{14} \log _{2} \frac{5}{14} \approx 0.940$

然后计算特征$F_{1}$对数据集$D$的条件熵，$F_{1}$将数据集分为3部分：晴天、雨天、阴天，分别对应数据子集$D_{1}$，$\mathrm{D}_{2}, \mathrm{D}_{3}$。

$\begin{aligned} \text { Entropie }\left(D | F_{1}\right)=&\frac{5}{14} \text { Entropie }\left(D_{1}\right)+\frac{5}{14} \text { Entropie }\left(D_{2}\right)+\frac{4}{14} \text { Entropie }\left(D_{3}\right) \\=& \frac{5}{14}\left(-\frac{2}{5} \log _{2} \frac{2}{5}-\frac{3}{5} \log _{2} \frac{3}{5}\right)+\frac{5}{14}\left(-\frac{3}{5} \log _{2} \frac{3}{5}-\frac{2}{5} \log _{2} \frac{2}{5}\right) \\ &+\frac{4}{14}\left(-\frac{4}{4} \log _{2} \frac{4}{4}\right) \\ \approx 0.694 \end{aligned}$

最后计算信息增益：

gain $\left(\mathrm{D}, \mathrm{F}_{1}\right)=$ Entropie $(\mathrm{D})-$ Entropie $\left(\mathrm{D} | \mathrm{F}_{1}\right)=0.940-0.694=0.246$

同理可以计算$F_{2}$、$F_{3}$、$F_{4}$的信息增益：

gain $\left(D, F_{2}\right)=0.940-0.911=0.029$
gain $\left(D, F_{3}\right)=0.940-0.788=0.152$
gain $\left(D, F_{4}\right)=0.940-0.892=0.048$

显然，特征$F_{1}$的信息增益最大，可以将其作为节点分裂的特征。

##### ID3算法过程
1. 从根节点开始分裂。
2. 节点分裂前，遍历所有未被使用的特征，计算特征的信息增益。
3. 选取信息增益最大的特征作为节点分裂的特征，并对该特征每个可能的取值生成一个子节点。
4. 对子节点循环执行步骤2、3，直至所有特征信息增益均小于某阈值或没有特征可以选择。

当一个特征可能的取值较多时，根据该特征更容易得到纯度更好的样本子集，信息增益会更大，因此ID3算法会偏向选择取值较多的特征，但不适用于极端情况下连续取值的特征选择。
#### C4.5
C4.5是ID3算法的扩展，其构建决策树的算法过程也与ID3算法相似，唯一的区别在于C4.5不再采用信息增益，而是采用信息增益比进行特征选择，解决了ID3算法不能处理连续取值特征的问题。

信息增益比，是在信息增益的基础上乘以一个调节参数，特征的取值个数越多，该参数越小，反之则越大。信息增益比定义如下：

$\operatorname{gain}_{R}(D, F)=\frac{\operatorname{gain}(D, F)}{\text { Entropie }_{F}(D)}$

调节参数为${ Entropie }_{F}(D)$的倒数，${ Entropie }_{F}(D)$表示数据集$D$以特征$F$作为随机变量的熵，其中$n$为特征$F$的取值个数。定义如下：

${ Entropie }_{F}(D)$=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log \frac{\left|D_{i}\right|}{|D|}$

#### CART
CART和ID3、C4.5算法类似，也是由特征选择、决策树构建和剪枝几部分组成。CART采用的是二分递归分裂的思想，因此生成的决策树均为二叉树。CART包含两种类型的决策树：分类树和回归树。分类树的预测值是离散的，通常会将叶子节点中多数样本的类别作为该节点的预测类别。回归树的预测值是连续的，通常会将叶子节点中多数样本的平均值作为该节点的预测值。分类树采用基尼系数进行特征选择，而回归树采用均方误差。CART是XGBoost树模型的重要组成部分，详细介绍参见第5章相关内容。

### 决策树剪枝
剪枝是决策树算法中必不可少的一部分。因为在决策树学习的过程中，为了尽可能正确地分类训练样本，算法会过多地进行节点分裂，导致生成的决策树非常详细且复杂。这样的模型对样本噪声非常敏感，容易产生过拟合，对新样本的预测能力也较差，因此需要通过决策树剪枝来提高模型的泛化能力。

决策树的剪枝策略分为预剪枝（pre-pruning）和后剪枝（post-pruning）两类。预剪枝是在决策树分裂过程中，在每个节点分裂前预先进行评估，若该节点分裂后并不能使决策树模型泛化能力有所提升，则该节点不分裂。后剪枝则是先构造一棵完全决策树，然后自底向上对非叶子节点进行评估，若将该非叶子节点剪枝有助于决策树模型泛化能力的提升，则将该节点子树剪去，使其变为叶子节点。

### 决策树解决肿瘤分类问题
本节介绍如何应用决策树解决实际问题，以预测良性/恶性乳腺肿瘤的二分类问题为例。scikit-learn实现了决策树算法，它采用的是一种优化的CART版本，既可以解决分类问题，也可以解决回归问题。分类问题使用DecisionTreeClassifier类，回归问题使用DecisionTreeRegressor类。两个类的参数相似，只有部分有所区别，以下是对主要参数的说明。

1. `criterion`：特征选择采用的标准。`DecisionTreeClassifier`分类树默认采用`gini`（基尼系数）进行特征选择，也可以使用`entropy`（信息增益）。`DecisionTreeRegressor`默认采用`MSE`（均方误差），也可以使用`MAE`（平均绝对误差）。
2. `splitter`：节点划分的策略。支持`best`和`random`两种方式，默认为`best`，即选取所有特征中最优的切分点作为节点的分裂点，`random`则随机选取部分切分点，从中选取局部最优的切分点作为节点的分裂点。
3. `max_depth`：树的最大深度，默认为`None`，表示没有最大深度限制。节点停止分裂的条件是：样本均属于相同类别或所有叶子节点包含的样本数量小于`min_samples_split`。若将该参数设置为`None`以外的其他值，则决策树生成过程中达到该阈值深度时，节点停止分裂。
4. `min_samples_split`：节点划分的最小样本数，默认为2。若节点包含的样本数小于该值，则该节点不再分裂。若该字段设置为浮点数，则表示最小样本百分比，划分的最小样本数为`ceil（min_samples_split*n_samples）`。
5. `min_samples_leaf`：叶子节点包含的最小样本数，默认为`1`。此字段和`min_samples_split`类似，取值可以是整型，也可以是浮点型。整型表示一个叶子节点包含的最小样本数，浮点型则表示百分比。叶子节点包含的最小样本数为`ceil（min_samples_leaf*n_samples）`。
6. `max_features`：划分节点时备选的最大特征数，默认为`None`，表示选用所有特征。若该字段为整数，表示选用的最大特征数；若为浮点数，则表示选用特征的最大百分比。最大特征数为`int（max_features*n_features）`。
7. `max_leaf_nodes`：最大叶子节点数，默认为`None`，表示不作限制。通过配置该字段，可以限制决策树的最大叶子节点数，防止模型过拟合。
8. `class_weight`：类别权重，默认为`None`，表示每个类别的权重均为1。该字段主要用于分类问题，防止训练集中某些类别的样本过多，导致决策树训练更偏向于这些类别。通过该字段可以指定每个类别的权重，也可以设置为`“balanced”`，使算法自动计算类别的权重，样本量少的类别权重会高一些。

以上是scikit-learn决策树中的常用参数，另外还有`random_state`、`min_impurity_decrease`、`min_impurity_split`等参数，其说明和使用方法可参阅相关资料。

加载任务数据集：

```python
from sklearn import datasets
cancer=datasets.load_breast_cancer()
X=cancer.data
y=cancer.target
```
划分训练集和测试集，比例为4:1。

```python
from sklearn.model_selection import  train_test_split
X_train,X_test,Y_trian,Y_test=train_test_split(X,y,test_size=1/5.,random_state=8)
```

因为是分类任务，所以调用scikit-learn中的DecisionTreeClassifier接口进行模型训练。通过训练好的模型对测试集进行预测，并评估预测效果，具体代码如下：
```python
from sklearn import tree
from sklearn.model_selection import  cross_val_score
from sklearn.metrics import classification_report

#决策树
clf=tree.DecisionTreeClassifier(max_depth=4)

#训练模型
clf.fit(X_train,y_trian)

#预测
y_pred=clf.predict(X_test)

#评估预测效果
print(classification_report(y_test,y_pred,target_names=['Benign','Malignant']))
```
评估结果输出如图3-11所示。
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416003736.png)

决策树可视化结果如图3-12所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416003825.png)

该决策树训练时指定了max_depth为4，读者可以根据实际情况自行调节该参数。图3-12直观地展示了各个节点的基尼系数、包含样本数、预测值、划分类别以及非叶子节点的分裂条件等信息。以其中一个非叶子节点为例，如图3-13所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416003930.png)

该节点的分裂条件为worst perimeter特征是否小于等于101.65，计算的基尼系数为0.0435，共包含270个样本，两个类别预测值分别为6和264，因为第2个类别预测值更高，所以该节点类别被判定属于benign。
# XGBoost小试牛刀
## XGBoost实现原理
XGBoost由多棵决策树（CART）组成，每棵决策树预测真实值与之前所有决策树预测值之和的残差（残差=真实值-预测值），将所有决策树的预测值累加起来即为最终结果。如图4-1所示，现有A、B、C、D这4个用户，通过决策树预测其是否使用支付宝（预测值越大，表明使用支付宝的可能性越大）
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416214259.png)
在图4-1中，非叶子节点为决策树采用的特征，分别为：年龄是否小于35岁、是否经常网上购物、是否经常使用手机上网。每个样本最终都会被分到一个叶子节点上，叶子节点的值表示被分到该节点的样本的预测值。根据残差的定义，样本所在的所有叶子节点的预测值之和为最终预测值，则A、B、C、D 4个用户使用支付宝的预测值计算如下：

$f(A)=1.6+2=3.6, f(B)=0.9+2=2.9$
$f(C)=0.5+0.2=0.7, f(D)=0.5+0.2=0.7$

预测结果为，A、B用户比C、D用户更倾向于使用支付宝。

## 二分类问题

XGBoost树模型由多棵回归树组成，并将多棵决策树的预测值累计相加作为最终结果。回归树产生的是连续的回归值，如何用它解决二分类问题呢？通过前面的学习知道，逻辑回归是在线性回归的基础上通过sigmoid函数将预测值映射到0～1的区间来代表二分类结果的概率。和逻辑回归一样，XGBoost也是采用sigmoid函数来解决二分类问题，即先通过回归树对样本进行预测，得到每棵树的预测结果，然后将其进行累加求和，最后通过sigmoid函数将其映射到0～1的区间代表二分类结果的概率。另外，对于二分类问题，XGBoost的目标函数采用的是类似逻辑回归的logloss，而非最小二乘。

XGBoost中关于二分类的常用参数有如下几个。

1. Objective
该参数用来指定目标函数，XGBoost可以根据该参数判断进行何种学习任务，binary:logistic和binary:logitraw都表示学习任务类型为二binary:logistic输出为概率，binary:logitraw输出为逻辑转换前的输出分数。
2. eval_metric
该参数用来指定模型的评估函数，和二分类相关的评估函数有：error、logloss和auc。error也称错误率，即预测错误的样本数占总样本数的比例，准确来说是预测错误样本的权重和占总样本权重和的比例，也可通过error@k的形式手工指定二分类的阈值。logloss通过惩罚分类来量化模型的准确性，最大限度减少log loss，等同于最大化模型的准确率。另外，AUC也是二分类中最常用的评估指标之一.

蘑菇数据集是一个非常著名的二分类数据集。该数据集一共包含23个特征，包括大小、表面、颜色等，每一种蘑菇都会被定义为可食用的或者有毒的，需要通过样本数据分析这些特征与蘑菇毒性的关系。以下是各个特征的详细说明。

1. 帽形（cap-shape）：钟形=b，圆锥形=c，凸形=x，平面=f，把手形=k，凹陷=S
2. 帽面（cap-surface）：纤维状=f，凹槽状=g，鳞片状=y，光滑=s。
3. 帽颜色（cap-color）：棕色=n，浅黄色=b，肉桂色=c，灰色=g，绿色=r，粉红色=p，紫色=u，红色=e，白色=w，黄色=y。
4. 创伤（bruises）：创伤=t，no=f。
5. 气味（odor）：杏仁=a，茴香=l，石灰=c，腥味=y，臭味=f，霉味=m，无=n，刺鼻=p，辣=s。
6. 菌褶附属物（gill-attachment:）：附着=a，下降=d，自由=f，缺口=n。
7. 菌褶间距（gill-spacing）：紧密=c，拥挤=w，远隔=d。
8. 菌褶大小（gill-size）：宽=b，窄=n。
9. 菌褶颜色（gill-color）：黑色=k，棕色=n，浅黄色=b，巧克力色=h，灰色=g，绿色=r，橙色=o，粉红色=p，紫色=u，红色=e，白色=w，黄色=y。
10. 茎形（stalk-shape）：扩大=e，锥形=t。
11. 茎根（stalk-root）：球根=b，棒状=c，杯状=u，均等的=e，根状菌索=z，扎根=r，缺省=？。
12. 环上茎面（stalk-surface-above-ring）：纤维状=f，鳞片状=y，丝状=k，光滑=s。
13. 环下茎面（stalk-surface-below-ring）：纤维状=f，鳞片状=y，丝状=k，光滑=s。
14. 环上茎颜色（stalk-color-above-ring）：棕色=n，浅黄色=b，黄棕色=c，灰色=g，橙色=o，粉红色=p，红色=e，白色=w，黄色=y。
15. 环下茎颜色（stalk-color-below-ring）：棕色=n，浅黄色=b，黄棕色=c，灰色=g，橙色=o，粉红色=p，红色=e，白色=w，黄色=y。
16. 菌幕类型（veil-type）：部分=p，普遍=u。
17. 菌幕颜色（veil-color）：棕色=n，橙色=o，白色=w，黄色=y。
18. 环数量（ring-number）：没有=n，一个=o，两个=t。
19. 环类型（ring-type）：蛛网状=c，消散状=e，喇叭形=f，大规模的=l，无=n，悬垂的=p，覆盖=s，环带=z。
20. 孢子显现颜色（spore-print-color）：黑色=k，棕色=n，蓝色=b，巧克力色=h，绿色=r，橙色=o，紫色=u，白色=w，黄色=y。
21. 种群（population）：丰富=a，聚集=c，众多=n，分散=s，个别=v，单独=y。
22. 栖息地（habitat）：草地=g，树叶=l，草甸=m，路上=p，城市=u，荒地=w，树林=d。23）class：label字段，有可食用（edible）和有毒性（poisonous）两个取值。

该数据集总共有8124个样本，其中类别为可食用的样本有4208个，类别为有毒性的样本有3916个。

首先需要对特征进行预处理。因为原始文件agaricus-lepiota.data中的数据并不能直接作为XGBoost的输入进行加载，原始数据格式如下：

$p, x, s, n, t, p, f, c, n, k, e, e, s, s, w, w, p, w, 0, p, k, s, u$

$e_{1} x, s, y, t, a, f, c, b, k, e, c, s, s, w, w, p, w, 0, p, n, n, g$

XGBoost不能直接处理上述数据，需要进行预处理。这里将其中的字符数据转为数值型，并以LibSVM的格式输出。LibSVM是机器学习中经常采用的一种数据格式，如下：

<label> <index1>:<value1> <index2>:<value2>...

label为训练数据集的目标值；index为特征索引，是一个以1为起始的整数；value是该特征的取值，如果某一特征的值缺省，则该特征可以空着不填，因此对于一个样本来讲，输出后的数据文件index可能并不连续，上述样本处理后的格式如下：

1 3:1 10:1 11:1 21:1 30:1 34:1 36:140: 141:1 53:1 58:1 65:1 69:1 77:1 86:1 88:1 92:1 95:1 102:1 105:1 117:1 124:1

0 3:1 10:1 20:1 21:1 23:1 34:1 36:1 39:1 41:1 53:1 56:1 65:1 69:1 77:1 86:1 88:1 92:1 95:1 102:1 106:1 116:1 120:1

第一个样本中最开始的“1”便是该样本的label，在二分类问题中，一般1代表正样本，0代表负样本。之后的每个特征为一项，冒号前为该特征的索引，如3、10等，冒号后为该特征取值，如3、10两个特征的取值都是1。另外，观察处理后的数据可以发现，特征索引已经远远超过了22，如第一行样本中特征索引最大已经达到了124。

观察该数据集可以发现，其中大部分特征是离散型特征，连续型特征较少。在机器学习算法中，特征之间距离的计算是十分重要的，因此，直接把离散变量的取值转化为数值，并不能很好地代表特征间的距离，如菌幕颜色特征，其总共有棕色、橙色、白色、黄色4种颜色，假如将其映射为1、2、3、4，则棕色和橙色之间的距离是2-1=1，而棕色和白色之间的距离是3-1=2。这显然是不符合实际情况的，因为任意两个颜色之间的距离应该是相等的。因此，需要对特征进行独热编码（one-hot encoding）。简单来讲，独热编码就是离散特征有多少取值，就用多少维来表示该特征。仍然以菌幕颜色特征为例，经过独热编码后，其将会转为4个特征，分别是菌幕颜色是否为棕色、菌幕颜色是否为橙色、菌幕颜色是否为白色和菌幕颜色是否为黄色，并且这4个特征取值只有0和1。经过独热编码之后，每两个颜色之间的距离都是一样的，比之前的处理更合理。离散特征经过独热编码之后，数据集的总特征数会变多，这就是上述示例中出现较大特征索引的原因。

先加载训练集和测试集：

```python
import xgboost as xgb

xgb_train=xgb.DMatrix('./agaricus.txt.train')
xgb_test=xgb.DMatrix('./agaricus.txt.test')

params={
    "objective":"binary:logistic",
    "booster":"gbtree",
    "eta":1.0,
    "gamma":1.0,
    "min_child_weight":1,
    "max_depth":3
}
num_round=2
watchlist=[(xgb_train,'train'),(xgb_test,'test')]

model=xgb.train(params,xgb_train,num_round,watchlist)
```
params中的objective和booster参数已经介绍过了，分别用于指定任务的学习目标和booster类型。objective设为binary:logistic，表示任务为二分类问题，最终输出为sigmoid变换后的概率。booster为gbtree表示采用XGBoost中的树模型。参数eta表示学习率，类似于梯度下降中法的α，每次迭代完更新权重的步长。参数gamma表示节点分裂时损失函数减小的最小值，此处为1.0，表示损失函数至少下降1.0该节点才会进行分裂。参数min_child_weight表示叶子节点最小样本权重和，若节点分裂导致叶子节点的样本权重和小于该值，则节点不进行分裂。参数max_depth表示决策树分裂的最大深度。另外，该示例中指定了num_round为2，即模型会进行两轮booster训练，最终会生成两棵决策树。通过定义参数watchlist，模型在训练过程中会实时输出训练集和验证集的评估指标。模型训练过程的输出结果如图4-2所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416223620.png)

模型训练完成之后，可通过save_model方法将模型保存成模型文件，以供后续预测使用，如下：

```python
model.save_model("/0002. Model")
```

预测时，先加载保存的模型文件，然后再对数据集进行预测，如下：

```python
bst=xgb.Booster()
bst.load_model("./0002.model")
pred=bst.predict(xgb_test)
```
下面来看一下文本格式的XGBoost树模型文件，如图4-5所示。
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416230940.png)

在图4-5中，一个booster代表一棵决策树，该模型一共有两棵决策树。在每棵决策树中，每一行代表一个节点，位于行首的数字代表该节点的索引，数字0表示该节点为根节点。若该行节点是非叶子节点，则索引后面是该节点的分裂条件，如图4-5中第2行：

0:[odor=pungent] yes=2, no=1

该节点的索引为0，表示该节点是根节点，其分裂条件是odor=pungent，满足该条件的样本会被划分到节点2，不满足的则被划分到节点1。若该行节点是叶子节点，则索引后面是该叶子节点最终得到的权重。如图4-5中的第5行：

7:1eaf=1.90175

leaf表示该节点为叶子节点，最终得到的权重为1.90175。

## 多分类问题
与处理二分类问题类似，XGBoost在处理多分类问题时也是在树模型的基础上进行转换，不过不再是sigmoid函数，而是softmax函数。相信读者对softmax变换并不陌生，第3章已有所介绍，它可以将多分类的预测值映射到0到1之间，代表样本属于该类别的概率。

XGBoost中解决多分类问题的主要参数如下。

1. `num_class`：说明在该分类任务的类别数量。
2. `objective`：该参数中的`multi:softmax`和`multi:softprob`均是指定学习任务为多分类。`multi:softmax`通过`softmax`函数解决多分类问题。`multi:softprob`和`multi:softmax`一样，主要区别在于其输出的是一个`ndata*nclass`向量，表示样本属于每个分类的预测概率。
3. `eval_metric`：与多分类相关的评估函数有`merror`和`mlogloss`。`merror`也称多分类错误率，通过判断样本所有分类预测值中预测值最大的分类和样本`label`是否一致来确定预测是否正确，其计算方式和`error`相似。`mlogloss`也是多分类问题中常用的评估指标。有关`merror`和`mlogloss`会在第7章中详细介绍。

下面以识别小麦种子的类别作为示例，介绍如何通过XGBoost解决多分类问题。已知小麦种子数据集包含7个特征，分别为面积、周长、紧凑度、籽粒长度、籽粒宽度、不对称系数、籽粒腹沟长度，且均为连续型特征，以及小麦类别字段，共有3个类别，分别用1、2、3表示。
加载该数据并进行特征处理，代码如下：
```python
import pandas as pd
import xgboost as xgb
import numpy as np
#使label取值在num_class-1范围内
data=pd.read_csv('./seeds_datasets.txt',header=None,sep='\s+',converters={7:lambda x:int(x)-1})
#将最后一列字段名设置为label
data.rename(columns={7:'label'},inplace=True)
```
为便于后续处理，将最后一个类别字段作为label字段，因为label的取值需在0到num_class-1范围内，因此需对类别字段进行处理（数据集中的3个类别取值分别为1～3），这里直接减1即可。完成数据加载后，下面来看一下数据集的数据结构，输出前10行数据，代码如下：
```python
data.head(10)
```
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416232532.png)

可以看到，数据集共包含8列，其中前7列为特征列，最后1列为label列，和数据集描述相符。除label列外，剩余特征没有指定列名，所以pandas自动以数字索引作为列名。

下面对数据集进行划分（训练集和测试集的划分比例为4:1），并指定label字段生成XGBoost中的DMatrix数据结构，代码如下：

```python
#生成一个随机数并选择小于 0.8 的数据
mask=np.random.randn(len(data))<0.8
train=data[mask]
test=data[~mask]

#生成DMatrix
xgb_train=xgb.DMatrix(train.loc[:,:6],label=train.label)
xgb_test=xgb.DMatrix(test[:,:6],label=test.label)
```
置参数objective为multi:softmax，表示采用softmax进行多分类，学习率参数eta和最大树深度max_depth在之前的示例中已有所介绍，不再赘述。参数num_class指定类别数量为3。相关代码如下：

```python
#通过softmax进行分类
params={
    'objective':'multi:softmax',
    'eta':0.1,
    'max_depth':5,
    'num_class':3
}
watchlist=[(xgb_train,'train'),(xgb_test,'test')]
num_round=50
bst=xgb.train(params,xgb_train,num_round,watchlist)
```
总共训练50轮，训练过程的部分输出结果如图4-7所示。
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416234803.png)
在未指定评估函数的情况下，XGBoost默认采用`merror`作为多分类问题的评估指标。下面通过训练好的模型对测试集进行预测，并计算错误率，代码如下：
```python
#模型预测
pred=bst.predict(xgb_test)
error_rate=np.sum(pred!=test.label)/test.shape[0]
```
通过模型得到预测值pred，然后对比预测值和实际的label值，计算错误率，输出如下：测试集错误率（softmax）：0.0408163265306

为了方便对比学习，下面采用`multi:softprob`方法重新训练模型，代码如下：
```python
#重新训练模型，输出概率值
params['objective']='multi:softprob'
bst=xgb.train(params,xgb_train,num_rounds,watchlist)
```
模型训练过程输出结果如图4-8所示。对比两种函数变换方法的训练输出结果可以看出，不论采用multi:softmax还是multi:softprob作为objective训练模型，并不会影响到模型精度。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200416235444.png)

下面对测试集进行预测并计算错误率，代码如下：
```python
pred_prob=bst.predict(xgb_test)
pred_label=np.argmax(pred_prob,axis=1)
error_rate=np.sum(pred_prob!=test.label)/test.shape[0]
```
此时模型预测输出的pred_prob是样本属于各类别的概率向量，如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417000639.png)

下一步是比较预测值与实际的label值来判断是否预测正确。和`multi:softmax`不同，此时需要借助`numpy`中的`argmax`函数将概率向量转化为样本预测值。`argmax`函数可以获取每个样本的概率向量中最大值的索引，即为样本的预测类别，最终返回所有样本所属类别索引的向量，如下：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417001731.png)

之后的处理则和采用multi:softmax时一样，统计预测错误的样本数，最终计算出分类错误率。采用multi:softprob得到的错误率和multi:softmax也是一样的：测试集错误率（softprob）：0.0408163265306

## 回归问题
用XGBoost解决回归问题是很顺理成章的事情，因为XGBoost本身采用的就是回归树，将每棵回归树对样本的预测值相加即为最终预测值。XGBoost支持多种回归模型，包括线性回归、泊松回归、伽马（gamma）回归等，不同的回归模型有不同的目标函数。以下是回归问题中用到的主要参数。

### `objective`
1. `reg:linear`：线性回归，并非指线性模型（线性模型由`booster`参数指定），用于数据符合正态分布的回归问题，目标函数为最小二乘。
2. `reg:logistic`：逻辑回归，目标函数为`logloss`。
3. `count:poisson`：计数数据的泊松回归。
4. `reg:gamma`：对数连接函数下的伽马回归。
5. `reg:tweedie`：对数连接函数下的`tweedie`回归。
### `eval_metric`
回归问题的评估指标主要有RMSE、MAE，另外还有一些如poisson-nloglik、gamma-nloglik、gamma-deviance、tweedie-nloglik等用于特定回归的评估指标。其中RMSE（Root Mean Square Error，均方根误差），是回归模型中最常采用的评估指标之一，是预测值与真实值偏差的平方和与样本数比值的平方根。指标MAE（Mean Absolute Error，平均绝对误差），是回归模型中常用的评估指标，衡量的是预测值和真实值之间绝对差异的平均值。指标RMSE和MAE会在第7章详细介绍。

下面通过评估混凝土坍度的示例，介绍如何通过XGBoost解决一般的回归问题。混凝土坍度测试数据集是一个通过混凝土的各种指标特征评估其抗压强度的数据集，共包含1030个样本、9个特征，特征的具体信息如下。
1. 水泥：数据类型为浮点型，单位为立方米每千克。
2. 高炉渣：数据类型为浮点型，单位为立方米每千克。
3. 煤灰：数据类型为浮点型，单位为立方米每千克。
4. 水：数据类型为浮点型，单位为立方米每千克。
5. 高效减水剂：数据类型为浮点型，单位为立方米每千克。
6. 粗骨料：数据类型为浮点型，单位为立方米每千克。
7. 细骨料：数据类型为浮点型，单位为立方米每千克。
8. 年龄：数据类型为整型，单位为天。
9. 混凝土抗压强度定量：数据类型为浮点型，单位为MPa。

其中混凝土抗压强度定量为目标特征。可以看到，所有特征均为数值型特征且不存在缺省值，此类特征不用进行处理。

对数据进行加载和特征处理，这里采用pandas中的read_excel读取该数据集，处理代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417004734.png)

通过`head`函数输出前10行数据，查看数据集的数据结构:
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417004815.png)

为便于后续处理，将混凝土抗压强度一列改名为label，代码如下：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417004857.png)

数据集划分。按4:1的比例将数据集划分为训练集和测试集，并指定label字段生成XGBoost中的DMatrix数据结构，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417005241.png)

得到训练集和测试集后，即可加载数据并进行模型训练。本示例的模型训练代码和二分类示例十分相似，唯一不同的是参数objective设置为回归参数reg:linear，代码如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417005320.png)

训练过程的部分输出结果如图4-10所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417005339.png)

reg:linear默认采用的评估指标是RMSE，用户也可以通过设置eval_metric参数采用其他评估指标。预测时首先加载训练好的模型，然后对测试集进行预测，如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417005423.png)

输出预测结果如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417005445.png)

可以看到，回归模型的预测结果不再是离散的分类值，或0～1的概率值，而是连续的数值型数据。

## 排序问题
XGBoost也可应用于机器学习中的排序问题。XGBoost的排序学习采用一种将Lambda-Rank和MART（MultipleAdditive Regression Trees）结合的排序算法，即LambdaMART算法。其中MART模型的输出为一组回归树输出的线性组合，而LambdaRank则提供了一种梯度定义方法，针对不同问题可定义不同的梯度。LambdaMART算法将会在第5章详细介绍。XGBoost中排序问题的相关参数如下。

1. `objective`：该参数用来指定目标函数，`rank:pairwise`、`rank:ndcg`和`rank:map`均表示排序任务。`rank:pairwise`通过最小化`pairwise`损失完成排序任务，`rank:ndcg`是以最大化`NDCG`为目标实现`list-wise`排序，而`rank:map`则以最大化`MAP`为目标实现`list-wise`排序。
2. `eval_metric`：该参数用来设置模型的评估指标，排序问题的评估指标有`map`、`ndcg`、`auc`.
   
### 排序学习算法

大数据时代的今天，从海量数据中快速找到需要的信息变得越来越困难，搜索引擎早已成为人们日常生活中必不可少的工具。搜索引擎中最为重要的一项技术便是排序（rank），利用机器学习优化搜索引擎的排序结果，很自然地成为业界的研究热点。

先来看一个经典的排序场景——文档检索。给定一个查询，将返回相关的文档，然后通过排序模型f（q,d）根据查询和文档之间的相关度对文档进行排序，再返回给用户，如图3-16所示，其中q代表查询（query），d代表一个文档。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417185526.png)

通过机器学习解决排序问题，主要任务就是构建一个排序模型f（q,d），对查询下的文档进行排序，这个过程即为排序学习（learning to rank）。图3-17所示为排序学习的基本框架：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417185648.png)

在学习系统中，先通过训练集对模型进行训练。训练集包括文档和查询相关的特征，每个查询与多个文档相关联，文档与查询之间的相关度用label表示，label得分越高表明相关度越大。每个查询-文档对的相关特征（如余弦相似度等）构成样本的特征向量$x_{i}$,i，对应的相关度得分作为该样本的label，用$y_{i}$，对应的相关度得分作为该样本的label，用$y_{i}$表示，这样就构成了一个具体的训练实例$\left(x_{i}, y_{i}\right)$,i,yi）。所有的训练实例构成训练集，输入到学习系统中训练模型，得到排序模型$f(q, d)$。所有的训练实例构成训练集，输入到学习系统中训练模型，得到排序模型f（q,d）。对于给定的查询-文档对的特征向量，使用排序模型预测其相关度得分。

从目前的研究方法来看，排序学习算法主要分为以下3种：Pointwise方法、Pairwise方法和Listwise方法。

#### Pointwise方法

Pointwise方法是把排序问题转化为机器学习中常规的分类、回归问题，从而可以直接应用现有的分类、回归方法解决排序问题。Pointwise的处理对象是单一文档。将单一查询文档对转化为特征向量，相关度作为Label，构成训练样本，然后采用分类或回归方法进行训练。得到训练模型后，再通过模型对新的查询和文档进行预测，得到相关度得分，最终将该得分作为文档排序的依据。下面通过例子来说明Pointwise方法。表3-2提供了4个训练样本，每个样本有2个特征：文档的余弦相似度以及页面的PageRank值。Label分为3个等级，即不相关、相关和非常相关。Pointwise方法通过多分类算法训练该数据集，然后通过训练后得到的模型对新的查询和文档进行预测。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417190242.png)

Pointwise方法存在一定的局限性，它仅仅考虑单个文档的绝对相关度，没有考虑给定查询下的文档集合的排序关系。此外，排在前面的文档相比于排在后面的文档对排序的影响更为重要，因为在很多情况下人们只关注top k的文档，而Pointwise方法并没有考虑这方面的影响。

#### Pairwise方法
如前所述，Pointwise方法只考虑单个文档与查询的绝对相关度，没有考虑给定查询下的文档集合的排序关系。Pairwise则将重点转向了文档之间的排序关系，它将排序问题转化为文档对$<d_{i}, d_{j}>$排序关系的分类和回归问题。Pairwise方法有许多实现，如Ranking SVM、RankNet、Lambda Rank以及LambdaMART等。

对于给定查询下的文档集合，其中任何两个相关度不同的文档都可以组成一个训练实例$<d_{i}, d_{j}>$。若$d_{i}$比$d_{j}$更相关，则该实例的label为1，否则为-1，这样就得到一个二分类的训练集。使用该训练集进行训练，得到模型后，可以预测所有文档对的排序关系，进而实现对所有文档进行排序。

Pairwise方法虽然考虑了文档之间的相对排序关系，但仍然没有考虑文档出现在结果列表中的位置。排在前面的文档更为重要，如果前面的文档出现错误，远比排在后面的文档出现错误影响更大。另外，对于不同的查询，相关文档数量有时差别会很大，转化为文档对后，有的查询可能有几百个文档对，而有的只有几个。这会使模型评估变得非常困难，如查询1对应100个文档对，查询2对应10个文档对。如果模型对查询1可以正确预测80个文档对，对查询2可正确预测3个，则总的文档对的预测准确率为[（80+3）/（100+10）]×100%≈75%，而对于两个查询的准确率分别为80%和30%，平均准确率为55%，与总文档对的预测准确率差别很大，即模型更偏向于相关文档集更大的查询。

#### Listwise方法
Pointwise方法和Pairwise方法分别以一个文档和文档对作为训练实例，Listwise方法则采用更直接的方式，即以整个文档列表作为一个训练实例。Listwise方法包括ListNet、AdaRank等。

Listwise根据训练集样本训练得到一个最优模型f，对新查询通过模型对每个文档进行打分，然后将得分由高到低排序，得到最终排序结果，其中的关键问题是如何优化训练得到最优模型。其中一种方法是针对排序指标进行，如MAP、NDCG作为最优评分函数，但因为很多类似NDCG这样的评分函数具有非连续性，因此比较难优化。另外一种方法是优化损失函数，如以正确排序与预测排序的分值向量间余弦相似度作为损失函数等。

### 排序评价指标
#### 精确率、召回率和F-Score
#### AUC
AUC（Area Under the Curve，曲线下面积）是二分类问题中较常用的评估指标之一，此处的曲线即ROC（Receiver Operating Characteristic）曲线。ROC曲线描述的是模型的TPR（True Positive Rate）和FPR（False Postive Rate）之间的变化关系，其中TPR为模型分类正确的正样本个数占总正样本个数的比例，FPR为模型分类错误的负样本个数占总负样本个数的比例。

ROC曲线的横轴为FPR，纵轴为TPR。对于二分类问题，可以为每个样本预测一个正样本概率，通过与设定阈值的比较，决定其属于正样本还是负样本。例如，假设设定阈值为0.7，概率大于0.7的为正样本，小于0.7的为负样本，即可求得一组（FPR,TPR）的值作为坐标点，然后逐渐减小阈值，则更多的样本会划分为正样本，但也会导致一些负样本被错误划分为正样本，即FPR和TPR会同时增大，最大为坐标（1,1）。相反，阈值逐渐增大时，FPR和TPR会同时减小，最小为（0,0）。由此，便可得到ROC曲线。

ROC曲线有4个比较特殊的点：

1. （0,0）——FPR和TPR均为0，表示模型将样本全部预测为负样本；
2. （0,1）——FPR为0，TPR为1，表示模型将样本全部预测正确；
3. （1,0）——FPR为1，TPR为0，表示模型将样本全部预测错误；
4. （1,1）——FPR和TPR均为1，表示模型将样本全部预测为正样本。

当FPR和TPR相等时（即斜对角线），表示预测的正样本一半是对的，一半是错的，即可代表随机分类的结果。由此可知，若ROC曲线在斜对角线以下，表示模型分类的效果比随机分类还要差，反之，则表明模型分类效果优于随机分类。虽然ROC曲线可以在一定程度上反映分类的效果，但需要比较多个模型的分类效果时，ROC曲线则不够直观。因此，AUC便应运而生。AUC即ROC曲线下的面积，它是一种定量的指标，取值范围为[0,1]，AUC越大，表明模型分类效果越好。当AUC<0.5时，表明模型分类效果差于随机分类；当0.5<AUC<1时，表明模型分类效果优于随机分类；当AUC=1时，表明模型完全分类正确。AUC在XGBoost中的参数为auc，可用于二分类问题和排序问题。

#### MAP
MAP（Mean Average Precision）是信息检索中的一个评价指标。MAP假定相关度有两个级别——相关与不相关。在学习MAP之前，我们先来了解一下AP（Average Precision），其计算方法如下：

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417191108.png)

其中，k为文档在排序列表中的位置，P（k）为前k个结果的准确率，即

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417191213.png)

rel（k）表示位置k的文档是否相关，相关为1，不相关为0。

MAP为一组查询的AP的平均值，公式如下：

$\mathrm{MAP}=\frac{\sum_{q=1}^{Q} \mathrm{AP}(q)}{Q}$

其中，q为查询，AP（q）为查询的平均准确率，Q为查询个数。

#### NDCG
前面介绍了准确率、MAP等信息检索评价指标，下面介绍另外一种评价指标——NDCG（Normalized DiscountedCumulative Gain）。在MAP中，相关度只有相关、不相关两个级别。NDCG则可以定义多级相关度，相关度级别更高的文档排序更靠前。在了解NDCG之前，先介绍一下DCG（Discounted Cumulative Gain），即折扣累计增益。DCG认为应对出现在排序列表中靠后的文档进行惩罚，因此文档相关度与其所在位置的对数成反比。只考虑前P个文档，DCG定义为：

$\mathrm{DCG}_{p}=\sum_{i=1}^{p} \frac{\mathrm{rel}_{i}}{\log _{2}(i+1)}$

其中，$\mathrm{rel}_{\mathrm{i}}$i是位置i上的文档相关度得分，$\frac{1}{\log _{2}(i+1)}$为折算因子。DCG还有另外一种定义，也经常被使用：

$\mathrm{DCG}_{p}=\sum_{i=1}^{p} \frac{2^{\mathrm{rel}_{i}}-1}{\log _{2}(i+1)}$

该定义更强调检索相关度高的文档，被广泛应用于网络搜索公司和Kaggle等机器学习竞赛中。

因为不同的搜索结果列表长度很可能有所不同，因此不能用DCG对不同搜索结果进行对比，需要对DCG值进行归一化，即需要用到下面要介绍的NDCG。首先计算位置p最大可能的DCG，即理想情况下的DCG（IDCG）：

$\operatorname{IDCG}_{p}=\sum_{i=1}^{p} \frac{2^{\mathrm{rel}_{i}}-1}{\log _{2}(i+1)}$

则NDCG为：

$\mathrm{NDCG}_{p}=\frac{\mathrm{DCG}_{p}}{\mathrm{IDCG}_{p}}$

NDCG会对文档排名较高的给予高分。对于理想情况下的排名，每个位置的NDCG值总为1；对于非理想的排名，NDCG值小于1。

下面通过例子来介绍NDCG的计算过程，假设查询q的结果列表包含5个文档，分别为$\mathrm{D}_{1} \sim \mathrm{D}_{5}$(下标表示当前排序位置),相关度级别取值为1、2、3，分别代表不相关、相关、非常相关,$\mathrm{D}_{1} \sim \mathrm{D}_{5}$的相关度分别为3、1、3、2、2。对每个文档计算$\log _{2}(i+1)$和$\frac{\operatorname{rel}_{i}}{\log _{2}(i+1)}$，如表3-3所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417193218.png)

由此可计算$\mathrm{DCG}_{5}$为：

$\mathrm{DCG}_{5}=\sum_{i=1}^{5} \frac{\mathrm{rel}_{i}}{\log _{2}(i+1)}=3+0.631+1.5+0.861+0.774=6.766$

由表3-3可知，交换排名第一位和第二位的文档会导致DCG减小，因为不相关的文档排到了更高的位置，而非常相关文档却排到了更低的位置，因此不相关文档对应较大的折算因子，而相关文档对应较小的折算因子，导致DCG减小。假设除了查询结果列表的5个文档之外，还有两个文档$D_{6}, D_{7}$6、D7未进入结果列表，其相关度分别为3和1。对这7个文档按相关度进行排序，有：

3，3，3，2，2，1，1

计算到位置5的理想DCG（IDCG）为：

$\mathrm{IDCG}_{5}=8.025$

可求得$\mathrm{NDCG}_{5}$为:

$\mathrm{NDCG}_{5}=\frac{\mathrm{DCG}_{5}}{\mathrm{IDCG}_{5}}=\frac{6.766}{8.025} \approx 0.843$

## 其他常用功能
本节以诊断乳腺肿瘤的二分类问题为例，介绍XGBoost中其他的常用功能，以下是数据加载和模型训练的代码：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417194042.png)
### 将DMatrix保存为二进制文件
XGBoost可以将DMatrix数据结构保存为可直接加载的二进制文件。该操作的好处是将完成特征处理并且已转换为DMatrix格式的数据进行保存，下次加载时避免重复操作从而节省加载时间。将测试集保存为二进制文件，然后重新加载，最后的预测结果与之前是一致的。相关代码如下：
![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200417194129.png)
### 

















