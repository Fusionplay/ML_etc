<!-- TOC -->

- [线性模型(《机器学习》)](#线性模型机器学习)
  - [线性回归](#线性回归)
  - [对数几率回归](#对数几率回归)
- [逻辑斯谛回归与最大熵模型](#逻辑斯谛回归与最大熵模型)
  - [逻辑斯谛回归模型](#逻辑斯谛回归模型)
    - [逻辑斯谛分布](#逻辑斯谛分布)
    - [二项逻辑斯谛回归模型](#二项逻辑斯谛回归模型)
    - [模型参数估计](#模型参数估计)
    - [多项逻辑斯谛回归](#多项逻辑斯谛回归)

<!-- /TOC -->
[在GitHub Page上阅读](https://bailingnan.github.io/post/lesslesstong-ji-xue-xi-fang-fa-greatergreater-lesslessji-qi-xue-xi-greatergreater-xian-xing-hui-gui-luo-ji-hui-gui/)
# 线性模型(《机器学习》)
给定由$d$个属性描述的示例$\boldsymbol{x}=\left(x_{1} ; x_{2} ; \ldots ; x_{d}\right)$,其中$x_{i}$是$x$在第$i$个属性上的取值，线性模型（inear modell）试图学得一个通过属性的线性组合来进行预测的函数，即:

$f(\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{d} x_{d}+b$

一般用向量形式写成:

$f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$

其中$\boldsymbol{w}=\left(w_{1} ; w_{2} ; \ldots ; w_{d}\right)$。$\boldsymbol{w}$和$b$学得之后，模型就得以确定。

线性模型形式简单、易于建模，但却蕴涵着机器学习中一些重要的基本思想许多功能更为强大的非线性模型（monlinear model）可在线性模型的基础上通过引入层级结构或高维映射而得。此外，由于$\boldsymbol{w}$直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性（comprehensibility）。
## 线性回归
给定数据集$D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$，其中$\boldsymbol{x}_{i}=\left(x_{i 1}\right.$$\left.x_{i 2} ; \ldots ; x_{i d}\right)$，$y_{i} \in \mathbb{R}$。“线性回归”（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

我们先考虑一种最简单的情形：输入属性的数目只有一个。为便于讨论，此时我们忽略关于属性的下标，即$D=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{m}$，其中$x_{i} \in \mathbb{R}$。对离散属性若属性值间存在“序”（order）关系，可通过连续化将其转化为连续值，例如二值属性“身高”的取值“高”“矮”可转化为{1.0,0.0}，三值属性“高度”的取值“高”“中”“低”可转化为{1.0,0.5,0.0}；若属性值间不存在序关系，假定有$k$个属性值，则通常转化为$k$维向量，例如属性“瓜类”的取值“西瓜”“南瓜”“黄瓜”可转化为(0,0,1), (0,1,0), (1,0,0)。

若将无序属性连续化，则会不恰当地引入序关系，对后续处理如距离计算等造成误导。

线性回归试图学得

$f\left(x_{i}\right)=w x_{i}+b$，使得$f\left(x_{i}\right) \simeq y_{i}$。

如何确定$w$和$b$呢？显然，关键在于如何衡量$f(x)$与$y$之间的差别。均方误差是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即

$\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\ &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2} \end{aligned}$

>$w^{*}$，$b^{*}$表示$w$和$b$的解

均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧氏距离”（Euclidean distance）。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”（least square method，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

求解$w$和$b$使$E_{(w, b)}=\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$最小化的过程，称为线性回归模型的最小二乘“参数估计”（parameter estimation）。我们可将$E_{(w, b)}$分别对$w$和$b$求导，得到

$\frac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)$

$\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)$

>这里$E_{(w, b)}$是关于$w$和$b$的凸函数，当它关于$w$和$b$的导数均为零时，得到$w$和$b$的最优解。

>对区间$[a, b]$上定义的函数$f$，若它对区间中任意两点$x_{1}, x_{2}$均有$f\left(\frac{x_{1}+x_{2}}{2}\right) \leqslant \frac{f\left(x_{1}\right)+f\left(x_{2}\right)}{2}$则称$f$为区间$[a, b]$上的凸函数。U 形曲线的函数如$f(x)=x^{2}$, 通常是凸函数

>对实数集上的函数，可通过求二阶导数来判别若二阶导数在区间上非负则称为凸函数；若二阶导数在区间上恒大于 0, 则称为严格凸函数


然后令上式为零可得到$w$和$b$最优解的闭式（Closed-form）解：

$w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}$

$b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)$

其中$\bar{x}=\frac{1}{m} \sum_{i=1}^{m} x_{i}$ 为 $x$为$x$的均值。

更一般的情形是如本节开头的数据集$D$，样本由$d$个属性描述。此时我们试图学得:

$f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b$,使得$f\left(\boldsymbol{x}_{i}\right) \simeq y_{i}$

这称为“多元线性回归”（multivariate linear regression)

类似的，可利用最小二乘法来对$\boldsymbol{w}$和$b$进行估计。为便于讨论，我们把$\boldsymbol{w}$和$b$吸收入向量形式$\hat{\boldsymbol{w}}=(\boldsymbol{w} ; b)$，相应的，把数据集$D$表示为一个$m \times(d+1)$大小的矩阵$\mathbf{X}$，其中每行对应于一个示例，该行前$d$个元素对应于示例的$d$个属性值，最后一个元素恒置为 1, 即

$\mathbf{X}=\left(\begin{array}{ccccc}x_{11} & x_{12} & \dots & x_{1 d} & 1 \\ x_{21} & x_{22} & \dots & x_{2 d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m 1} & x_{m 2} & \dots & x_{m d} & 1\end{array}\right)=\left(\begin{array}{cc}\boldsymbol{x}_{1}^{\mathrm{T}} & 1 \\ \boldsymbol{x}_{2}^{\mathrm{T}} & 1 \\ \vdots & \vdots \\ \boldsymbol{x}_{m}^{\mathrm{T}} & 1\end{array}\right)$

再把标记也写成向量形式$\boldsymbol{y}=\left(y_{1} ; y_{2} ; \ldots ; y_{m}\right)$,则有：

$\hat{\boldsymbol{w}}^{*}=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$

令$E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$，对$\hat{\boldsymbol{w}}$求导得到：

$\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})$

令上式为零可得$\hat{\boldsymbol{w}}$最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些。下面我们做一个简单的讨论。

当$\mathbf{X}^{\mathrm{T}} \mathbf{X}$为满秩矩阵（full-rank matrix）或正定矩阵（positive definite ma trix）时，令式为零可得

$\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$

其中$\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1}$是矩阵$\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)$的逆矩阵。令$\hat{\boldsymbol{x}}_{i}=\left(\boldsymbol{x}_{i}, 1\right)$，则最终学得的多元线性回归模型为：

$f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{x}}_{i}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$

然而，现实任务中$\mathbf{X}^{\mathrm{T}} \mathbf{X}$往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致 $\mathbf{X}$的列数多于行数，$\mathbf{X}^{\mathrm{T}} \mathbf{X}$显然不满秩。此时可解出多个$\hat{\boldsymbol{w}}$，它们都能使均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化（regularization）项。

线性模型虽简单，却有丰富的变化。例如对于样例$(\boldsymbol{x}, y), y \in \mathbb{R}$，当我们希望线性模型的预测值逼近真实标记$y$时，就得到了线性回归模型。为便于观察，我们把线性回归模型简写为

$y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$

可否令模型预测值逼近$y$的衍生物呢？譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即

$\ln y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$

这就是“对数线性回归”（log-linear regression），它实际上是在试图让$e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}$逼近$y$。上式在形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射，如图所示。这里的对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200427150204.png)

更一般地，考虑单调可微函数$g(\cdot)$，$g(\cdot)$连续且充分光滑，令：

$y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)$

这样得到的模型称为“广义线性模型”（generalized linear model），其中函数$g(\cdot)$称为“联系函数”（link function）。显然，对数线性回归是广义线性模型在$g(\cdot)=\ln (\cdot)$时的特例。

>广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行。
## 对数几率回归
上一节讨论了如何使用线性模型进行回归学习，但若要做的是分类任务该怎么办？答案蕴涵在广义线性模型中：只需找一个单调可微函数将分类任务的真实标记 $y$与线性回归模型的预测值联系起来。

考虑二分类任务，其输出标记$y \in\{0,1\}$，而线性回归模型产生的预测值$z=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$是实值,于是,我们需将实值$z$转换为0/1值.最理想的是“单位阶跃函数”（unit- step function):

$y=\left\{\begin{aligned} 0, & z<0 \\ 0.5, & z=0 \\ 1, & z>0 \end{aligned}\right.$

若预测值 2 大于零就判为正例，小于零则判为反例，预测值为临界值零则可任意判别，如图所示。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200427150632.png)

但从图可看出，单位阶跃函数不连续，因此不能直接用作式中的$g^{-}(\cdot)$，于是我们希望找到能在一定程度上近似单位阶跃函数的“替代函数”（surrogate function），并希望它单调可微。对数几率函数（logistic function）正是这样一个常用的替代函数：

$y=\frac{1}{1+e^{-z}}$

从图可看出，对数几率函数是一种“Sigmoid 函数”，它将$z$值转化为一个接近 0 或 1 的$y$值，并且其输出值在 x=0 附近变化很陡。将对数几率函数作为$g^{-}(\cdot)$代入式，得到

$y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}$

$\ln \frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$

若将$y$视为样本$\boldsymbol{x}$作为正例的可能性，则$1-y$是其反例可能性，两者的比值

$\frac{y}{1-y}$

称为“几率”（odls），反映了$\boldsymbol{x}$作为正例的相对可能性。对几率取对数则得到“对数几率”（log odds，亦称 logit):

$\ln \frac{y}{1-y}$

由此可看出，实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为“对数几率回归”（logistic regression，亦称 logit regression）。特别需注意到，虽然它的名字是“回归”，但实际却是一种分类学习方法。这种方法有很多优点，例如它是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题；它不是仅预测出“类别”，而是可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用；此外，对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。

下面我们来看看如何确定$\boldsymbol{w}$和$b$。若将$y$视为类后验概率估计$p(y=1 | \boldsymbol{x})$，则式可重写为

$\ln \frac{p(y=1 | \boldsymbol{x})}{p(y=0 | \boldsymbol{x})}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$

显然有:

$p(y=1 | \boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}$

$p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}$

于是，我们可通过“极大似然法”（maximum likelihood method）来估计$\boldsymbol{w}$和$b$。给定数据集$\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{m}$, 对率回归模型最大化“对数似然”（oglikelihood):

$\ell(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)$

即令每个样本属于其真实标记的概率越大越好。为便于讨论，令$\boldsymbol{\beta}=(\boldsymbol{w} ; b)$，$\hat{\boldsymbol{x}}=(\boldsymbol{x} ; 1)$,则$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$可简写为$\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}$。再令$p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=1 | \hat{\boldsymbol{x}} ; \boldsymbol{\beta})$,$p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=0 | \hat{\boldsymbol{x}} ; \boldsymbol{\beta})=1-p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})$，则式中的似然项可重写为:

$p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)=y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)$

最大化式$\ell(\boldsymbol{w}, b)$等价于最小化:

$\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(-y_{i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}}\right)\right)$

上式是关于$\boldsymbol{\beta}$的高阶可导连续凸函数，根据凸优化理论, 经典的数值优化算法如梯度下降法（gradient descent method）、牛顿法（Newton method）等都可求得其最优解，于是就得到

$\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}}{\arg \min } \ell(\boldsymbol{\beta})$

以牛顿法为例，其第$t+1$轮迭代解的更新公式为：

$\boldsymbol{\beta}^{t+1}=\boldsymbol{\beta}^{t}-\left(\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{T}}\right)^{-1} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}$

其中关于$\boldsymbol{\beta}$的一阶、二阶导数分别为：

$\begin{aligned} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &=-\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i}\left(y_{i}-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right) \\ \frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}} &=\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i} \hat{\boldsymbol{x}}_{i}^{\mathrm{T}} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\left(1-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right) \end{aligned}$

# 逻辑斯谛回归与最大熵模型
逻辑斯谛回归（logistic regression）是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）。逻辑斯谛回归模型与最大熵模型都属于对数线性模型。本章首先介绍逻辑斯谛回归模型，然后介绍最大熵模型，最后讲述逻辑斯谛回归与最大熵模型的学习算法，包括改进的迭代尺度算法和拟牛顿法。

## 逻辑斯谛回归模型
### 逻辑斯谛分布
设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列分布函数和密度函数：

$F(x)=P(X \leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu) / \gamma}}$

$f(x)=F^{\prime}(x)=\frac{\mathrm{e}^{-(x-\mu) / \gamma}}{\gamma\left(1+\mathrm{e}^{-(x-\mu) / \gamma}\right)^{2}}$

式中,$\mu$为位置参数，$\gamma>0$为形状参数

逻辑斯谛分布的密度函数$f(x)$和分布函数$F(x)$的图形如图所示。分布函数属于逻辑斯谛函数，其图形是一条$S$形曲线（sigmoid curve）。该曲线以点$\left(\mu, \frac{1}{2}\right)$中心对称，即满足:

$F(-x+\mu)-\frac{1}{2}=-F(x+\mu)+\frac{1}{2}$

曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。

![](https://picgp.oss-cn-beijing.aliyuncs.com/img/20200427161248.png)
### 二项逻辑斯谛回归模型
项逻辑斯谛回归模型（binomial logistic regression model）是一种分类模型，由条件概率分布$P(Y | X)$表示，形式为参数化的逻辑斯谛分布。这里，随机变量$X$取值为实数，随机变量$Y$取值为 1 或 0。我们通过监督学习的方法来估计模型参数。

逻辑斯谛回归模型：

二项逻辑斯谛回归模型是如下的条件概率分布：

$P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}$

$P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x+b)}$

这里，$x \in \mathbf{R}^{n}$是输入，$Y \in\{0,1\}$是输出，$w \in \mathbf{R}^{n}$和$b \in \mathbf{R}$是参数，$w$称为权值向量，$b$称为偏置，$w \cdot x$为$w$和$x$的内积。

对于给定的输入实例$x$，按照上式可以求得$P(Y=1 | x)$和$P(Y=0 | x)$。逻辑斯谛回归比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。

有时为了方便，将权值向量和输入向量加以扩充，仍记作$w, x$，即$w=\left(w^{(1)}\right.$$\left.w^{(2)}, \cdots, w^{(n)}, b\right)^{\mathrm{T}}, x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(n)}, 1\right)^{\mathrm{T}}$。这时，逻辑斯谛回归模型如下

$P(Y=1 | x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}$

$P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x)}$

现在考査逻辑斯谛回归模型的特点。一个事件的几率（ods）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率是该事件的对数几率（log odds）或 lgit 函数是:

$\operatorname{logit}(p)=\log \frac{p}{1-p}$

对逻辑斯谛回归而言，得

$\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x$

这就是说，在逻辑斯谛回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。或者说，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯谛回归模型。

换一个角度看，考虑对输入$x$进行分类的线性函数$w \cdot x$，其值域为实数域。注意，这里$x \in \mathbf{R}^{n+1}$,$w \in \mathbf{R}^{n+1}$。通过逻辑斯谛回归模型定义式可以将线性函数$w \cdot x$转换为概率：

$P(Y=1 | x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}$

这时，线性函数的值越接近正无穷，概率值就越接近 1; 线性函数的值越接近负无穷，概率值就越接近0。这样的模型就是逻辑斯谛回归模型。

### 模型参数估计
逻辑斯谛回归模型学习时，对于给定的训练数据集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots\right.$，其中,$x_{i} \in \mathbf{R}^{n}$, $y_{i} \in\{0,1\}$，可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型:

设:

$P(Y=1 | x)=\pi(x), \quad P(Y=0 | x)=1-\pi(x)$

似然函数为:

$\prod_{i=1}^{N}\left[\pi\left(x_{i}\right)\right]^{y_{i}}\left[1-\pi\left(x_{i}\right)\right]^{1-y_{i}}$

对数似然函数为:

$\begin{aligned} L(w) &=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-\pi\left(x_{i}\right)\right)\right] \\ &=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right] \\ &=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.\end{aligned}$

对$L(w)$求极大值，得到$w$的估计值。

这样，问题就变成了以对数似然函数为目标函数的最优化问题。逻辑斯谛回归学习中通常采用的方法是梯度下降法及拟牛顿法。

假设$w$的极大似然估计值是$\hat{w}$，那么学到的逻辑斯谛回归模型为:

$P(Y=1 | x)=\frac{\exp (\hat{w} \cdot x)}{1+\exp (\hat{w} \cdot x)}$

$P(Y=0 | x)=\frac{1}{1+\exp (\hat{w} \cdot x)}$

### 多项逻辑斯谛回归
上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类。可以将其推广为多项逻辑斯谛回归模型（multi-nominal logistic regression model），用于多类分类。假设离散型随机变量$Y$的取值集合是$\{1,2, \cdots, K\}$，那么多项逻辑斯谛回归模型是:

$P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1$
$P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}$

这里，$x \in \mathbf{R}^{n+1}, w_{k} \in \mathbf{R}^{n+1}$。

项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归。
