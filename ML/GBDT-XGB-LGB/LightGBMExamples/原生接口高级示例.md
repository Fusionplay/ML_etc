# 原生接口高级示例
```python
import json
import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error

try:
    import cPickle as pickle
except BaseException:
    import pickle
```

## 加载或创建自己的数据集
```python
print('Loading data...')
df_train = pd.read_csv('./data/binary_classification/binary.train', header=None, sep='\t')
df_test = pd.read_csv('./data/binary_classification/binary.test', header=None, sep='\t')
W_train = pd.read_csv('./data/binary_classification/binary.train.weight', header=None)[0]
W_test = pd.read_csv('./data/binary_classification/binary.test.weight', header=None)[0]

y_train = df_train[0]
y_test = df_test[0]
X_train = df_train.drop(0, axis=1)
X_test = df_test.drop(0, axis=1)

num_train, num_feature = X_train.shape
```
## 创建`lightgbm`数据集 
```python
lgb_train = lgb.Dataset(X_train, y_train, #weight:一个列表、1维的numpy array 或者None， 它指定了样本的权重。默认为None
                        weight=W_train, free_raw_data=False)
#free_raw_data：一个布尔值，指定是否在创建完Dataset 之后释放原始的数据。默认为True调用Dataset() 之后，并没有构建完Dataset。 构建完需要等到构造一个Booster 的时候。如果要重复使用数据，请记住设置free_raw_data = False,
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,#reference:如如果当前构建的数据集用于验证集，则reference 必须传入训练集。否则会报告has different bin 
                       weight=W_test, free_raw_data=False)
```
## 指定配置
```python
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
    #'device':'gpu'
    #'gpu_use_dp`=True,默认使用32位浮点数来求和。启动64位浮点数，但是它会使得训练速度降低。
    #'gpu_device_id`:-1,表示设备ID。 默认为-1 。在选定的platform 上，每个GPU 都有一个唯一的设备ID。 -1 表示选定platform 上的默认设备。
}
```
## 生成特征名字
```python
feature_name = ['feature_' + str(col) for col in range(num_feature)]
```

## 训练
```python
#feature_name and categorical_feature
print('Starting training...')
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                valid_sets=lgb_train,  # eval training data
                feature_name=feature_name,
                categorical_feature=[21])

print('Finished first 10 rounds...')
# check feature name
print('7th feature name is:', lgb_train.feature_name[6])
```
## 保存模型
```python
print('Saving model...')
gbm.save_model('model.txt')
print('Dumping model to JSON...')
#dump model to JSON (and save to file)
model_json = gbm.dump_model()

with open('model.json', 'w+') as f:
    json.dump(model_json, f, indent=4)
```
## 特征名字
```python
print('Feature names:', gbm.feature_name())
```
## 特征重要性
```python
print('Feature importances:', list(gbm.feature_importance()))
```

## 预测
```python
print('Loading model to predict...')
bst = lgb.Booster(model_file='model.txt')
#只能以最佳迭代（或保存迭代）进行预测
y_pred = bst.predict(X_test)
#用加载模型评估
print("The rmse of loaded model's prediction is:", mean_squared_error(y_test, y_pred) ** 0.5)
```
```python
print('Dumping and loading model with pickle...')
#dump model with pickle
with open('model.pkl', 'wb') as fout:
    pickle.dump(gbm, fout)
#load model with pickle to predict
with open('model.pkl', 'rb') as fin:
    pkl_bst = pickle.load(fin)
#可以以pickle方式加载时进行任何迭代预测
y_pred = pkl_bst.predict(X_test, num_iteration=7)
#使用加载的模型评估
print("The rmse of pickled model's prediction is:", mean_squared_error(y_test, y_pred) ** 0.5)
```
## 继续训练
init_model accepts:
1. model file name
2. Booster()
```python
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model='model.txt',
                valid_sets=lgb_eval)

print('Finished 10 - 20 rounds with model file...')
```
## 学习率衰减
learning_rates accepts:
1. list/tuple with length = num_boost_round
2. function(curr_iter)
```python
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                learning_rates=lambda iter: 0.05 * (0.99 ** iter),
                valid_sets=lgb_eval)

print('Finished 20 - 30 rounds with decay learning rates...')
```
## 训练期间更改其他参数
```python
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                valid_sets=lgb_eval,
                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])

print('Finished 30 - 40 rounds with changing bagging_fraction...')
```

## 自定义目标函数
`f(preds: array, train_data: Dataset) -> grad: array, hess: array`
```python
#log likelihood loss
def loglikelihood(preds, train_data):
    labels = train_data.get_label()
    preds = 1. / (1. + np.exp(-preds))
    grad = preds - labels
    hess = preds * (1. - preds)
    return grad, hess
```

# 自定义评估指标
`(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool`
```python
#binary error
def binary_error(preds, train_data):
    labels = train_data.get_label()
    preds = 1. / (1. + np.exp(-preds))
    return 'error', np.mean(labels != (preds > 0.5)), False


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                fobj=loglikelihood,
                feval=binary_error,
                valid_sets=lgb_eval)

print('Finished 40 - 50 rounds with self-defined objective function and eval metric...')
```
## 另一个自定义评估指标
`f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool`
```python
#accuracy
def accuracy(preds, train_data):
    labels = train_data.get_label()
    preds = 1. / (1. + np.exp(-preds))
    return 'accuracy', np.mean(labels == (preds > 0.5)), True


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                fobj=loglikelihood,
                feval=lambda preds, train_data: [binary_error(preds, train_data),
                                                 accuracy(preds, train_data)],
                valid_sets=lgb_eval)

print('Finished 50 - 60 rounds with self-defined objective function '
      'and multiple self-defined eval metrics...')

print('Starting a new training job...')
```

## callback
```python
def reset_metrics():
    def callback(env):
        lgb_eval_new = lgb.Dataset(X_test, y_test, reference=lgb_train)
        if env.iteration - env.begin_iteration == 5:
            print('Add a new valid dataset at iteration 5...')
            env.model.add_valid(lgb_eval_new, 'new_valid')
    callback.before_iteration = True
    callback.order = 0
    return callback


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                valid_sets=lgb_train,
                callbacks=[reset_metrics()])

print('Finished first 10 rounds with callback function...')
```